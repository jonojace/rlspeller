{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4207e21b-d910-485b-8b9c-3fc7d5a59c5a",
   "metadata": {},
   "source": [
    "# Goal of this notebook\n",
    "\n",
    "Develop a training loop for finetuning ASR models using TTS loss by recreating RL training found in RL4LMs/rl4lms/envs/text_generation/training_utils.py\n",
    "\n",
    "```bash\n",
    "conda activate speller\n",
    "cd /home/s1785140/rlspeller\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "22a894b6-b557-4c63-a830-70c8954d7a6e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this notebook is alive!!!\n"
     ]
    }
   ],
   "source": [
    "print(\"this notebook is alive!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70687ad-9f13-4cdf-8ef7-07eecb34f496",
   "metadata": {},
   "source": [
    "# automatic reloading magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "c3ef7885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738aac35-fdd1-4452-aff0-92caa77ec5ab",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "9c8a89c2-38ea-4778-8028-0c7854152301",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import List, Dict, Tuple, Any\n",
    "import hyperpyyaml\n",
    "from tqdm import tqdm\n",
    "from torchaudio.models.decoder import ctc_decoder\n",
    "from torch.nn.functional import softmax\n",
    "import random\n",
    "from jiwer import cer\n",
    "import numpy as np\n",
    "import speechbrain as sb\n",
    "from IPython.display import Audio\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa2cd37",
   "metadata": {},
   "source": [
    "## check if gpu available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "53032f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "barre.inf.ed.ac.uk\n"
     ]
    }
   ],
   "source": [
    "# print hostname to make sure we are on correct node\n",
    "disallowed_nodes = ['escience6']\n",
    "import socket\n",
    "hostname = socket.gethostname()\n",
    "print(hostname)\n",
    "node = hostname.split('.')[0]\n",
    "if node in disallowed_nodes:\n",
    "    raise ValueError(f\"Running on disallowed node {node}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "f67b1b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.is_available()\n",
    "assert torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "b6f67235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/disk/nfs/ostrom/s1785140/rlspeller'"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b236619f-6c04-44b1-a697-a342417a6694",
   "metadata": {},
   "source": [
    "# HPARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "c8a11879-640c-433e-a192-9c8b551d8ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_hparams = {\n",
    "    \"sentencepiece_model_path\": \"/home/s1785140/speechbrain/templates/speech_recognition_CharTokens_NoLM/Tokenizer/save/0_char.model\",\n",
    "\n",
    "    # 'speechbrain_hparams_file': '/home/s1785140/rlspeller/infer_speechbrain_subsampling1x_nowhitespace.yaml',\n",
    "    'speechbrain_hparams_file': '/home/s1785140/rlspeller/infer_speechbrain_subsampling1x.yaml',\n",
    "    # 'speechbrain_hparams_file': '/home/s1785140/rlspeller/infer_speechbrain_subsampling2x.yaml',\n",
    "    # 'speechbrain_hparams_file': '/home/s1785140/rlspeller/infer_speechbrain.yaml', # 4x subsampling\n",
    "\n",
    "    'sample_rate': 16000, # TODO this isn't used???\n",
    "    \n",
    "    ### Loading ASR model \n",
    "    'enable_dropout': True,\n",
    "    # 'dropout_p': 0.0, # 0.15 is the default value in speechbrain\n",
    "    'dropout_p': 0.15, # 0.15 is the default value in speechbrain\n",
    "    # 'dropout_p': 0.3, # 0.15 is the default value in speechbrain\n",
    "    # 'dropout_p': 0.5, # 0.15 is the default value in speechbrain\n",
    "    # 'dropout_p': 0.99, # 0.15 is the default value in speechbrain\n",
    "    \n",
    "    ### filter ASR dataset by words in oov list\n",
    "    \"filter_by_per\": True,\n",
    "    \"str_min_len_threshold\": 10, # only keep wordtypes with more than this many characters\n",
    "    \"wordtypes_to_keep\": 10, # only transribe this number of wordtypes\n",
    "    \n",
    "    ### TRANSCRIBING AUDIO USING PRETRAINED ASR\n",
    "    \"batch_size\": 2, # for debug\n",
    "    # \"batch_size\": 64\n",
    "    'num_asr_iterations': 2, # for debug\n",
    "    # 'num_asr_iterations': 50, \n",
    "    'num_batches_to_transcribe': 2, # for debug\n",
    "    # 'num_batches_to_transcribe': None, # transcribe all dataset\n",
    "    'max_hyps_per_sample': 2, # for debug\n",
    "    # 'max_hyps_per_sample': 50,\n",
    "    \n",
    "    ### FASTPITCH INFERENCE\n",
    "    'fastpitch_inference_batch_size': 32,\n",
    "    \n",
    "    ### RANKING RESPELLINGS VIA ACOUSTIC DISTANCES FROM GT\n",
    "    # 'distance_input_reprs': 'mfcc',\n",
    "    'distance_input_reprs': 'hubert',\n",
    "    # 'hubert_type': 'soft', \n",
    "    # 'hubert_type': 'discrete',\n",
    "    'hubert_type': 'raw',\n",
    "    # 'discrete_type': 'centroid',\n",
    "    'discrete_type': 'code',\n",
    "    \n",
    "    'distance_type': 'euclidean',\n",
    "    # 'distance_type': 'cosine',\n",
    "    \n",
    "# NUM_BATCHES_TO_TRANSCRIBE = None\n",
    "# MAX_HYPS_PER_SAMPLE = 50\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e877615-a6ff-4755-a7e4-e6f72e542432",
   "metadata": {},
   "source": [
    "# TOKENIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "0505c38d-bc92-4f84-9f1f-cfc942ad337d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "# load pretrained tokenizer used to tokenizer ASR training inputs \n",
    "import sentencepiece as spm \n",
    "spm_path = inference_hparams[\"sentencepiece_model_path\"]\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(spm_path)\n",
    "print(sp.vocab_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "a4af21be-9224-4c45-87f6-b86a8d7c63ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 10 2 12 12 4 1 17 4 9 12 11 1 16 20 1 6 5 16 2 1 7 8 1 26 5 8 4 6\n"
     ]
    }
   ],
   "source": [
    "# test tokenizer\n",
    "s = \"hello world my name is jason\"\n",
    "# TODO pass string through text cleaners? \n",
    "encoded = sp.EncodeAsIds(s)\n",
    "assert 0 not in encoded, \"tried to encode an unknown character\"\n",
    "print(\" \".join(str(idx) for idx in encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "fe3c6048-79c5-47b4-9524-88d2e36ad8d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello world my name is jason'"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.DecodeIds(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2996d1f1-d331-4919-8742-7907b46a46ce",
   "metadata": {},
   "source": [
    "# NEW! SIMPLE TOKENIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "f8951a84-e22e-4c62-a169-403a1b5269af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from speechbrain.tokenizers.SimpleTokenizer import SimpleTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "bfec85a8-d20d-445e-8483-cf27092e2962",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = SimpleTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "9c52e4b5-8d26-44c3-a1ad-c772b6feac62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello|my|name|is|jason\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[9, 6, 13, 13, 16, 1, 14, 26, 1, 15, 2, 14, 6, 1, 10, 20, 1, 11, 2, 20, 16, 15]"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"hello my name is jason\"\n",
    "text = text.replace(' ', '|')\n",
    "print(text)\n",
    "ids = tokenizer.encode_as_ids(text)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "f5f1bf5e-500b-47ce-986c-3cbe059cdf41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello|my|name|is|jason'"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode_ids(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5ea5f7-6224-4089-8136-5c4b8d47a2e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## test simple tokenizer with probability distribution, and see if CTC decoder successfully generates n-best lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "a60ffb1b-6770-466c-90f6-6daa1dbe461b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create empty array of correct dimensions\n",
    "min_len, max_len = 50, 100\n",
    "bsz = 4\n",
    "lens = torch.randint(min_len, max_len, (bsz,))\n",
    "vocab_size = len(tokenizer.vocab)\n",
    "\n",
    "# randomly assign probaility distribution to each timestep\n",
    "\n",
    "# try to decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "cac82712-8559-44d0-86d9-66cf1ddfbd5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "randn = torch.randn(bsz, max_len, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "26e7e51c-bab1-4f2d-bb12-34f9f9c4454c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ctc_probs = softmax(randn, dim=1)\n",
    "# ctc_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "7f9abdc6-bd05-4c5c-a091-d04dadbaa9e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('sample 1, hyp 1/2', ['|tlipf|mqfsosvwsdjtcws|rxnrdceikrgbkbpecg|rpsugwfydcsxztajlzpykfqanihnxhpbadlxzldag|'])\n",
      "('sample 1, hyp 2/2', ['|tlipf|mqfsosvwsdjtcws|rxnrdceikrgbkbpecg|rpsugwfydcsxztajlzpykfqanqhnxhpbadlxzldag|'])\n",
      "('sample 2, hyp 1/2', ['|krplvkwuxnczqaovtgjxcruoktj|nxfzjrqptaosjsedhkmbkzjftcfrdvpdephfrzxhgaioxqanxnwzmyvtiuy|'])\n",
      "('sample 2, hyp 2/2', ['|krplvkwuxnczqaontgjxcruoktj|nxfzjrqptaosjsedhkmbkzjftcfrdvpdephfrzxhgaioxqanxnwzmyvtiuy|'])\n",
      "('sample 3, hyp 1/2', ['|nujmhfiunbcsbirbsjw|xosygelwheaduimzh|dfjpznypxdovbrnjaq|'])\n",
      "('sample 3, hyp 2/2', ['|nujmhfiunbcsbirbsjw|xqsygelwheaduimzh|dfjpznypxdovbrnjaq|'])\n",
      "('sample 4, hyp 1/2', ['|n|dqxadbnpsvjzkoyeq|ecvyxkazafgmthiqmpifb|hrxfudzlejvop|qbanlvjhsjyjmbpulzdsxtyizkyhqw|'])\n",
      "('sample 4, hyp 2/2', ['|n|dqxadbnpsvjzkoyeq|ecvyxkazafgmthiqmpifb|haxfudzlejvop|qbanlvjhsjyjmbpulzdsxtyizkyhqw|'])\n"
     ]
    }
   ],
   "source": [
    "ctc_beamsearch_decoder_test = ctc_decoder(\n",
    "    lexicon=None,\n",
    "    # tokens=\"/home/s1785140/rlspeller/templates/speech_recognition_CharTokens_NoLM/Tokenizer/save/tokens.txt\",\n",
    "    tokens=tokenizer.vocab,\n",
    "    nbest=2,\n",
    "    blank_token='-',\n",
    "    sil_token=\"|\",\n",
    ")\n",
    "\n",
    "predicted_ids = ctc_beamsearch_decoder_test(ctc_probs, lens)\n",
    "\n",
    "predicted_words = []\n",
    "for i, hyps in enumerate(predicted_ids):\n",
    "    for j, hyp in enumerate(hyps):\n",
    "        words = tokenizer.decode_ids(hyp.tokens.tolist()).split(\" \")\n",
    "        tup = (f\"sample {i+1}, hyp {j+1}/{len(hyps)}\", words)\n",
    "        predicted_words.append(tup)\n",
    "        print(tup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd650fd4",
   "metadata": {},
   "source": [
    "# LOAD ASR (PRETRAINED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "56bf160c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from templates.speech_recognition_CharTokens_NoLM.ASR.train import ASR\n",
    "from templates.speech_recognition_CharTokens_NoLM.ASR.train import dataio_prepare\n",
    "from torch.utils.data import DataLoader\n",
    "from speechbrain.dataio.dataloader import LoopedLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "36d8019b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/s1785140/speechbrain/templates/speech_recognition_CharTokens_NoLM/data/rirs_noises.zip exists. Skipping download\n"
     ]
    }
   ],
   "source": [
    "# Load hyperparameters file with command-line overrides\n",
    "speechbrain_hparams_file = inference_hparams['speechbrain_hparams_file']\n",
    "with open(speechbrain_hparams_file) as f:\n",
    "    speechbrain_hparams = hyperpyyaml.load_hyperpyyaml(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "9b624228-ac94-4503-a563-ad90d8fbe95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overrides\n",
    "if 'batch_size' in inference_hparams:\n",
    "    speechbrain_hparams['batch_size'] = inference_hparams['batch_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "26c15de1-25d9-428e-900f-967c3af9a8ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/s1785140/speechbrain/templates/speech_recognition_CharTokens_NoLM/ASR/results/CRDNN_CHAR_LJSpeech_halved_subsampling1x/2602/save'"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speechbrain_hparams['save_folder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "896a3d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace some values with inference time values\n",
    "speechbrain_hparams['dropout'] = inference_hparams['dropout_p']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "b4072195-fb2b-4c9a-b201-c84b7a7ae104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise trainer (we don't want to train, but model is tightly coupled with trainer)\n",
    "asr_brain = ASR(\n",
    "    modules=speechbrain_hparams[\"modules\"],\n",
    "    opt_class=speechbrain_hparams[\"opt_class\"],\n",
    "    hparams=speechbrain_hparams,\n",
    "    checkpointer=speechbrain_hparams[\"checkpointer\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "1e8b676b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if on_evaluate_start() get runtime error, likely need to restart notebook kernel\n",
      "Enabled dropout in model:\n",
      "changed dropout p of encoder-CNN.block_0.drop.drop-Dropout2d(p=0.15, inplace=False) from 0.15 to 0.15\n",
      "changed dropout p of encoder-CNN.block_1.drop.drop-Dropout2d(p=0.15, inplace=False) from 0.15 to 0.15\n",
      "changed dropout p of encoder-RNN.rnn-LSTM(2560, 512, num_layers=2, batch_first=True, dropout=0.15, bidirectional=True) from 0.15 to 0.15\n",
      "changed dropout p of encoder-DNN.block_0.dropout-Dropout(p=0.15, inplace=False) from 0.15 to 0.15\n",
      "changed dropout p of encoder-DNN.block_1.dropout-Dropout(p=0.15, inplace=False) from 0.15 to 0.15\n"
     ]
    }
   ],
   "source": [
    "def change_dropout_p(brain, new_dropout_p):\n",
    "    for module_name in brain.modules.keys():\n",
    "        module = brain.modules[module_name]\n",
    "        for submodule_name, submodule in module.named_modules():\n",
    "            if isinstance(submodule, torch.nn.Dropout) or isinstance(submodule, torch.nn.Dropout2d):\n",
    "                # handle proper dropout submodules\n",
    "                print(f\"changed dropout p of {module_name}-{submodule_name}-{submodule} from {submodule.p} to {new_dropout_p}\")\n",
    "                submodule.p = new_dropout_p\n",
    "            else:\n",
    "                # handle dropout in nn.LSTM\n",
    "                for attr in [\"dropout\", \"drop\"]:\n",
    "                    if hasattr(submodule, attr) and type(getattr(submodule, attr)) == float: \n",
    "                        print(f\"changed dropout p of {module_name}-{submodule_name}-{submodule} from {getattr(submodule, attr)} to {new_dropout_p}\")\n",
    "                        setattr(submodule, attr, new_dropout_p)\n",
    "\n",
    "def setup_asr_brain_for_infer(asr_brain, enable_dropout=False, new_dropout_p=0.15):\n",
    "    asr_brain.on_evaluate_start(min_key=\"WER\") # We call the on_evaluate_start that will load the best model\n",
    "    if enable_dropout:\n",
    "        asr_brain.modules.train()\n",
    "        print(\"Enabled dropout in model:\")\n",
    "        change_dropout_p(asr_brain, new_dropout_p)\n",
    "    else:\n",
    "        asr_brain.modules.eval() # We set the model to eval mode (remove dropout etc)\n",
    "\n",
    "print(\"if on_evaluate_start() get runtime error, likely need to restart notebook kernel\")\n",
    "setup_asr_brain_for_infer(asr_brain, enable_dropout=inference_hparams[\"enable_dropout\"], new_dropout_p=inference_hparams[\"dropout_p\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "909b3c39-3a03-4384-82ca-2305b3ed2272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset and dataloader for inference\n",
    "datasets = dataio_prepare(speechbrain_hparams)\n",
    "\n",
    "test_set = datasets['test']\n",
    "\n",
    "if not isinstance(test_set, DataLoader) or isinstance(test_set, LoopedLoader):\n",
    "    test_loader_kwargs=speechbrain_hparams[\"test_dataloader_opts\"]\n",
    "    test_set = asr_brain.make_dataloader(\n",
    "        test_set, stage=sb.Stage.TEST, **test_loader_kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "69b4b4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ⁇ ', '', 'e', 't', 'o', 'a', 'n', 'i', 's', 'r', 'h', 'd', 'l', 'c', 'f', 'u', 'm', 'w', 'p', 'g', 'y', 'b', 'v', 'k', 'x', 'q', 'j', 'z']\n",
      "['-', '|', 'e', 't', 'o', 'a', 'n', 'i', 's', 'r', 'h', 'd', 'l', 'c', 'f', 'u', 'm', 'w', 'p', 'g', 'y', 'b', 'v', 'k', 'x', 'q', 'j', 'z']\n"
     ]
    }
   ],
   "source": [
    "# get vocab from tokenizer (needed for ctc decoding)\n",
    "vocab_size = len(asr_brain.hparams.tokenizer)\n",
    "vocab = []\n",
    "for i in range(vocab_size):\n",
    "    vocab.append(asr_brain.hparams.tokenizer.decode_ids([i]))\n",
    "print(vocab)\n",
    "\n",
    "# edit vocab to match default ctc decoder symbols for blank and silence\n",
    "vocab[0] = '-'\n",
    "vocab[1] = \"|\"\n",
    "\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "6af49d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctc_beamsearch_decoder = ctc_decoder(\n",
    "    lexicon=None,\n",
    "    # tokens=\"/home/s1785140/rlspeller/templates/speech_recognition_CharTokens_NoLM/Tokenizer/save/tokens.txt\",\n",
    "    tokens=vocab,\n",
    "    nbest=100,\n",
    "    blank_token='-',\n",
    "    sil_token=\"|\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "b4d6501c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate transcriptions for all batches in test set\n",
    "def transcribe_dataset(asr_brain, dataset, greedy=False, num_batches_to_transcribe=None):\n",
    "    # Now we iterate over the dataset and we simply compute_forward and decode\n",
    "    with torch.no_grad():\n",
    "        transcripts = []\n",
    "        for batch in tqdm(list(dataset)[:num_batches_to_transcribe], dynamic_ncols=True):\n",
    "            orig_transcriptions = batch.words\n",
    "\n",
    "            # Make sure that your compute_forward returns the predictions !!!\n",
    "            # In the case of the template, when stage = TEST, a beam search is applied \n",
    "            # in compute_forward(). \n",
    "            predictions = asr_brain.compute_forward(batch, stage=sb.Stage.TEST)\n",
    "            \n",
    "            ctc_probs = predictions['ctc_logprobs'] # FOR DEBUG\n",
    "\n",
    "            if greedy:\n",
    "                predicted_ids = sb.decoders.ctc_greedy_decode(\n",
    "                    predictions[\"ctc_logprobs\"], asr_brain.feat_lens, blank_id=asr_brain.hparams.blank_index\n",
    "                )\n",
    "                predicted_words = [\n",
    "                    asr_brain.tokenizer.decode_ids(ids).split(\" \")\n",
    "                    for ids in predicted_ids\n",
    "                ]\n",
    "            else:\n",
    "                # get mel lens from wav len ratios since torch ctc decoder requires lens in frames\n",
    "                batch_max_len = predictions[\"ctc_logprobs\"].size(1)\n",
    "                bsz = predictions[\"ctc_logprobs\"].size(0)\n",
    "                mel_lens = torch.zeros(bsz)\n",
    "                for i, len_ratio in enumerate(asr_brain.feat_lens):\n",
    "                    mel_lens[i] = int(torch.round(len_ratio * batch_max_len))\n",
    "                \n",
    "                predicted_ids = ctc_beamsearch_decoder(\n",
    "                    predictions[\"ctc_logprobs\"], lengths=mel_lens\n",
    "                )\n",
    "\n",
    "                predicted_words = []\n",
    "                for i, (utt_id, orig_text, hyps) in enumerate(zip(batch.utt_id, orig_transcriptions, predicted_ids)):\n",
    "                    print(f\"\\nsample {i+1} - ({utt_id}: '{orig_text}')\")\n",
    "                    sample_cers = []\n",
    "                    for j, hyp in enumerate(hyps):\n",
    "                        words = asr_brain.hparams.tokenizer.decode_ids(hyp.tokens.tolist()) # .split(\"|\")\n",
    "                        # words = tokenizer.decode_ids(hyp.tokens.tolist()) # .split(\"|\")\n",
    "                        hyp_cer = 100 * cer(orig_text, words)\n",
    "                        sample_cers.append(hyp_cer)\n",
    "                        print(f\"\\thyp {j+1}/{len(hyps)} (CER={hyp_cer:.1f}%): '{words}'\")\n",
    "                        predicted_words.append((f\"sample {i+1}, hyp {j+1}/{len(hyps)}\", words))\n",
    "                        \n",
    "                    print(f\"\\t=== Mean CER: {np.mean(sample_cers):.1f}%, Std CER: {np.std(sample_cers):.1f}% ===\")\n",
    "\n",
    "            transcripts.append(predicted_words)\n",
    "\n",
    "    return transcripts, ctc_probs\n",
    "\n",
    "# transcripts, ctc_probs = transcribe_dataset(asr_brain, test_set, greedy=False, num_batches_to_transcribe=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf186366",
   "metadata": {},
   "source": [
    "# LOAD WORD ALIGNED WAVS into dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "c91aad24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imitate CLAs\n",
    "import sys\n",
    "import argparse\n",
    "import math\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "c0e87365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set these in yaml config!\n",
    "\n",
    "# these are to be loaded!\n",
    "train_annotation_path = '/home/s1785140/speechbrain/templates/speech_recognition_CharTokens_NoLM/data/respeller_train_wordtoken_wav_annotation.json'\n",
    "valid_annotation_path = '/home/s1785140/speechbrain/templates/speech_recognition_CharTokens_NoLM/data/respeller_valid_wordtoken_wav_annotation.json'\n",
    "test_annotation_path = '/home/s1785140/speechbrain/templates/speech_recognition_CharTokens_NoLM/data/respeller_test_wordtoken_wav_annotation.json'\n",
    "\n",
    "# this will be saved!\n",
    "allsplits_annotation_path = '/home/s1785140/speechbrain/templates/speech_recognition_CharTokens_NoLM/data/respeller_allsplits_wordtoken_wav_annotation.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "37bbdf2d-1221-49fe-ae1b-26c4479d40f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a new \"combined annotation\", that includes all of train, valid and test\n",
    "def load_json(p):\n",
    "    with open(p) as f:\n",
    "        d = json.load(f)\n",
    "    return d\n",
    "\n",
    "allsplits_d = {}\n",
    "for p in [train_annotation_path, valid_annotation_path, test_annotation_path]:\n",
    "    allsplits_d.update(load_json(p))\n",
    "    \n",
    "json_data = json.dumps(allsplits_d)\n",
    "with open(allsplits_annotation_path, 'w') as f:\n",
    "    f.write(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "06edab0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "speechbrain_hparams['train_annotation'] = train_annotation_path\n",
    "speechbrain_hparams['valid_annotation'] = valid_annotation_path\n",
    "speechbrain_hparams['test_annotation'] = test_annotation_path\n",
    "speechbrain_hparams['allsplits_annotation'] = allsplits_annotation_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "a6adcc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataio_prepare(\n",
    "    hparams, \n",
    "    split2data=None # use to optionally initialise datasets from data dicts rather than json files\n",
    "):\n",
    "    \"\"\"This function prepares the datasets to be used in the brain class.\n",
    "    It also defines the data processing pipeline through user-defined functions.\n",
    "\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    hparams : dict\n",
    "        This dictionary is loaded from the `train.yaml` file, and it includes\n",
    "        all the hyperparameters needed for dataset construction and loading.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    datasets : dict\n",
    "        Dictionary containing \"train\", \"valid\", and \"test\" keys that correspond\n",
    "        to the DynamicItemDataset objects.\n",
    "    \"\"\"\n",
    "    # Define audio pipeline. In this case, we simply read the path contained\n",
    "    # in the variable wav with the audio reader.\n",
    "    # wav path like: data/ljspeech_wavs_16khz_word_aligned/differs/differs__LJ001-0001__occ1__len8320.wav\n",
    "    @sb.utils.data_pipeline.takes(\"wav\")\n",
    "    @sb.utils.data_pipeline.provides(\"sig\", \"wav_path\", \"utt_id\")\n",
    "    def audio_pipeline(wav_path):\n",
    "        \"\"\"Load the audio signal. This is done on the CPU in the `collate_fn`.\"\"\"\n",
    "        sig = sb.dataio.dataio.read_audio(wav_path)\n",
    "        yield sig\n",
    "\n",
    "        yield wav_path\n",
    "\n",
    "        utt_id = wav_path.split(\"/\")[-1].split(\".\")[0]\n",
    "        yield utt_id\n",
    "\n",
    "    @sb.utils.data_pipeline.takes(\"samples_to_graphemes_ratio\")\n",
    "    @sb.utils.data_pipeline.provides(\"samples_to_graphemes_ratio\")\n",
    "    def ratio_pipeline(samples_to_graphemes_ratio):\n",
    "        yield samples_to_graphemes_ratio\n",
    "\n",
    "    @sb.utils.data_pipeline.takes(\"length\")\n",
    "    @sb.utils.data_pipeline.provides(\"length\")\n",
    "    def length_pipeline(length):\n",
    "        yield length\n",
    "\n",
    "    @sb.utils.data_pipeline.takes(\"words\")\n",
    "    @sb.utils.data_pipeline.provides(\"words\")\n",
    "    def text_pipeline(words):\n",
    "        \"\"\"Processes the transcriptions to generate proper labels\n",
    "\n",
    "        NB Make sure that you yield exactly what is defined above in @sb.utils.data_pipeline.provides()\"\"\"\n",
    "        yield words\n",
    "\n",
    "    # Define datasets from json data manifest file\n",
    "    # Define datasets sorted by ascending lengths for efficiency\n",
    "    datasets = {}\n",
    "\n",
    "    load_annotations_from_json_files = split2data is None\n",
    "    if load_annotations_from_json_files:\n",
    "        # by default load all splits from the json files specified in hparams\n",
    "        data_info = {\n",
    "            \"train\": hparams[\"train_annotation\"],\n",
    "            \"valid\": hparams[\"valid_annotation\"],\n",
    "            \"test\": hparams[\"test_annotation\"],\n",
    "            \"allsplits\": hparams[\"allsplits_annotation\"],\n",
    "        }\n",
    "    else:\n",
    "        data_info = split2data\n",
    "        \n",
    "    for split in data_info:\n",
    "        print(f\"create Dataset from split {split}\")\n",
    "        if load_annotations_from_json_files:\n",
    "            constructor = sb.dataio.dataset.DynamicItemDataset.from_json\n",
    "        else:\n",
    "            constructor = sb.dataio.dataset.DynamicItemDataset\n",
    "            \n",
    "        datasets[split] = constructor(\n",
    "            data_info[split],\n",
    "            dynamic_items=[audio_pipeline, ratio_pipeline, length_pipeline, text_pipeline],\n",
    "            output_keys = [\n",
    "                \"id\",\n",
    "                \"sig\",\n",
    "                \"wav_path\",\n",
    "                \"utt_id\",\n",
    "                \"samples_to_graphemes_ratio\",\n",
    "                \"length\",\n",
    "                \"words\",\n",
    "            ],\n",
    "        )\n",
    "        hparams[f\"{split}_dataloader_opts\"][\"shuffle\"] = True\n",
    "        datasets[split].split = split # add attribute to keep track of the split of the dataset\n",
    "\n",
    "    def print_dataset_lens(extra_str):\n",
    "        for split in data_info:\n",
    "            dataset_split = datasets[split]\n",
    "            print(f\"{split} dataset has {len(dataset_split)} samples\", extra_str)\n",
    "\n",
    "    print_dataset_lens(\"before any filtering\")\n",
    "\n",
    "    # Filter data for samples_to_graphemes_ratio that is either too small or too large\n",
    "    key_min_value = {}\n",
    "    if hparams[\"min_samples_to_graphemes_ratio\"] is not None:\n",
    "        key_min_value = {\"samples_to_graphemes_ratio\": hparams[\"min_samples_to_graphemes_ratio\"]}\n",
    "\n",
    "    key_max_value = {}\n",
    "    if hparams[\"max_samples_to_graphemes_ratio\"] is not None:\n",
    "        key_max_value = {\"samples_to_graphemes_ratio\": hparams[\"max_samples_to_graphemes_ratio\"]}\n",
    "\n",
    "    for split in data_info:\n",
    "        datasets[split] = datasets[split].filtered_sorted(\n",
    "            key_min_value=key_min_value,\n",
    "            key_max_value=key_max_value,\n",
    "        )\n",
    "    print_dataset_lens(\"after filtering by min and max samples to graphemes ratio\")\n",
    "\n",
    "    # Filter samples whos length is too short\n",
    "    key_min_value = {}\n",
    "    if hparams[\"min_length_seconds\"] is not None:\n",
    "        key_min_value = {\"length\": hparams[\"min_length_seconds\"]}\n",
    "    for split in data_info:\n",
    "        datasets[split] = datasets[split].filtered_sorted(\n",
    "            key_min_value=key_min_value,\n",
    "        )\n",
    "    print_dataset_lens(\"after filtering by minimum length\")\n",
    "\n",
    "    # Sorting training data with ascending order makes the code  much\n",
    "    # faster  because we minimize zero-padding. In most of the cases, this\n",
    "    # does not harm the performance.\n",
    "    if load_annotations_from_json_files:\n",
    "        if hparams[\"sorting\"] == \"ascending\":\n",
    "            datasets[\"train\"] = datasets[\"train\"].filtered_sorted(sort_key=\"length\")\n",
    "            hparams[\"train_dataloader_opts\"][\"shuffle\"] = False\n",
    "\n",
    "        elif hparams[\"sorting\"] == \"descending\":\n",
    "            datasets[\"train\"] = datasets[\"train\"].filtered_sorted(\n",
    "                sort_key=\"length\", reverse=True\n",
    "            )\n",
    "            hparams[\"train_dataloader_opts\"][\"shuffle\"] = False\n",
    "\n",
    "        elif hparams[\"sorting\"] == \"random\":\n",
    "            hparams[\"train_dataloader_opts\"][\"shuffle\"] = True\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                \"sorting must be random, ascending or descending\"\n",
    "            )\n",
    "    \n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "b9c40abe-e4d0-4469-8108-41b0eb089874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create Dataset from split allsplits\n",
      "allsplits dataset has 14 samples before any filtering\n",
      "allsplits dataset has 14 samples after filtering by min and max samples to graphemes ratio\n",
      "allsplits dataset has 14 samples after filtering by minimum length\n"
     ]
    }
   ],
   "source": [
    "## filter dataset by PER of wordtypes as predicted by RNN-based G2P model\n",
    "def filter_data_by_per(data, str_min_len_threshold, N):\n",
    "    p = '/home/s1785140/data/ljspeech_fastpitch/oov_list_with_PER.pickle'\n",
    "    loaded_df = pd.read_pickle(p)\n",
    "    loaded_df = loaded_df[loaded_df['wordtype'].str.len() >= str_min_len_threshold]\n",
    "    loaded_df = loaded_df.sort_values('per', ascending=False).head(N)\n",
    "    wordtypes_to_keep = set(loaded_df['wordtype'])\n",
    "    new_data = {}\n",
    "    for token_id, token_id_values in data.items():\n",
    "        wordtype = token_id_values['words']\n",
    "        if wordtype not in wordtypes_to_keep:\n",
    "            continue\n",
    "        new_data[token_id] = token_id_values\n",
    "    return new_data\n",
    "\n",
    "filter_by_per = inference_hparams[\"filter_by_per\"]\n",
    "# load from json files\n",
    "if not filter_by_per:\n",
    "    datasets = dataio_prepare(speechbrain_hparams)\n",
    "else:\n",
    "    filtered_allsplits_d = filter_data_by_per(allsplits_d, str_min_len_threshold=10, N=50)\n",
    "    filtered_allsplits_d = filter_data_by_per(allsplits_d, str_min_len_threshold=inference_hparams[\"str_min_len_threshold\"], N=inference_hparams[\"wordtypes_to_keep\"])\n",
    "    datasets = dataio_prepare(speechbrain_hparams, {'allsplits': filtered_allsplits_d})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "10d20085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert from datasets to dataloaders\n",
    "split2stage = {\"train\": sb.Stage.TRAIN, \"valid\": sb.Stage.VALID, \"test\": sb.Stage.TEST, \"allsplits\": sb.Stage.TEST}\n",
    "for split in datasets.keys():\n",
    "    if not isinstance(datasets[split], DataLoader) or isinstance(datasets[split], LoopedLoader):\n",
    "        dataloader_kwargs=speechbrain_hparams[f\"{split}_dataloader_opts\"]\n",
    "        datasets[split] = asr_brain.make_dataloader(\n",
    "            datasets[split], stage=split2stage[split], **dataloader_kwargs\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "c61a3fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_whitespace_to_0_probability(ctc_probs, vocab, log_probs=True, whitespace_symbol=\"|\"):\n",
    "    \"\"\"ctc_probs [bsz, max_seq_len, vocab_size]\"\"\"\n",
    "    new_probability = -math.inf if log_probs else 0.0\n",
    "    ctc_probs[:,:,vocab.index(whitespace_symbol)] = new_probability\n",
    "    return ctc_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2932070",
   "metadata": {},
   "source": [
    "# TRANSCRIBE WORD ALIGNED WAVS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "25a61168",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# generate transcriptions for all batches in test set\n",
    "def transcribe_dataset(asr_brain, dataset, \n",
    "                       num_batches_to_transcribe=None,\n",
    "                       hack_whitespace_probs=False, collapse_whitespace=True,\n",
    "                       print_info=False, max_hyps_per_sample=None,\n",
    "                       wordtypes_to_transcribe=[]):\n",
    "    # Now we iterate over the dataset and we simply compute_forward and decode\n",
    "\n",
    "    orig_words = []\n",
    "    printouts = []\n",
    "    orig_wavs = []\n",
    "    token_ids = []\n",
    "    transcribed_words = defaultdict(list)\n",
    "    n = 0 # number of batches transcribed\n",
    "    \n",
    "    break_forloop_early = num_batches_to_transcribe is not None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataset, dynamic_ncols=True, total=num_batches_to_transcribe):\n",
    "            # break out of loop if we have transcribed enough batches\n",
    "            if break_forloop_early and n >= num_batches_to_transcribe:\n",
    "                break\n",
    "            n += 1\n",
    "\n",
    "            orig_transcriptions = batch.words\n",
    "            wavs = batch.sig.data\n",
    "\n",
    "            # Make sure that your compute_forward returns the predictions !!!\n",
    "            # In the case of the template, when stage = TEST, a beam search is applied \n",
    "            # in compute_forward(). \n",
    "            predictions = asr_brain.compute_forward(batch, stage=sb.Stage.TEST)\n",
    "            ctc_probs = predictions['ctc_logprobs'] # FOR DEBUG\n",
    "\n",
    "            # hack probabilities to set all probs to 0 for whitespace\n",
    "            if hack_whitespace_probs:\n",
    "                ctc_probs = set_whitespace_to_0_probability(ctc_probs, vocab, log_probs=True, whitespace_symbol=\"|\")\n",
    "\n",
    "            # get mel lens from wav len ratios since torch ctc decoder requires lens in frames\n",
    "            batch_max_len = predictions[\"ctc_logprobs\"].size(1)\n",
    "            bsz = predictions[\"ctc_logprobs\"].size(0)\n",
    "            mel_lens = torch.zeros(bsz)\n",
    "            for i, len_ratio in enumerate(asr_brain.feat_lens):\n",
    "                mel_lens[i] = int(torch.round(len_ratio * batch_max_len))\n",
    "            \n",
    "            predicted_ids = ctc_beamsearch_decoder(\n",
    "                predictions[\"ctc_logprobs\"], lengths=mel_lens\n",
    "            )\n",
    "\n",
    "            # iterate over samples in batch\n",
    "            for i, (utt_id, orig_text, hyps, wav) in enumerate(zip(batch.utt_id, orig_transcriptions, predicted_ids, wavs)):\n",
    "                sample_printout = f\"\\nsample {i+1} - ({utt_id}: '{orig_text}')\"\n",
    "                orig_utt_text = \"INSERT ORIG UTT TEXT\"\n",
    "                sample_printout += f\"\\nOriginal Utterance: {orig_utt_text}\"\n",
    "                # if print_info: print(f\"\\nsample {i+1} - ({utt_id}: '{orig_text}')\")\n",
    "                sample_cers = []\n",
    "                for j, hyp in enumerate(list(hyps)[:max_hyps_per_sample]):\n",
    "                    words = asr_brain.hparams.tokenizer.decode_ids(hyp.tokens.tolist())\n",
    "                    if collapse_whitespace:\n",
    "                        words = \"\".join(c for c in words if c != \" \")\n",
    "                    hyp_cer = 100 * cer(orig_text, words)\n",
    "                    sample_cers.append(hyp_cer)\n",
    "                    sample_printout += f\"\\n\\thyp {j+1}/{len(hyps)} (CER={hyp_cer:.1f}%): '{words}'\"\n",
    "                    # if print_info: print(f\"\\thyp {j+1}/{len(hyps)} (CER={hyp_cer:.1f}%): '{words}'\")\n",
    "                    if not wordtypes_to_transcribe or orig_text in wordtypes_to_transcribe:\n",
    "                        transcribed_words[orig_text].append(words)\n",
    "                    \n",
    "                sample_printout += f\"\\n\\t=== Mean CER: {np.mean(sample_cers):.1f}%, Std CER: {np.std(sample_cers):.1f}% ===\"\n",
    "                # if print_info: print(f\"\\t=== Mean CER: {np.mean(sample_cers):.1f}%, Std CER: {np.std(sample_cers):.1f}% ===\")\n",
    "\n",
    "                include_current_wordtype = (wordtypes_to_transcribe == [] or orig_text in wordtypes_to_transcribe)\n",
    "                if include_current_wordtype:\n",
    "                    orig_words.append(orig_text)\n",
    "                    printouts.append(sample_printout)\n",
    "                    orig_wavs.append(wav)\n",
    "                    token_ids.append(utt_id)\n",
    "\n",
    "    output_dict = {\n",
    "        \"orig_words\": orig_words,\n",
    "        \"transcribed_words\": transcribed_words,\n",
    "        \"wavs\": orig_wavs,\n",
    "        \"printouts\": printouts,\n",
    "        \"token_ids\": token_ids,\n",
    "    }\n",
    "\n",
    "    return output_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99df03d5",
   "metadata": {},
   "source": [
    "## Generate multiple times to see if outputs change (i.e. should change if dropout is enabled in model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "96603643-3a86-409a-badb-29d086535eee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|███████████████████████████▌                           | 1/2 [00:04<00:04,  4.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|███████████████████████████▌                           | 1/2 [00:04<00:04,  4.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added 6 new spellings for 'experimental' to its word set\n",
      "added 5 new spellings for 'depredators' to its word set\n",
      "added 3 new spellings for 'assignation' to its word set\n",
      "added 2 new spellings for 'plaintiffs' to its word set\n",
      "added 4 new spellings for 'subpoenaed' to its word set\n",
      "added 4 new spellings for 'phosphoric' to its word set\n",
      "added 6 new spellings for 'assurbanipal' to its word set\n",
      "added 3 new spellings for 'exhumation' to its word set\n",
      "added 2 new spellings for 'individually' to its word set\n",
      "added 4 new spellings for 'accelerator' to its word set\n",
      "\n",
      " *** Will synthesise these 39 spellings using TTS pipeline\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "wordtypes_to_transcribe = []\n",
    "# wordtypes_to_transcribe = ['anesthesiologists'] # used for debugging and checking results only for a subset of wordtypes\n",
    "\n",
    "word_sets = defaultdict(set)\n",
    "\n",
    "print_printout = False\n",
    "display_audio = False\n",
    "\n",
    "'''create output dict of form:\n",
    "token_id:\n",
    "    word: xxx\n",
    "    hyps:\n",
    "        xxx\n",
    "        xxx\n",
    "        xxx\n",
    "        ...\n",
    "    wav: /path/to/token_id.wav\n",
    "'''\n",
    "token_id2gt_audio = {}\n",
    "master_transcription_dict = defaultdict(dict)\n",
    "print_num_new_spellings_per_iteration = False\n",
    "\n",
    "for i in range(inference_hparams['num_asr_iterations']):\n",
    "    print(f\"Iteration {1+i}/{inference_hparams['num_asr_iterations']}\")\n",
    "    transcription_output_dict = transcribe_dataset(asr_brain, datasets[\"allsplits\"], \n",
    "                                                num_batches_to_transcribe=inference_hparams['num_batches_to_transcribe'],\n",
    "                                                collapse_whitespace=False, hack_whitespace_probs=True,\n",
    "                                                max_hyps_per_sample=inference_hparams['max_hyps_per_sample'],\n",
    "                                                wordtypes_to_transcribe=wordtypes_to_transcribe)\n",
    "    \n",
    "    for token_id, orig_word, printout, wav in zip(\n",
    "        transcription_output_dict[\"token_ids\"], \n",
    "        transcription_output_dict[\"orig_words\"], \n",
    "        transcription_output_dict[\"printouts\"], \n",
    "        transcription_output_dict[\"wavs\"]\n",
    "    ):\n",
    "        if print_printout:\n",
    "            print(printout)\n",
    "        if display_audio:\n",
    "            display(Audio(wav, rate=16000))\n",
    "        old_count = len(word_sets[orig_word])\n",
    "        asr_hypotheses = transcription_output_dict[\"transcribed_words\"][orig_word]\n",
    "        asr_hypotheses = [hyp.rstrip() for hyp in asr_hypotheses] # as CTC decoder always adds extra whitespace to end\n",
    "        word_sets[orig_word].update(asr_hypotheses)\n",
    "        master_transcription_dict[token_id][\"orig_word\"] = orig_word\n",
    "        master_transcription_dict[token_id][\"orig_wav\"] = wav.numpy() \n",
    "        token_id2gt_audio[token_id] = master_transcription_dict[token_id][\"orig_wav\"]\n",
    "        \n",
    "        if \"hyps\" not in master_transcription_dict[token_id]:\n",
    "            master_transcription_dict[token_id][\"hyps\"] = set(asr_hypotheses)\n",
    "        else:\n",
    "            master_transcription_dict[token_id][\"hyps\"].update(asr_hypotheses)\n",
    "        \n",
    "        new_count = len(word_sets[orig_word])\n",
    "        if new_count > old_count:\n",
    "            if print_num_new_spellings_per_iteration:\n",
    "                print(f\"added {new_count - old_count} new spellings for '{orig_word}' to its word set\")\n",
    "\n",
    "total_num = 0\n",
    "for orig_word, word_set in word_sets.items():\n",
    "    print(f\"added {len(word_set)} new spellings for '{orig_word}' to its word set\")\n",
    "    total_num += len(word_set)\n",
    "    \n",
    "print(f\"\\n *** Will synthesise these {total_num} spellings using TTS pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847e3594",
   "metadata": {},
   "source": [
    "# create dataframe to store respellings for each original spelling (and respective CERs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "8790788f-e03f-4763-ae5a-64ebbd687532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of token_id-spelling combinations 62\n"
     ]
    }
   ],
   "source": [
    "# create big dataframe to hold all data\n",
    "df = pd.DataFrame(columns=['token_id', 'wordtype', 'respelling', 'cer'])\n",
    "data = [] # used to populate pandas dataframe\n",
    "\n",
    "for token_id in master_transcription_dict:    \n",
    "    for hyp in master_transcription_dict[token_id]['hyps']:\n",
    "        data.append({'token_id': token_id, \n",
    "                     'wordtype': master_transcription_dict[token_id]['orig_word'], \n",
    "                     'respelling': hyp, \n",
    "                     'orig_wav': master_transcription_dict[token_id]['orig_wav'],\n",
    "                     'cer': 100*cer(master_transcription_dict[token_id]['orig_word'], hyp)}),\n",
    "\n",
    "df = pd.concat([df, pd.DataFrame(data)])\n",
    "print('number of token_id-spelling combinations', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "e7de5fd6-1b8a-4852-8bea-364b446bd0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of token_id-spelling combinations 68\n"
     ]
    }
   ],
   "source": [
    "# guarantee that original spelling will be generated by TTS\n",
    "# i.e. add orig_word to \"respelling\" column if it doesn't occur in it already\n",
    "dfs_to_concat = []\n",
    "for orig_word in df['wordtype'].unique():\n",
    "    match_mask = df['respelling'] == orig_word\n",
    "    matching_respellings = df.loc[match_mask]\n",
    "    if matching_respellings.empty:\n",
    "        # copy the first row, but change value for respelling\n",
    "        first_row_df = df.loc[df['wordtype'] == orig_word].head(1)\n",
    "        row_idx = first_row_df.index[0]\n",
    "        first_row_df.loc[row_idx, 'respelling'] = orig_word\n",
    "        first_row_df.loc[row_idx, 'cer'] = 0.0\n",
    "        dfs_to_concat.append(first_row_df)\n",
    "        # print(f'added original spelling to respelling column for \"{orig_word}\"')\n",
    "\n",
    "for temp_df in dfs_to_concat:\n",
    "    df = pd.concat([df, temp_df], ignore_index=True)\n",
    "print('number of token_id-spelling combinations', len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87e8270",
   "metadata": {},
   "source": [
    "# load pretrained fastpitch and hifigan models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "466a024c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_pretrained_tts_and_vocoder import load_vocoder, load_pretrained_fastpitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "de1a0f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hifigan_checkpoint = \"/home/s1785140/pretrained_models/hifigan/ljspeech/LJ_V1/generator_v1\"\n",
    "fastpitch_checkpoint = '/home/s1785140/respeller/fastpitch/exps/halved_ljspeech_data_nospaces_noeos_pad_lowercase_nopunc/FastPitch_checkpoint_1000.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "3d5fd0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hifigan = load_vocoder(hifigan_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "be3d2747",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastpitch, *_ = load_pretrained_fastpitch(fastpitch_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b22229",
   "metadata": {},
   "source": [
    "# run each respelling through TTS to get mel-spectrogram (and optionally vocode to get wav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "283f5dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastpitch.inference import prepare_input_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "ee8415ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create input to fastpitch batch maker\n",
    "all_orig_words = [orig_word for orig_word in df['wordtype']]\n",
    "all_respellings = [respelling for respelling in df['respelling']]\n",
    "fields = {'orig_spelling': all_orig_words, \n",
    "          'respelling': all_respellings,\n",
    "          'text': all_respellings} # will be encoded by text processes into IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "6bf1f8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading texts: 100%|████████████████████████████████████| 68/68 [00:00<00:00, 5826.61it/s]\n"
     ]
    }
   ],
   "source": [
    "# create batches for fastpitch inference\n",
    "batches = prepare_input_sequence(fields, device, input_type='char', \n",
    "                       symbol_set='english_pad_lowercase_nopunc', \n",
    "                       text_cleaners=['lowercase_no_punc'],\n",
    "                       batch_size=inference_hparams['fastpitch_inference_batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "e05a9e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def melspec2mfcc(mel, n_mfcc=12):\n",
    "    log_mel = librosa.core.amplitude_to_db(mel, ref=np.max)\n",
    "    mfcc = librosa.feature.mfcc(S=log_mel, n_mfcc=n_mfcc)\n",
    "    return mfcc\n",
    "\n",
    "def generate_audio(y, vocoder=None, hop_length=256):\n",
    "    \"\"\"Generate audio from spectrograms for n utterances in batch\"\"\"\n",
    "    mel, mel_lens = y\n",
    "    bs = mel.size(0)\n",
    "    with torch.no_grad():\n",
    "        audios = vocoder(mel).cpu().squeeze(1).numpy() # [bsz, dim, samples ]only squeeze away dim (equals 1 for waveform)\n",
    "        mel_lens = mel_lens.cpu().numpy() - 1\n",
    "    audios_to_return = []\n",
    "    for audio, mel_len in zip(audios, mel_lens):\n",
    "        audio = audio[:mel_len * hop_length]\n",
    "        audio = audio / np.max(np.abs(audio))\n",
    "        audios_to_return.append(audio)\n",
    "        \n",
    "    return audios_to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "39f8651d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "synthesising respellings: 100%|█████████████████████████████| 3/3 [00:00<00:00,  3.14it/s]\n"
     ]
    }
   ],
   "source": [
    "do_vocode = True\n",
    "\n",
    "spelling2tts_mel = {}\n",
    "spelling2tts_mfcc = {}\n",
    "spelling2vocoded_audio = {}\n",
    "audios = None\n",
    "\n",
    "with torch.no_grad():\n",
    "    for b in tqdm(batches, \"synthesising respellings\"):\n",
    "        mel, mel_lens, *_ = fastpitch(b['text'])\n",
    "        \n",
    "        if do_vocode:\n",
    "            vocoder_inputs = (mel, mel_lens)\n",
    "            audios = generate_audio(vocoder_inputs, hifigan)\n",
    "            \n",
    "        # add samples in batch to dicts\n",
    "        bsz = len(mel)\n",
    "        for i in range(bsz):\n",
    "            respelling = b['respelling'][i]\n",
    "            mel_len = mel_lens[i]\n",
    "            spelling2tts_mel[respelling] = mel[i, :, :mel_len].cpu().numpy() # mel [bsz, feats, time]\n",
    "            spelling2tts_mfcc[respelling] = melspec2mfcc(spelling2tts_mel[respelling])\n",
    "            \n",
    "            if audios is not None:\n",
    "                spelling2vocoded_audio[respelling] = audios[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ceb55c-6815-4cd2-a553-f23b6cedb920",
   "metadata": {},
   "source": [
    "## Optionally encode TTS output into Hubert representations (continuous or discrete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "64236bca-4c97-403c-8740-97d0c842451a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['rsweresuppeenat', 'nexperiamental', 'rsweresupeanet', 'rsweresuppenat', 'icxcellerator', 'experamentale', 'ecxcellerator', 'nexperamental', 'rsweresupenet', 'assurbanipal', 'individually', 'experamental', 'experimental', 'ixperimental', 'excellerater', 'excellerator', 'depredoturs', 'depredators', 'depreditors', 'deprediturs', 'extcumation', 'individualy', 'exccumation', 'aserbonypul', 'aserbonypol', 'asingnation', 'assignation', 'aserbonypal', 'aserbonypow', 'accelerator', 'exhumation', 'phosphoric', 'subpoenaed', 'plaintiffs', 'excumation', 'dpreditors', 'asignation', 'aserbonypo', 'plaintivfs', 'plaintivs', 'aserbnypo', 'fhosforac', 'thosforac', 'fosforac', 'fosforic'])"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spelling2vocoded_audio.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "21ffab4f-ba8b-4759-9f54-4c7e992e6d97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([21760])\n",
      "torch.Size([1, 1, 21760])\n",
      "torch.Size([1, 1, 15790])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/s1785140/.cache/torch/hub/bshall_hubert_main\n"
     ]
    }
   ],
   "source": [
    "import torch, torchaudio\n",
    "\n",
    "resampler = torchaudio.transforms.Resample(orig_freq=22050,\n",
    "                                               new_freq=16000).cuda()\n",
    "\n",
    "wav = torch.tensor(spelling2vocoded_audio['experamentale'])\n",
    "\n",
    "print(wav.size())\n",
    "wav = wav.unsqueeze(0).unsqueeze(0).cuda()\n",
    "print(wav.size())\n",
    "wav = resampler(wav)\n",
    "print(wav.size())\n",
    "\n",
    "hubert = torch.hub.load(\"bshall/hubert:main\", \"hubert_discrete\").cuda() # input sample rate should be 16khz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "001a474d-bf72-4ce8-a864-3f47f781a21e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([49])"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "units = hubert.units(wav)\n",
    "units.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "ac20d213-a54a-4c2e-adca-39392bfaa722",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6, 66,  0,  0, 33, 33, 33, 33, 47, 36, 30, 30, 30, 34, 34, 74, 94, 94,\n",
       "         8,  8,  8,  0, 49, 49, 25, 25, 95, 23, 23,  1,  1, 26, 26, 63, 38, 38,\n",
       "        35, 35, 77, 77, 77, 41, 41, 41, 33, 33, 78, 74,  6], device='cuda:0')"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "f43f0871-2696-4e64-9a92-e43a2b3b0e84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "codes = units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "598152e0-2545-4fff-b8aa-c94804409402",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "centroids = hubert.kmeans.cluster_centers_\n",
    "units = [centroids[code] for code in codes] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "f3423f95-2c07-46fb-8b48-2079e548cfa0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "units[0].shape # first time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "5b1052c7-715a-47ba-97a7-135a27c39f61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "units_array = np.stack(units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "1d571d34-e66c-45a2-b50a-548889791f7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49, 768)"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "units_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "ea60eee8-cd14-4469-9c5a-f75d50fefdec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def codes2centroids(code_seq):\n",
    "    assert codes.dim() == 1\n",
    "    centroids = hubert.kmeans.cluster_centers_\n",
    "    centroid_seq = [centroids[code] for code in code_seq] \n",
    "    stacked_centroid_seq = np.stack(centroid_seq)\n",
    "    assert stacked_centroid_seq.shape[0] == len(centroid_seq)\n",
    "    return stacked_centroid_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "1ca5777d-59d2-479c-ab7d-026e5d133519",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def wav2ssl_feats(wav):\n",
    "    # print(wav.size())\n",
    "    wav = wav.unsqueeze(0).unsqueeze(0).cuda()\n",
    "    # print(wav.size())\n",
    "    wav = resampler(wav)\n",
    "    # print(wav.size())\n",
    "\n",
    "    # Extract speech units\n",
    "    if inference_hparams['hubert_type'] == 'raw':\n",
    "        feats = hubert.encode(wav, layer=output_layer)[0].detach()\n",
    "        # print(feats.size())\n",
    "    else:\n",
    "        feats = hubert.units(wav)\n",
    "        # print('111', feats.size())\n",
    "\n",
    "        if inference_hparams['hubert_type'] == 'discrete' and inference_hparams['discrete_type'] == 'centroid':\n",
    "            # convert from sequence of discrete codes to sequence of centroid vectors that correspond to the discrete codes\n",
    "            # print(\"handling discrete centroids!!!\")\n",
    "            feats = codes2centroids(feats)\n",
    "        # print('222', feats.size())\n",
    "\n",
    "    # print('333', feats.shape, type(feats))\n",
    "\n",
    "    # standardise feats by removing batch dimension, and converting to numpy arrays\n",
    "    if feats.dim() == 3: # likely [b_sz, timesteps, feats]\n",
    "        # squeeze away batchsize dimension\n",
    "        feats = feats.squeeze(0)\n",
    "        assert feats.dim() == 2\n",
    "    if type(feats) == torch.Tensor:\n",
    "        feats = feats.cpu().numpy()\n",
    "\n",
    "    # print('444', feats.shape, type(feats))\n",
    "    \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "85b5f4d6-1ca2-4a2c-8df7-1163f9adb2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference_hparams['distance_input_reprs']='hubert'\n",
      "inference_hparams['hubert_type']='raw'\n",
      "inference_hparams['discrete_type']='code'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/s1785140/.cache/torch/hub/bshall_hubert_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting TTS synthesised spellings into Hubert features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 45/45 [00:00<00:00, 96.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting GT word token recordings into Hubert features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 14/14 [00:00<00:00, 96.84it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch, torchaudio\n",
    "\n",
    "spelling2tts_hubert = {}\n",
    "token_id2gt_hubert = {}\n",
    "output_layer = 7 # from paper (A COMPARISON OF DISCRETE AND SOFT SPEECH UNITS FOR IMPROVED VOICE CONVERSION)\n",
    "\n",
    "print(f\"{inference_hparams['distance_input_reprs']=}\")\n",
    "print(f\"{inference_hparams['hubert_type']=}\")\n",
    "print(f\"{inference_hparams['discrete_type']=}\")\n",
    "\n",
    "if inference_hparams['distance_input_reprs'] == \"hubert\":      \n",
    "    # Load checkpoint (either hubert_soft or hubert_discrete)\n",
    "    if inference_hparams['hubert_type'] in ['soft', 'raw']: # NB raw feats can be extracted from hubert_soft or discrete\n",
    "        hubert = torch.hub.load(\"bshall/hubert:main\", \"hubert_soft\").cuda() # input sample rate should be 16khz\n",
    "    elif inference_hparams['hubert_type'] == 'discrete':\n",
    "        hubert = torch.hub.load(\"bshall/hubert:main\", \"hubert_discrete\").cuda() # input sample rate should be 16khz\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "    resampler = torchaudio.transforms.Resample(orig_freq=22050,\n",
    "                                               new_freq=16000).cuda()\n",
    "\n",
    "    print(\"Converting TTS synthesised spellings into Hubert features...\")\n",
    "    for spelling in tqdm(spelling2vocoded_audio):\n",
    "        wav = torch.tensor(spelling2vocoded_audio[spelling])\n",
    "        feats = wav2ssl_feats(wav)\n",
    "        spelling2tts_hubert[spelling] = feats\n",
    "        \n",
    "    print(\"Converting GT word token recordings into Hubert features...\")\n",
    "    for token_id in tqdm(token_id2gt_audio):\n",
    "        wav = torch.tensor(token_id2gt_audio[token_id])\n",
    "        feats = wav2ssl_feats(wav)\n",
    "        token_id2gt_hubert[spelling] = feats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e72723b-3449-4ad1-bd62-a48b001d09fe",
   "metadata": {},
   "source": [
    "# create mapping from token_id to ground truth fastpitch mels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e267003-b7f8-4763-abbc-8a26af0db137",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#build mapping from token_id to path to its gt word aligned mel \n",
    "word_aligned_feats_path = '/home/s1785140/data/ljspeech_fastpitch/wordaligned_mels_lowercase_nopunc'\n",
    "token_id2fastpitch_gt_mel_path = {}\n",
    "\n",
    "for token_id in df['token_id'].unique():\n",
    "    wordtype = token_id.split('__')[0]\n",
    "    path = os.path.join(word_aligned_feats_path, wordtype, token_id + '.pt')\n",
    "    token_id2fastpitch_gt_mel_path[token_id] = path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da5d358-e3ba-4574-962a-a9a0bbfec2f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load mels from disk\n",
    "token_id2fastpitch_gt_mel = {}\n",
    "\n",
    "for token_id, gt_mel_path in token_id2fastpitch_gt_mel_path.items():\n",
    "    # remove len and file extension from path\n",
    "    # e.g. 'wordaligned_mels/unconstitutionality/unconstitutionality__LJ021-0191__occ1__len25760.wav' \n",
    "    # ---> 'wordaligned_mels/unconstitutionality/unconstitutionality__LJ021-0191__occ1\"\n",
    "    path_no_len_no_ext = gt_mel_path.split('__len')[0]\n",
    "    \n",
    "    # glob to fine the matching ground truth mel saved on disk\n",
    "    matches = glob.glob(path_no_len_no_ext + '*.pt')\n",
    "    assert len(matches) == 1\n",
    "    fastpitch_gt_mel = torch.load(matches[0]).transpose(0,1).cpu().numpy()\n",
    "    token_id2fastpitch_gt_mel[token_id] = fastpitch_gt_mel\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0768f426-16a5-47ea-9469-d94d385236e9",
   "metadata": {},
   "source": [
    "# Verify that predicted mel-spectrograms are normalised in same way as GT mel-spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708ff4a6-2b89-4541-a7e0-9761f46fd525",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_melspec(mel, title=\"\"):\n",
    "    if type(mel) == torch.Tensor:\n",
    "        mel = mel.cpu().numpy()\n",
    "    fig, ax = plt.subplots(figsize=(10, 2))\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.imshow(mel, aspect='auto', origin='lower', interpolation='none')\n",
    "    plt.colorbar()\n",
    "    \n",
    "def pad_along_axis(array: np.ndarray, target_length: int, axis: int = 0, constant_values=0) -> np.ndarray:\n",
    "    # Pad the data with NaN values at both ends\n",
    "    pad_size = target_length - array.shape[axis]\n",
    "    assert pad_size % 2 == 0\n",
    "\n",
    "    if pad_size <= 0:\n",
    "        return array\n",
    "\n",
    "    npad = [(0, 0)] * array.ndim\n",
    "    npad[axis] = (int(pad_size/2), int(pad_size/2))\n",
    "\n",
    "    return np.pad(array, pad_width=npad, mode='constant', constant_values=constant_values)\n",
    "\n",
    "def wma_smooth(mel, weights, dim_to_smooth=0, maintain_time_dim=False, extra_padding=0):\n",
    "    # mel should be shape [t, dim]\n",
    "    if dim_to_smooth == 0:\n",
    "        data = mel.transpose(0,1)\n",
    "    elif dim_to_smooth == 1:\n",
    "        data = mel\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "    weights = weights / np.sum(weights)\n",
    "    window = weights.shape[0]\n",
    "    dim = data.shape[1]\n",
    "        \n",
    "    # stack along feat dimension\n",
    "    stacked_weights = [weights for _ in range(dim)]\n",
    "    stacked_weights = np.stack(stacked_weights, axis=-1)\n",
    "    weights = stacked_weights\n",
    "\n",
    "    # pad data\n",
    "    target_len_after_padding = data.shape[0] + 2 * (window // 2) + 2 * extra_padding\n",
    "    padded_data = pad_along_axis(data, \n",
    "                                 target_length=target_len_after_padding, \n",
    "                                 axis=0, \n",
    "                                 constant_values=data.min()) # use data.min() as padding val because for log mel, 0 is actually large!!!\n",
    "\n",
    "    # Compute the weighted moving average \n",
    "    N = len(padded_data) - window + 1\n",
    "    wma = np.array([np.sum(weights * padded_data[i:i+window], axis=0) / np.sum(np.nan_to_num(weights), axis=0) for i in range(N)])\n",
    "    \n",
    "    if maintain_time_dim:\n",
    "        assert wma.shape == data.shape, f\"{wma.shape=} == {data.shape=}\"\n",
    "    else:\n",
    "        assert wma.shape[1] == data.shape[1], f\"{wma.shape[1]=} == {data.shape[1]=}\"\n",
    "    \n",
    "    return wma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73aad651-be15-4b67-84d5-372c229d179a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot GT mel spectrogram\n",
    "token_id = list(token_id2fastpitch_gt_mel.keys())[0]\n",
    "wordtype = token_id.split('__')[0]\n",
    "gt_mel = token_id2fastpitch_gt_mel[token_id]\n",
    "\n",
    "plot_melspec(gt_mel, token_id + \" - ORIGINAL!!! (no weighted moving average)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cd27a1-d66a-4ca0-93c8-86599d56abbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filter = [1,2,1]\n",
    "wma = wma_smooth(gt_mel.T, weights=filter).T\n",
    "plot_melspec(wma, token_id + f\" weights={filter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2eb4c4-a396-4a49-8e87-c8358156c436",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "temporal_filter = [1,2,1]\n",
    "feature_filter = [1,1,2,1,1]\n",
    "wma = wma_smooth(gt_mel.T, dim_to_smooth=0, weights=temporal_filter).T\n",
    "wma = wma_smooth(wma.T, dim_to_smooth=1, weights=feature_filter).T\n",
    "plot_melspec(wma, token_id + f\" weights={filter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d9b976-05b3-4395-b665-3ba2908ea471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot predicted mel spectrogram\n",
    "mel = spelling2tts_mel[wordtype]\n",
    "\n",
    "print(wordtype)\n",
    "plot_melspec(mel, wordtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0354f2-2420-4f02-8b25-8bcd77e4b731",
   "metadata": {},
   "source": [
    "# convert mel spec to Mel-Cepstrum then plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d310587d-3272-45bd-9a7a-3fb1c3a9f3ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_mfcc(mfcc, extra_title=\"\"):\n",
    "    # assume mfcc is a numpy array of shape (n_mfcc, n_frames)\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.imshow(mfcc, aspect='auto', origin='lower', cmap='coolwarm')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('MFCC')\n",
    "    title = 'MFCC'\n",
    "    if extra_title:\n",
    "        title += f\": {extra_title}\"\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f165b8cd-2f83-496b-ba2a-12e23870dcd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mfcc = melspec2mfcc(mel)\n",
    "plot_mfcc(mfcc, wordtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dca6c5-0b3a-4316-b98a-981b0e8e941a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gt_mfcc = melspec2mfcc(gt_mel)\n",
    "plot_mfcc(gt_mfcc, token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6adef8-4070-4f84-be7e-64a05a9b693d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# (optional) apply weighted moving average filters to GT mels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3795ab2c-6a07-426e-a3cb-7175104ce5a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# temporal_filter = []\n",
    "# feature_filter = []\n",
    "temporal_filter = [1,2,1]\n",
    "feature_filter = [1,1,2,1,1]\n",
    "\n",
    "for token_id, fastpitch_gt_mel in token_id2fastpitch_gt_mel.items():\n",
    "    if temporal_filter:\n",
    "        fastpitch_gt_mel = wma_smooth(fastpitch_gt_mel.T, dim_to_smooth=0, weights=temporal_filter).T\n",
    "    if feature_filter:\n",
    "        fastpitch_gt_mel = wma_smooth(fastpitch_gt_mel.T, dim_to_smooth=1, weights=feature_filter).T    \n",
    "        \n",
    "    token_id2fastpitch_gt_mel[token_id] = fastpitch_gt_mel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcc40c1-9008-429e-b0f4-19a4f6fa8a24",
   "metadata": {
    "tags": []
   },
   "source": [
    "# convert GT mels to MFCCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b715362-3cc2-44ec-9ad7-3bb046361998",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "token_id2fastpitch_gt_mfcc = {}\n",
    "for token_id, fastpitch_gt_mel in token_id2fastpitch_gt_mel.items():\n",
    "    token_id2fastpitch_gt_mfcc[token_id] = melspec2mfcc(fastpitch_gt_mel)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e692e0",
   "metadata": {},
   "source": [
    "# add acoustic distances between ground truth audio for word and synthesised respellings\n",
    "\n",
    "compare ranking by:\n",
    "- MCD-DTW\n",
    "- SoftDTW\n",
    "- L1, L2 distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3a8ac1-e268-4fc1-830a-f2767f05a71c",
   "metadata": {},
   "source": [
    "## Setup MCD-DTW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f858759b-4722-44ac-b8dd-2eb26a602e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mcd\n",
    "from mcd import dtw\n",
    "\n",
    "def eucCost(x, y):\n",
    "    \"\"\"AKA l2 norm\"\"\"\n",
    "    return math.sqrt(np.inner(x - y, x - y))\n",
    "\n",
    "def manCost(x, y):\n",
    "    \"\"\"AKA l1 norm\"\"\"\n",
    "    return np.sum(np.inner(x - y, x - y))\n",
    "\n",
    "metric_registry = {\n",
    "}\n",
    "\n",
    "metric_type = \"mcd_dtw\"\n",
    "\n",
    "distance2costfn = {\n",
    "    \"euclidean\": eucCost,\n",
    "    \"manhattan\": manCost,\n",
    "}\n",
    "\n",
    "distance_type = \"euclidean\"\n",
    "\n",
    "distance_input_reprs = inference_hparams['distance_input_reprs']\n",
    "\n",
    "distance_metric_col_name = f'{metric_type} ({distance_type}) [{distance_input_reprs}]'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96917d5b-35fd-47c9-98f8-7be21406f1f3",
   "metadata": {},
   "source": [
    "## MCD-DTW between speech representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fd5ddf-9c2c-46c6-978e-b7820d730ca7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculate MCD-DTW distance for each synthesised respelling in the dataframe\n",
    "# to the gt speech representation for the token_id that the respelling was generated from (using ASR)\n",
    "print(f\"calculating MCD-DTW distances between GT and predicted mels using {distance_type}\")\n",
    "\n",
    "for row_idx in tqdm(df.index):\n",
    "    if distance_input_reprs == 'mfcc':\n",
    "        respelling_speech_rep = spelling2tts_mfcc[df.loc[row_idx, 'respelling']].T # TODO avoid computing spelling2tts_mfcc if we don't use mfccs?\n",
    "        gt_speech_rep = token_id2fastpitch_gt_mfcc[df.loc[row_idx, 'token_id']].T\n",
    "        \n",
    "    elif distance_input_reprs == 'hubert':\n",
    "        respelling_speech_rep = spelling2tts_hubert[df.loc[row_idx, 'respelling']].T # TODO avoid computing spelling2tts_mfcc if we don't use mfccs?\n",
    "        gt_speech_rep = token_id2gt_hubert[df.loc[row_idx, 'token_id']].T\n",
    "        \n",
    "    elif distance_input_reprs == 'hubert_discrete':\n",
    "        raise NotImplemented\n",
    "    \n",
    "    distance_score, _path = dtw.dtw(gt_speech_rep, respelling_speech_rep, distance2costfn[distance_type])\n",
    "    df.loc[row_idx, distance_metric_col_name] = distance_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3674b74a",
   "metadata": {},
   "source": [
    "# play audios for each word, ranked by certain metric (e.g. CER, acoustic distance, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fed0e47-6825-4772-b634-e0f3fe76cb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort df by distance then spelling\n",
    "df = df.sort_values(['wordtype', distance_metric_col_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33eb9bf1-7ebe-4e1e-9266-4cb046b36956",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wordtypes = df[\"wordtype\"].unique()\n",
    "print(len(wordtypes))\n",
    "wordtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a895a9aa-2469-4121-bc97-7fd82e539329",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# optionally choose to only display a subset of the wordtypes\n",
    "\n",
    "words_to_include = None # include all words\n",
    "# words_to_include = [\n",
    "#     \"presenting\",\n",
    "#     \"admittedly\",\n",
    "# ]\n",
    "\n",
    "if words_to_include is not None:\n",
    "    filtered_df = pd.DataFrame()\n",
    "    for wordtype in words_to_include:\n",
    "        filtered_df = pd.concat([filtered_df, df.loc[df[\"wordtype\"] == wordtype]]) \n",
    "else:\n",
    "    filtered_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206e9434-6820-42f6-9d3a-f1ec05d060a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally only display N-best ranked respellings per original input audio (excluding original spellings!)\n",
    "get_smallest = True\n",
    "n_spellings_per_token_id = 5\n",
    "if get_smallest:\n",
    "    lambda_fn = lambda x: x.nsmallest(n_spellings_per_token_id, distance_metric_col_name)\n",
    "else:\n",
    "    lambda_fn = lambda x: x.nlargest(n_spellings_per_token_id, distance_metric_col_name)\n",
    "    \n",
    "original_spellings_mask = filtered_df['wordtype'] == filtered_df['respelling']\n",
    "original_spellings_df = filtered_df[original_spellings_mask]\n",
    "inverted_mask = ~original_spellings_mask\n",
    "no_orig_spellings_df = filtered_df[inverted_mask]\n",
    "filtered_df = no_orig_spellings_df.groupby('wordtype', group_keys=False).apply(lambda_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ebf00c-2183-4fe1-80bf-1fd1dce7eb3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add back original spellings and sort by distance again\n",
    "filtered_df = pd.concat([filtered_df, original_spellings_df], ignore_index=True)\n",
    "filtered_df = filtered_df.groupby('wordtype', group_keys=True).apply(lambda x: x.sort_values(distance_metric_col_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cffc8b-2e12-4022-8003-3e283c60db3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column to indicate whether it is original spelling or not\n",
    "new_col_idx = 1 + filtered_df.columns.get_loc('respelling') # insert after respelling column\n",
    "filtered_df.insert(new_col_idx, 'is_orig', filtered_df['wordtype'] == filtered_df['respelling'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ede93a8-da3c-47eb-9561-2ec3269fbefd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "current_wordtype = \"\"\n",
    "for token_id, wordtype, respelling, distance, orig_wav, is_orig in zip(filtered_df['token_id'], \n",
    "                                                              filtered_df['wordtype'], \n",
    "                                                              filtered_df['respelling'], \n",
    "                                                              filtered_df[distance_metric_col_name], \n",
    "                                                              filtered_df['orig_wav'],\n",
    "                                                              filtered_df['is_orig']):\n",
    "    # new wordtype, print GT token_ids and display audios\n",
    "    if current_wordtype != wordtype:\n",
    "        current_wordtype = wordtype\n",
    "        \n",
    "        # get all token_ids and orig_wavs for this wordtype\n",
    "        wordtype_df = filtered_df[filtered_df['wordtype'] == wordtype]\n",
    "        first_row_for_token_id = wordtype_df.groupby('token_id').nth(0)\n",
    "        \n",
    "        print(\"\\n===============================================================\")\n",
    "        print(f\"====== Original word aligned WAV(s) ===========================\")\n",
    "        print(\"===============================================================\")\n",
    "        for index, row in first_row_for_token_id.iterrows():\n",
    "            token_id = index # index is now equal to token_id as we used groupby above\n",
    "            print(f\"[GT - {token_id}]:\")\n",
    "            display(Audio(row['orig_wav'], rate=16000))\n",
    "        print(\"===============================================================\")\n",
    "    \n",
    "    # create string to print for this respelling\n",
    "    print_str = \"\"\n",
    "    if is_orig:\n",
    "        print_str += f\"{' '+wordtype+' ':*^35}\"\n",
    "    else:\n",
    "        print_str += f\" ---> {respelling=}\"\n",
    "    distance_str = f\" {distance:.2f} [{distance_metric_col_name}] [calc'ed vs GT: {token_id}]\"\n",
    "    print_str += distance_str\n",
    "    print(print_str)\n",
    "    \n",
    "    display(Audio(spelling2vocoded_audio[respelling], rate=22050))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d0751d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "toc-autonumbering": true,
  "toc-showtags": false,
  "vscode": {
   "interpreter": {
    "hash": "6a8422eeb13ba8f92f71047f64b5c33152e234c2bbad3f45433feda7b6f3b4c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
