{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4207e21b-d910-485b-8b9c-3fc7d5a59c5a",
   "metadata": {},
   "source": [
    "# Goal of this notebook\n",
    "\n",
    "Develop a training loop for finetuning ASR models using TTS loss by recreating RL training found in RL4LMs/rl4lms/envs/text_generation/training_utils.py\n",
    "\n",
    "```bash\n",
    "conda activate speller\n",
    "cd /home/s1785140/rlspeller\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70687ad-9f13-4cdf-8ef7-07eecb34f496",
   "metadata": {},
   "source": [
    "# automatic reloading magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3ef7885",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738aac35-fdd1-4452-aff0-92caa77ec5ab",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9c8a89c2-38ea-4778-8028-0c7854152301",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import List, Dict, Tuple, Any\n",
    "import hyperpyyaml\n",
    "from tqdm import tqdm\n",
    "from torchaudio.models.decoder import ctc_decoder\n",
    "from torch.nn.functional import softmax\n",
    "import random\n",
    "from jiwer import cer\n",
    "import numpy as np\n",
    "import speechbrain as sb\n",
    "from IPython.display import Audio\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa2cd37",
   "metadata": {},
   "source": [
    "## check if gpu available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53032f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greider.inf.ed.ac.uk\n"
     ]
    }
   ],
   "source": [
    "# print hostname to make sure we are on correct node\n",
    "disallowed_nodes = ['escience6']\n",
    "import socket\n",
    "hostname = socket.gethostname()\n",
    "print(hostname)\n",
    "node = hostname.split('.')[0]\n",
    "if node in disallowed_nodes:\n",
    "    raise ValueError(f\"Running on disallowed node {node}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f67b1b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.is_available()\n",
    "assert torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6f67235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/disk/nfs/ostrom/s1785140/rlspeller'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b236619f-6c04-44b1-a697-a342417a6694",
   "metadata": {},
   "source": [
    "# HPARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8a11879-640c-433e-a192-9c8b551d8ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_hparams = {\n",
    "    \"sentencepiece_model_path\": \"/home/s1785140/speechbrain/templates/speech_recognition_CharTokens_NoLM/Tokenizer/save/0_char.model\",\n",
    "\n",
    "    # 'speechbrain_hparams_file': '/home/s1785140/rlspeller/infer_speechbrain_subsampling1x_nowhitespace.yaml',\n",
    "    'speechbrain_hparams_file': '/home/s1785140/rlspeller/infer_speechbrain_subsampling1x.yaml',\n",
    "    # 'speechbrain_hparams_file': '/home/s1785140/rlspeller/infer_speechbrain_subsampling2x.yaml',\n",
    "    # 'speechbrain_hparams_file': '/home/s1785140/rlspeller/infer_speechbrain.yaml', # 4x subsampling\n",
    "\n",
    "    'sample_rate': 16000,\n",
    "    \n",
    "    'enable_dropout': True,\n",
    "    # 'dropout_p': 0.0, # 0.15 is the default value in speechbrain\n",
    "    'dropout_p': 0.15, # 0.15 is the default value in speechbrain\n",
    "    # 'dropout_p': 0.3, # 0.15 is the default value in speechbrain\n",
    "    # 'dropout_p': 0.5, # 0.15 is the default value in speechbrain\n",
    "    # 'dropout_p': 0.99, # 0.15 is the default value in speechbrain\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e877615-a6ff-4755-a7e4-e6f72e542432",
   "metadata": {},
   "source": [
    "# TOKENIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0505c38d-bc92-4f84-9f1f-cfc942ad337d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "# load pretrained tokenizer used to tokenizer ASR training inputs \n",
    "import sentencepiece as spm \n",
    "spm_path = inference_hparams[\"sentencepiece_model_path\"]\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(spm_path)\n",
    "print(sp.vocab_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4af21be-9224-4c45-87f6-b86a8d7c63ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 10 2 12 12 4 1 17 4 9 12 11 1 16 20 1 6 5 16 2 1 7 8 1 26 5 8 4 6\n"
     ]
    }
   ],
   "source": [
    "# test tokenizer\n",
    "s = \"hello world my name is jason\"\n",
    "# TODO pass string through text cleaners? \n",
    "encoded = sp.EncodeAsIds(s)\n",
    "assert 0 not in encoded, \"tried to encode an unknown character\"\n",
    "print(\" \".join(str(idx) for idx in encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe3c6048-79c5-47b4-9524-88d2e36ad8d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello world my name is jason'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.DecodeIds(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2996d1f1-d331-4919-8742-7907b46a46ce",
   "metadata": {},
   "source": [
    "# NEW! SIMPLE TOKENIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8951a84-e22e-4c62-a169-403a1b5269af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from speechbrain.tokenizers.SimpleTokenizer import SimpleTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfec85a8-d20d-445e-8483-cf27092e2962",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = SimpleTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c52e4b5-8d26-44c3-a1ad-c772b6feac62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello|my|name|is|jason\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[9, 6, 13, 13, 16, 1, 14, 26, 1, 15, 2, 14, 6, 1, 10, 20, 1, 11, 2, 20, 16, 15]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"hello my name is jason\"\n",
    "text = text.replace(' ', '|')\n",
    "print(text)\n",
    "ids = tokenizer.encode_as_ids(text)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5f1bf5e-500b-47ce-986c-3cbe059cdf41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello|my|name|is|jason'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode_ids(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5ea5f7-6224-4089-8136-5c4b8d47a2e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## test simple tokenizer with probability distribution, and see if CTC decoder successfully generates n-best lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a60ffb1b-6770-466c-90f6-6daa1dbe461b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create empty array of correct dimensions\n",
    "min_len, max_len = 50, 100\n",
    "bsz = 4\n",
    "lens = torch.randint(min_len, max_len, (bsz,))\n",
    "vocab_size = len(tokenizer.vocab)\n",
    "\n",
    "# randomly assign probaility distribution to each timestep\n",
    "\n",
    "# try to decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cac82712-8559-44d0-86d9-66cf1ddfbd5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "randn = torch.randn(bsz, max_len, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26e7e51c-bab1-4f2d-bb12-34f9f9c4454c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ctc_probs = softmax(randn, dim=1)\n",
    "# ctc_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f9abdc6-bd05-4c5c-a091-d04dadbaa9e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('sample 1, hyp 1/2', ['|bxjwjusxlgtnhngvkceu|yokslkiqjfqsbtqucpmqix|vyxfrerjkbswofpcixlzrpza|nlv|jvcarktm|'])\n",
      "('sample 1, hyp 2/2', ['|bxjwjusxlgtnhngvkceu|yokslkiqjfqsbtqucpmqix|vyxfregjkbswofpcixlzrpza|nlv|jvcarktm|'])\n",
      "('sample 2, hyp 1/2', ['|bam|glpxiuvoc|vgynytgxsknhctfogdncsnck|rnmlovot|'])\n",
      "('sample 2, hyp 2/2', ['|bam|glpxiuvoc|vgynytgxsknwctfogdncsnck|rnmlovot|'])\n",
      "('sample 3, hyp 1/2', ['|trbiukvjvljqfdaspbxbrmd|reolrgbgzbftpcgcibsnmwkyqmhtshts|'])\n",
      "('sample 3, hyp 2/2', ['|trbiukvjvljqfdaspbxbrmd|reolrgbgzbftpcgcibsnmwkyqmheshts|'])\n",
      "('sample 4, hyp 1/2', ['|cdlgtrygvsxqfuyulfstgbznomg|jklzelnjquendebviwapvxckrmcqshf|'])\n",
      "('sample 4, hyp 2/2', ['|cdlgtrygvsxqfuyulfstgbcnomg|jklzelnjquendebviwapvxckrmcqshf|'])\n"
     ]
    }
   ],
   "source": [
    "ctc_beamsearch_decoder_test = ctc_decoder(\n",
    "    lexicon=None,\n",
    "    # tokens=\"/home/s1785140/rlspeller/templates/speech_recognition_CharTokens_NoLM/Tokenizer/save/tokens.txt\",\n",
    "    tokens=tokenizer.vocab,\n",
    "    nbest=2,\n",
    "    blank_token='-',\n",
    "    sil_token=\"|\",\n",
    ")\n",
    "\n",
    "predicted_ids = ctc_beamsearch_decoder_test(ctc_probs, lens)\n",
    "\n",
    "predicted_words = []\n",
    "for i, hyps in enumerate(predicted_ids):\n",
    "    for j, hyp in enumerate(hyps):\n",
    "        words = tokenizer.decode_ids(hyp.tokens.tolist()).split(\" \")\n",
    "        tup = (f\"sample {i+1}, hyp {j+1}/{len(hyps)}\", words)\n",
    "        predicted_words.append(tup)\n",
    "        print(tup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd650fd4",
   "metadata": {},
   "source": [
    "# LOAD ASR (PRETRAINED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56bf160c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from templates.speech_recognition_CharTokens_NoLM.ASR.train import ASR\n",
    "from templates.speech_recognition_CharTokens_NoLM.ASR.train import dataio_prepare\n",
    "from torch.utils.data import DataLoader\n",
    "from speechbrain.dataio.dataloader import LoopedLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36d8019b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/s1785140/speechbrain/templates/speech_recognition_CharTokens_NoLM/data/rirs_noises.zip exists. Skipping download\n"
     ]
    }
   ],
   "source": [
    "# Load hyperparameters file with command-line overrides\n",
    "speechbrain_hparams_file = inference_hparams['speechbrain_hparams_file']\n",
    "with open(speechbrain_hparams_file) as f:\n",
    "    speechbrain_hparams = hyperpyyaml.load_hyperpyyaml(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26c15de1-25d9-428e-900f-967c3af9a8ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/s1785140/speechbrain/templates/speech_recognition_CharTokens_NoLM/ASR/results/CRDNN_CHAR_LJSpeech_halved_subsampling1x/2602/save'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speechbrain_hparams['save_folder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "896a3d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace some values with inference time values\n",
    "speechbrain_hparams['dropout'] = inference_hparams['dropout_p']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4072195-fb2b-4c9a-b201-c84b7a7ae104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise trainer (we don't want to train, but model is tightly coupled with trainer)\n",
    "asr_brain = ASR(\n",
    "    modules=speechbrain_hparams[\"modules\"],\n",
    "    opt_class=speechbrain_hparams[\"opt_class\"],\n",
    "    hparams=speechbrain_hparams,\n",
    "    checkpointer=speechbrain_hparams[\"checkpointer\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e8b676b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if on_evaluate_start() get runtime error, likely need to restart notebook kernel\n",
      "Enabled dropout in model:\n",
      "changed dropout p of encoder-CNN.block_0.drop.drop-Dropout2d(p=0.15, inplace=False) from 0.15 to 0.15\n",
      "changed dropout p of encoder-CNN.block_1.drop.drop-Dropout2d(p=0.15, inplace=False) from 0.15 to 0.15\n",
      "changed dropout p of encoder-RNN.rnn-LSTM(2560, 512, num_layers=2, batch_first=True, dropout=0.15, bidirectional=True) from 0.15 to 0.15\n",
      "changed dropout p of encoder-DNN.block_0.dropout-Dropout(p=0.15, inplace=False) from 0.15 to 0.15\n",
      "changed dropout p of encoder-DNN.block_1.dropout-Dropout(p=0.15, inplace=False) from 0.15 to 0.15\n"
     ]
    }
   ],
   "source": [
    "def change_dropout_p(brain, new_dropout_p):\n",
    "    for module_name in brain.modules.keys():\n",
    "        module = brain.modules[module_name]\n",
    "        for submodule_name, submodule in module.named_modules():\n",
    "            if isinstance(submodule, torch.nn.Dropout) or isinstance(submodule, torch.nn.Dropout2d):\n",
    "                # handle proper dropout submodules\n",
    "                print(f\"changed dropout p of {module_name}-{submodule_name}-{submodule} from {submodule.p} to {new_dropout_p}\")\n",
    "                submodule.p = new_dropout_p\n",
    "            else:\n",
    "                # handle dropout in nn.LSTM\n",
    "                for attr in [\"dropout\", \"drop\"]:\n",
    "                    if hasattr(submodule, attr) and type(getattr(submodule, attr)) == float: \n",
    "                        print(f\"changed dropout p of {module_name}-{submodule_name}-{submodule} from {getattr(submodule, attr)} to {new_dropout_p}\")\n",
    "                        setattr(submodule, attr, new_dropout_p)\n",
    "\n",
    "def setup_asr_brain_for_infer(asr_brain, enable_dropout=False, new_dropout_p=0.15):\n",
    "    asr_brain.on_evaluate_start(min_key=\"WER\") # We call the on_evaluate_start that will load the best model\n",
    "    if enable_dropout:\n",
    "        asr_brain.modules.train()\n",
    "        print(\"Enabled dropout in model:\")\n",
    "        change_dropout_p(asr_brain, new_dropout_p)\n",
    "    else:\n",
    "        asr_brain.modules.eval() # We set the model to eval mode (remove dropout etc)\n",
    "\n",
    "print(\"if on_evaluate_start() get runtime error, likely need to restart notebook kernel\")\n",
    "setup_asr_brain_for_infer(asr_brain, enable_dropout=inference_hparams[\"enable_dropout\"], new_dropout_p=inference_hparams[\"dropout_p\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "909b3c39-3a03-4384-82ca-2305b3ed2272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset and dataloader for inference\n",
    "datasets = dataio_prepare(speechbrain_hparams)\n",
    "\n",
    "test_set = datasets['test']\n",
    "\n",
    "if not isinstance(test_set, DataLoader) or isinstance(test_set, LoopedLoader):\n",
    "    test_loader_kwargs=speechbrain_hparams[\"test_dataloader_opts\"]\n",
    "    test_set = asr_brain.make_dataloader(\n",
    "        test_set, stage=sb.Stage.TEST, **test_loader_kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "69b4b4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ⁇ ', '', 'e', 't', 'o', 'a', 'n', 'i', 's', 'r', 'h', 'd', 'l', 'c', 'f', 'u', 'm', 'w', 'p', 'g', 'y', 'b', 'v', 'k', 'x', 'q', 'j', 'z']\n",
      "['-', '|', 'e', 't', 'o', 'a', 'n', 'i', 's', 'r', 'h', 'd', 'l', 'c', 'f', 'u', 'm', 'w', 'p', 'g', 'y', 'b', 'v', 'k', 'x', 'q', 'j', 'z']\n"
     ]
    }
   ],
   "source": [
    "# get vocab from tokenizer (needed for ctc decoding)\n",
    "vocab_size = len(asr_brain.hparams.tokenizer)\n",
    "vocab = []\n",
    "for i in range(vocab_size):\n",
    "    vocab.append(asr_brain.hparams.tokenizer.decode_ids([i]))\n",
    "print(vocab)\n",
    "\n",
    "# edit vocab to match default ctc decoder symbols for blank and silence\n",
    "vocab[0] = '-'\n",
    "vocab[1] = \"|\"\n",
    "\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6af49d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctc_beamsearch_decoder = ctc_decoder(\n",
    "    lexicon=None,\n",
    "    # tokens=\"/home/s1785140/rlspeller/templates/speech_recognition_CharTokens_NoLM/Tokenizer/save/tokens.txt\",\n",
    "    tokens=vocab,\n",
    "    nbest=100,\n",
    "    blank_token='-',\n",
    "    sil_token=\"|\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b4d6501c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate transcriptions for all batches in test set\n",
    "def transcribe_dataset(asr_brain, dataset, greedy=False, num_batches_to_transcribe=None):\n",
    "    # Now we iterate over the dataset and we simply compute_forward and decode\n",
    "    with torch.no_grad():\n",
    "        transcripts = []\n",
    "        for batch in tqdm(list(dataset)[:num_batches_to_transcribe], dynamic_ncols=True):\n",
    "            orig_transcriptions = batch.words\n",
    "\n",
    "            # Make sure that your compute_forward returns the predictions !!!\n",
    "            # In the case of the template, when stage = TEST, a beam search is applied \n",
    "            # in compute_forward(). \n",
    "            predictions = asr_brain.compute_forward(batch, stage=sb.Stage.TEST)\n",
    "            \n",
    "            ctc_probs = predictions['ctc_logprobs'] # FOR DEBUG\n",
    "\n",
    "            if greedy:\n",
    "                predicted_ids = sb.decoders.ctc_greedy_decode(\n",
    "                    predictions[\"ctc_logprobs\"], asr_brain.feat_lens, blank_id=asr_brain.hparams.blank_index\n",
    "                )\n",
    "                predicted_words = [\n",
    "                    asr_brain.tokenizer.decode_ids(ids).split(\" \")\n",
    "                    for ids in predicted_ids\n",
    "                ]\n",
    "            else:\n",
    "                # get mel lens from wav len ratios since torch ctc decoder requires lens in frames\n",
    "                batch_max_len = predictions[\"ctc_logprobs\"].size(1)\n",
    "                bsz = predictions[\"ctc_logprobs\"].size(0)\n",
    "                mel_lens = torch.zeros(bsz)\n",
    "                for i, len_ratio in enumerate(asr_brain.feat_lens):\n",
    "                    mel_lens[i] = int(torch.round(len_ratio * batch_max_len))\n",
    "                \n",
    "                predicted_ids = ctc_beamsearch_decoder(\n",
    "                    predictions[\"ctc_logprobs\"], lengths=mel_lens\n",
    "                )\n",
    "\n",
    "                predicted_words = []\n",
    "                for i, (utt_id, orig_text, hyps) in enumerate(zip(batch.utt_id, orig_transcriptions, predicted_ids)):\n",
    "                    print(f\"\\nsample {i+1} - ({utt_id}: '{orig_text}')\")\n",
    "                    sample_cers = []\n",
    "                    for j, hyp in enumerate(hyps):\n",
    "                        words = asr_brain.hparams.tokenizer.decode_ids(hyp.tokens.tolist()) # .split(\"|\")\n",
    "                        # words = tokenizer.decode_ids(hyp.tokens.tolist()) # .split(\"|\")\n",
    "                        hyp_cer = 100 * cer(orig_text, words)\n",
    "                        sample_cers.append(hyp_cer)\n",
    "                        print(f\"\\thyp {j+1}/{len(hyps)} (CER={hyp_cer:.1f}%): '{words}'\")\n",
    "                        predicted_words.append((f\"sample {i+1}, hyp {j+1}/{len(hyps)}\", words))\n",
    "                        \n",
    "                    print(f\"\\t=== Mean CER: {np.mean(sample_cers):.1f}%, Std CER: {np.std(sample_cers):.1f}% ===\")\n",
    "\n",
    "            transcripts.append(predicted_words)\n",
    "\n",
    "    return transcripts, ctc_probs\n",
    "\n",
    "# transcripts, ctc_probs = transcribe_dataset(asr_brain, test_set, greedy=False, num_batches_to_transcribe=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf186366",
   "metadata": {},
   "source": [
    "# LOAD WORD ALIGNED WAVS into dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c91aad24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imitate CLAs\n",
    "import sys\n",
    "import argparse\n",
    "import math\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c0e87365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set these in yaml config!\n",
    "\n",
    "# these are to be loaded!\n",
    "train_annotation_path = '/home/s1785140/speechbrain/templates/speech_recognition_CharTokens_NoLM/data/respeller_train_wordtoken_wav_annotation.json'\n",
    "valid_annotation_path = '/home/s1785140/speechbrain/templates/speech_recognition_CharTokens_NoLM/data/respeller_valid_wordtoken_wav_annotation.json'\n",
    "test_annotation_path = '/home/s1785140/speechbrain/templates/speech_recognition_CharTokens_NoLM/data/respeller_test_wordtoken_wav_annotation.json'\n",
    "\n",
    "# this will be saved!\n",
    "allsplits_annotation_path = '/home/s1785140/speechbrain/templates/speech_recognition_CharTokens_NoLM/data/respeller_allsplits_wordtoken_wav_annotation.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "37bbdf2d-1221-49fe-ae1b-26c4479d40f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a new \"combined annotation\", that includes all of train, valid and test\n",
    "def load_json(p):\n",
    "    with open(p) as f:\n",
    "        d = json.load(f)\n",
    "    return d\n",
    "\n",
    "allsplits_d = {}\n",
    "for p in [train_annotation_path, valid_annotation_path, test_annotation_path]:\n",
    "    allsplits_d.update(load_json(p))\n",
    "    \n",
    "json_data = json.dumps(allsplits_d)\n",
    "with open(allsplits_annotation_path, 'w') as f:\n",
    "    f.write(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "06edab0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "speechbrain_hparams['train_annotation'] = train_annotation_path\n",
    "speechbrain_hparams['valid_annotation'] = valid_annotation_path\n",
    "speechbrain_hparams['test_annotation'] = test_annotation_path\n",
    "speechbrain_hparams['allsplits_annotation'] = allsplits_annotation_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a6adcc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataio_prepare(\n",
    "    hparams, \n",
    "    split2data=None # use to optionally initialise datasets from data dicts rather than json files\n",
    "):\n",
    "    \"\"\"This function prepares the datasets to be used in the brain class.\n",
    "    It also defines the data processing pipeline through user-defined functions.\n",
    "\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    hparams : dict\n",
    "        This dictionary is loaded from the `train.yaml` file, and it includes\n",
    "        all the hyperparameters needed for dataset construction and loading.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    datasets : dict\n",
    "        Dictionary containing \"train\", \"valid\", and \"test\" keys that correspond\n",
    "        to the DynamicItemDataset objects.\n",
    "    \"\"\"\n",
    "    # Define audio pipeline. In this case, we simply read the path contained\n",
    "    # in the variable wav with the audio reader.\n",
    "    # wav path like: data/ljspeech_wavs_16khz_word_aligned/differs/differs__LJ001-0001__occ1__len8320.wav\n",
    "    @sb.utils.data_pipeline.takes(\"wav\")\n",
    "    @sb.utils.data_pipeline.provides(\"sig\", \"wav_path\", \"utt_id\")\n",
    "    def audio_pipeline(wav_path):\n",
    "        \"\"\"Load the audio signal. This is done on the CPU in the `collate_fn`.\"\"\"\n",
    "        sig = sb.dataio.dataio.read_audio(wav_path)\n",
    "        yield sig\n",
    "\n",
    "        yield wav_path\n",
    "\n",
    "        utt_id = wav_path.split(\"/\")[-1].split(\".\")[0]\n",
    "        yield utt_id\n",
    "\n",
    "    @sb.utils.data_pipeline.takes(\"samples_to_graphemes_ratio\")\n",
    "    @sb.utils.data_pipeline.provides(\"samples_to_graphemes_ratio\")\n",
    "    def ratio_pipeline(samples_to_graphemes_ratio):\n",
    "        yield samples_to_graphemes_ratio\n",
    "\n",
    "    @sb.utils.data_pipeline.takes(\"length\")\n",
    "    @sb.utils.data_pipeline.provides(\"length\")\n",
    "    def length_pipeline(length):\n",
    "        yield length\n",
    "\n",
    "    @sb.utils.data_pipeline.takes(\"words\")\n",
    "    @sb.utils.data_pipeline.provides(\"words\")\n",
    "    def text_pipeline(words):\n",
    "        \"\"\"Processes the transcriptions to generate proper labels\n",
    "\n",
    "        NB Make sure that you yield exactly what is defined above in @sb.utils.data_pipeline.provides()\"\"\"\n",
    "        yield words\n",
    "\n",
    "    # Define datasets from json data manifest file\n",
    "    # Define datasets sorted by ascending lengths for efficiency\n",
    "    datasets = {}\n",
    "\n",
    "    load_annotations_from_json_files = split2data is None\n",
    "    if load_annotations_from_json_files:\n",
    "        # by default load all splits from the json files specified in hparams\n",
    "        data_info = {\n",
    "            \"train\": hparams[\"train_annotation\"],\n",
    "            \"valid\": hparams[\"valid_annotation\"],\n",
    "            \"test\": hparams[\"test_annotation\"],\n",
    "            \"allsplits\": hparams[\"allsplits_annotation\"],\n",
    "        }\n",
    "    else:\n",
    "        data_info = split2data\n",
    "        \n",
    "    for split in data_info:\n",
    "        print(f\"create Dataset from split {split}\")\n",
    "        if load_annotations_from_json_files:\n",
    "            constructor = sb.dataio.dataset.DynamicItemDataset.from_json\n",
    "        else:\n",
    "            constructor = sb.dataio.dataset.DynamicItemDataset\n",
    "            \n",
    "        datasets[split] = constructor(\n",
    "            data_info[split],\n",
    "            dynamic_items=[audio_pipeline, ratio_pipeline, length_pipeline, text_pipeline],\n",
    "            output_keys = [\n",
    "                \"id\",\n",
    "                \"sig\",\n",
    "                \"wav_path\",\n",
    "                \"utt_id\",\n",
    "                \"samples_to_graphemes_ratio\",\n",
    "                \"length\",\n",
    "                \"words\",\n",
    "            ],\n",
    "        )\n",
    "        hparams[f\"{split}_dataloader_opts\"][\"shuffle\"] = True\n",
    "        datasets[split].split = split # add attribute to keep track of the split of the dataset\n",
    "\n",
    "    def print_dataset_lens(extra_str):\n",
    "        for split in data_info:\n",
    "            dataset_split = datasets[split]\n",
    "            print(f\"{split} dataset has {len(dataset_split)} samples\", extra_str)\n",
    "\n",
    "    print_dataset_lens(\"before any filtering\")\n",
    "\n",
    "    # Filter data for samples_to_graphemes_ratio that is either too small or too large\n",
    "    key_min_value = {}\n",
    "    if hparams[\"min_samples_to_graphemes_ratio\"] is not None:\n",
    "        key_min_value = {\"samples_to_graphemes_ratio\": hparams[\"min_samples_to_graphemes_ratio\"]}\n",
    "\n",
    "    key_max_value = {}\n",
    "    if hparams[\"max_samples_to_graphemes_ratio\"] is not None:\n",
    "        key_max_value = {\"samples_to_graphemes_ratio\": hparams[\"max_samples_to_graphemes_ratio\"]}\n",
    "\n",
    "    for split in data_info:\n",
    "        datasets[split] = datasets[split].filtered_sorted(\n",
    "            key_min_value=key_min_value,\n",
    "            key_max_value=key_max_value,\n",
    "        )\n",
    "    print_dataset_lens(\"after filtering by min and max samples to graphemes ratio\")\n",
    "\n",
    "    # Filter samples whos length is too short\n",
    "    key_min_value = {}\n",
    "    if hparams[\"min_length_seconds\"] is not None:\n",
    "        key_min_value = {\"length\": hparams[\"min_length_seconds\"]}\n",
    "    for split in data_info:\n",
    "        datasets[split] = datasets[split].filtered_sorted(\n",
    "            key_min_value=key_min_value,\n",
    "        )\n",
    "    print_dataset_lens(\"after filtering by minimum length\")\n",
    "\n",
    "    # Sorting training data with ascending order makes the code  much\n",
    "    # faster  because we minimize zero-padding. In most of the cases, this\n",
    "    # does not harm the performance.\n",
    "    if load_annotations_from_json_files:\n",
    "        if hparams[\"sorting\"] == \"ascending\":\n",
    "            datasets[\"train\"] = datasets[\"train\"].filtered_sorted(sort_key=\"length\")\n",
    "            hparams[\"train_dataloader_opts\"][\"shuffle\"] = False\n",
    "\n",
    "        elif hparams[\"sorting\"] == \"descending\":\n",
    "            datasets[\"train\"] = datasets[\"train\"].filtered_sorted(\n",
    "                sort_key=\"length\", reverse=True\n",
    "            )\n",
    "            hparams[\"train_dataloader_opts\"][\"shuffle\"] = False\n",
    "\n",
    "        elif hparams[\"sorting\"] == \"random\":\n",
    "            hparams[\"train_dataloader_opts\"][\"shuffle\"] = True\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                \"sorting must be random, ascending or descending\"\n",
    "            )\n",
    "    \n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b9c40abe-e4d0-4469-8108-41b0eb089874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create Dataset from split allsplits\n",
      "allsplits dataset has 66 samples before any filtering\n",
      "allsplits dataset has 66 samples after filtering by min and max samples to graphemes ratio\n",
      "allsplits dataset has 66 samples after filtering by minimum length\n"
     ]
    }
   ],
   "source": [
    "## filter dataset by PER of wordtypes as predicted by RNN-based G2P model\n",
    "def filter_data_by_per(data, str_min_len_threshold, N):\n",
    "    p = '/home/s1785140/data/ljspeech_fastpitch/oov_list_with_PER.pickle'\n",
    "    loaded_df = pd.read_pickle(p)\n",
    "    loaded_df = loaded_df[loaded_df['wordtype'].str.len() >= str_min_len_threshold]\n",
    "    loaded_df = loaded_df.sort_values('per', ascending=False).head(N)\n",
    "    wordtypes_to_keep = set(loaded_df['wordtype'])\n",
    "    new_data = {}\n",
    "    for token_id, token_id_values in data.items():\n",
    "        wordtype = token_id_values['words']\n",
    "        if wordtype not in wordtypes_to_keep:\n",
    "            continue\n",
    "        new_data[token_id] = token_id_values\n",
    "    return new_data\n",
    "\n",
    "filter_by_per = True\n",
    "# load from json files\n",
    "if not filter_by_per:\n",
    "    datasets = dataio_prepare(speechbrain_hparams)\n",
    "else:\n",
    "    filtered_allsplits_d = filter_data_by_per(allsplits_d, str_min_len_threshold=10, N=50)\n",
    "    datasets = dataio_prepare(speechbrain_hparams, {'allsplits': filtered_allsplits_d})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "10d20085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert from datasets to dataloaders\n",
    "split2stage = {\"train\": sb.Stage.TRAIN, \"valid\": sb.Stage.VALID, \"test\": sb.Stage.TEST, \"allsplits\": sb.Stage.TEST}\n",
    "for split in datasets.keys():\n",
    "    if not isinstance(datasets[split], DataLoader) or isinstance(datasets[split], LoopedLoader):\n",
    "        dataloader_kwargs=speechbrain_hparams[f\"{split}_dataloader_opts\"]\n",
    "        datasets[split] = asr_brain.make_dataloader(\n",
    "            datasets[split], stage=split2stage[split], **dataloader_kwargs\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c61a3fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_whitespace_to_0_probability(ctc_probs, vocab, log_probs=True, whitespace_symbol=\"|\"):\n",
    "    \"\"\"ctc_probs [bsz, max_seq_len, vocab_size]\"\"\"\n",
    "    new_probability = -math.inf if log_probs else 0.0\n",
    "    ctc_probs[:,:,vocab.index(whitespace_symbol)] = new_probability\n",
    "    return ctc_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2932070",
   "metadata": {},
   "source": [
    "# TRANSCRIBE WORD ALIGNED WAVS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "25a61168",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# generate transcriptions for all batches in test set\n",
    "def transcribe_dataset(asr_brain, dataset, \n",
    "                       num_batches_to_transcribe=None,\n",
    "                       hack_whitespace_probs=False, collapse_whitespace=True,\n",
    "                       print_info=False, max_hyps_per_sample=None,\n",
    "                       wordtypes_to_transcribe=[]):\n",
    "    # Now we iterate over the dataset and we simply compute_forward and decode\n",
    "\n",
    "    orig_words = []\n",
    "    printouts = []\n",
    "    orig_wavs = []\n",
    "    token_ids = []\n",
    "    transcribed_words = defaultdict(list)\n",
    "    n = 0 # number of batches transcribed\n",
    "    \n",
    "    break_forloop_early = num_batches_to_transcribe is not None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataset, dynamic_ncols=True, total=num_batches_to_transcribe):\n",
    "            # break out of loop if we have transcribed enough batches\n",
    "            if break_forloop_early and n >= num_batches_to_transcribe:\n",
    "                break\n",
    "            n += 1\n",
    "\n",
    "            orig_transcriptions = batch.words\n",
    "            wavs = batch.sig.data\n",
    "\n",
    "            # Make sure that your compute_forward returns the predictions !!!\n",
    "            # In the case of the template, when stage = TEST, a beam search is applied \n",
    "            # in compute_forward(). \n",
    "            predictions = asr_brain.compute_forward(batch, stage=sb.Stage.TEST)\n",
    "            ctc_probs = predictions['ctc_logprobs'] # FOR DEBUG\n",
    "\n",
    "            # hack probabilities to set all probs to 0 for whitespace\n",
    "            if hack_whitespace_probs:\n",
    "                ctc_probs = set_whitespace_to_0_probability(ctc_probs, vocab, log_probs=True, whitespace_symbol=\"|\")\n",
    "\n",
    "            # get mel lens from wav len ratios since torch ctc decoder requires lens in frames\n",
    "            batch_max_len = predictions[\"ctc_logprobs\"].size(1)\n",
    "            bsz = predictions[\"ctc_logprobs\"].size(0)\n",
    "            mel_lens = torch.zeros(bsz)\n",
    "            for i, len_ratio in enumerate(asr_brain.feat_lens):\n",
    "                mel_lens[i] = int(torch.round(len_ratio * batch_max_len))\n",
    "            \n",
    "            predicted_ids = ctc_beamsearch_decoder(\n",
    "                predictions[\"ctc_logprobs\"], lengths=mel_lens\n",
    "            )\n",
    "\n",
    "            # iterate over samples in batch\n",
    "            for i, (utt_id, orig_text, hyps, wav) in enumerate(zip(batch.utt_id, orig_transcriptions, predicted_ids, wavs)):\n",
    "                sample_printout = f\"\\nsample {i+1} - ({utt_id}: '{orig_text}')\"\n",
    "                orig_utt_text = \"INSERT ORIG UTT TEXT\"\n",
    "                sample_printout += f\"\\nOriginal Utterance: {orig_utt_text}\"\n",
    "                # if print_info: print(f\"\\nsample {i+1} - ({utt_id}: '{orig_text}')\")\n",
    "                sample_cers = []\n",
    "                for j, hyp in enumerate(list(hyps)[:max_hyps_per_sample]):\n",
    "                    words = asr_brain.hparams.tokenizer.decode_ids(hyp.tokens.tolist())\n",
    "                    if collapse_whitespace:\n",
    "                        words = \"\".join(c for c in words if c != \" \")\n",
    "                    hyp_cer = 100 * cer(orig_text, words)\n",
    "                    sample_cers.append(hyp_cer)\n",
    "                    sample_printout += f\"\\n\\thyp {j+1}/{len(hyps)} (CER={hyp_cer:.1f}%): '{words}'\"\n",
    "                    # if print_info: print(f\"\\thyp {j+1}/{len(hyps)} (CER={hyp_cer:.1f}%): '{words}'\")\n",
    "                    if not wordtypes_to_transcribe or orig_text in wordtypes_to_transcribe:\n",
    "                        transcribed_words[orig_text].append(words)\n",
    "                    \n",
    "                sample_printout += f\"\\n\\t=== Mean CER: {np.mean(sample_cers):.1f}%, Std CER: {np.std(sample_cers):.1f}% ===\"\n",
    "                # if print_info: print(f\"\\t=== Mean CER: {np.mean(sample_cers):.1f}%, Std CER: {np.std(sample_cers):.1f}% ===\")\n",
    "\n",
    "                include_current_wordtype = (wordtypes_to_transcribe == [] or orig_text in wordtypes_to_transcribe)\n",
    "                if include_current_wordtype:\n",
    "                    orig_words.append(orig_text)\n",
    "                    printouts.append(sample_printout)\n",
    "                    orig_wavs.append(wav)\n",
    "                    token_ids.append(utt_id)\n",
    "\n",
    "    output_dict = {\n",
    "        \"orig_words\": orig_words,\n",
    "        \"transcribed_words\": transcribed_words,\n",
    "        \"wavs\": orig_wavs,\n",
    "        \"printouts\": printouts,\n",
    "        \"token_ids\": token_ids,\n",
    "    }\n",
    "\n",
    "    return output_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99df03d5",
   "metadata": {},
   "source": [
    "## Generate multiple times to see if outputs change (i.e. should change if dropout is enabled in model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96603643-3a86-409a-badb-29d086535eee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 2/2 [00:27<00:00, 13.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                               | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# NUM_BATCHES_TO_TRANSCRIBE = 1\n",
    "NUM_BATCHES_TO_TRANSCRIBE = None\n",
    "MAX_HYPS_PER_SAMPLE = 50\n",
    "\n",
    "wordtypes_to_transcribe = []\n",
    "# wordtypes_to_transcribe = ['anesthesiologists'] # used for debugging and checking results only for a subset of wordtypes\n",
    "\n",
    "NUM_ITERATIONS = 25\n",
    "\n",
    "word_sets = defaultdict(set)\n",
    "\n",
    "print_printout = False\n",
    "display_audio = False\n",
    "\n",
    "'''create output dict of form:\n",
    "token_id:\n",
    "    word: xxx\n",
    "    hyps:\n",
    "        xxx\n",
    "        xxx\n",
    "        xxx\n",
    "        ...\n",
    "    wav: /path/to/token_id.wav\n",
    "'''\n",
    "master_transcription_dict = defaultdict(dict)\n",
    "print_num_new_spellings_per_iteration = False\n",
    "\n",
    "for i in range(NUM_ITERATIONS):\n",
    "    print(f\"Iteration {1+i}/{NUM_ITERATIONS}\")\n",
    "    transcription_output_dict = transcribe_dataset(asr_brain, datasets[\"allsplits\"], \n",
    "                                                num_batches_to_transcribe=NUM_BATCHES_TO_TRANSCRIBE,\n",
    "                                                collapse_whitespace=False, hack_whitespace_probs=True,\n",
    "                                                max_hyps_per_sample=MAX_HYPS_PER_SAMPLE,\n",
    "                                                wordtypes_to_transcribe=wordtypes_to_transcribe)\n",
    "    \n",
    "    for token_id, orig_word, printout, wav in zip(\n",
    "        transcription_output_dict[\"token_ids\"], \n",
    "        transcription_output_dict[\"orig_words\"], \n",
    "        transcription_output_dict[\"printouts\"], \n",
    "        transcription_output_dict[\"wavs\"]\n",
    "    ):\n",
    "        if print_printout:\n",
    "            print(printout)\n",
    "        if display_audio:\n",
    "            display(Audio(wav, rate=16000))\n",
    "        old_count = len(word_sets[orig_word])\n",
    "        asr_hypotheses = transcription_output_dict[\"transcribed_words\"][orig_word]\n",
    "        asr_hypotheses = [hyp.rstrip() for hyp in asr_hypotheses] # as CTC decoder always adds extra whitespace to end\n",
    "        word_sets[orig_word].update(asr_hypotheses)\n",
    "        master_transcription_dict[token_id][\"orig_word\"] = orig_word\n",
    "        master_transcription_dict[token_id][\"orig_wav\"] = wav.numpy()             \n",
    "        \n",
    "        if \"hyps\" not in master_transcription_dict[token_id]:\n",
    "            master_transcription_dict[token_id][\"hyps\"] = set(asr_hypotheses)\n",
    "        else:\n",
    "            master_transcription_dict[token_id][\"hyps\"].update(asr_hypotheses)\n",
    "        \n",
    "        new_count = len(word_sets[orig_word])\n",
    "        if new_count > old_count:\n",
    "            if print_num_new_spellings_per_iteration:\n",
    "                print(f\"added {new_count - old_count} new spellings for '{orig_word}' to its word set\")\n",
    "\n",
    "total_num = 0\n",
    "for orig_word, word_set in word_sets.items():\n",
    "    print(f\"added {len(word_set)} new spellings for '{orig_word}' to its word set\")\n",
    "    total_num += len(word_set)\n",
    "    \n",
    "print(f\"\\n *** Will synthesise these {total_num} spellings using TTS pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847e3594",
   "metadata": {},
   "source": [
    "# create dataframe to store respellings for each original spelling (and respective CERs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8790788f-e03f-4763-ae5a-64ebbd687532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create big dataframe to hold all data\n",
    "df = pd.DataFrame(columns=['token_id', 'wordtype', 'respelling', 'cer'])\n",
    "data = [] # used to populate pandas dataframe\n",
    "\n",
    "for token_id in master_transcription_dict:    \n",
    "    for hyp in master_transcription_dict[token_id]['hyps']:\n",
    "        data.append({'token_id': token_id, \n",
    "                     'wordtype': master_transcription_dict[token_id]['orig_word'], \n",
    "                     'respelling': hyp, \n",
    "                     'orig_wav': master_transcription_dict[token_id]['orig_wav'],\n",
    "                     'cer': 100*cer(master_transcription_dict[token_id]['orig_word'], hyp)}),\n",
    "\n",
    "df = pd.concat([df, pd.DataFrame(data)])\n",
    "print('number of token_id-spelling combinations', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7de5fd6-1b8a-4852-8bea-364b446bd0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# guarantee that original spelling will be generated by TTS\n",
    "# i.e. add orig_word to \"respelling\" column if it doesn't occur in it already\n",
    "dfs_to_concat = []\n",
    "for orig_word in df['wordtype'].unique():\n",
    "    match_mask = df['respelling'] == orig_word\n",
    "    matching_respellings = df.loc[match_mask]\n",
    "    if matching_respellings.empty:\n",
    "        # copy the first row, but change value for respelling\n",
    "        first_row_df = df.loc[df['wordtype'] == orig_word].head(1)\n",
    "        row_idx = first_row_df.index[0]\n",
    "        first_row_df.loc[row_idx, 'respelling'] = orig_word\n",
    "        first_row_df.loc[row_idx, 'cer'] = 0.0\n",
    "        dfs_to_concat.append(first_row_df)\n",
    "        # print(f'added original spelling to respelling column for \"{orig_word}\"')\n",
    "\n",
    "for temp_df in dfs_to_concat:\n",
    "    df = pd.concat([df, temp_df], ignore_index=True)\n",
    "print('number of token_id-spelling combinations', len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87e8270",
   "metadata": {},
   "source": [
    "# load pretrained fastpitch and hifigan models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466a024c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_pretrained_tts_and_vocoder import load_vocoder, load_pretrained_fastpitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1a0f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hifigan_checkpoint = \"/home/s1785140/pretrained_models/hifigan/ljspeech/LJ_V1/generator_v1\"\n",
    "fastpitch_checkpoint = '/home/s1785140/respeller/fastpitch/exps/halved_ljspeech_data_nospaces_noeos_pad_lowercase_nopunc/FastPitch_checkpoint_1000.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5fd0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hifigan = load_vocoder(hifigan_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3d2747",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastpitch, *_ = load_pretrained_fastpitch(fastpitch_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b22229",
   "metadata": {},
   "source": [
    "# run each respelling through TTS to get mel-spectrogram (and optionally vocode to get wav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283f5dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastpitch.inference import prepare_input_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8415ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create input to fastpitch batch maker\n",
    "all_orig_words = [orig_word for orig_word in df['wordtype']]\n",
    "all_respellings = [respelling for respelling in df['respelling']]\n",
    "fields = {'orig_spelling': all_orig_words, \n",
    "          'respelling': all_respellings,\n",
    "          'text': all_respellings} # will be encoded by text processes into IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf1f8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = prepare_input_sequence(fields, device, input_type='char', \n",
    "                       symbol_set='english_pad_lowercase_nopunc', \n",
    "                       text_cleaners=['lowercase_no_punc'],\n",
    "                       batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05a9e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def melspec2mfcc(mel, n_mfcc=12):\n",
    "    log_mel = librosa.core.amplitude_to_db(mel, ref=np.max)\n",
    "    mfcc = librosa.feature.mfcc(S=log_mel, n_mfcc=n_mfcc)\n",
    "    return mfcc\n",
    "\n",
    "def generate_audio(y, vocoder=None, sampling_rate=22050, hop_length=256):\n",
    "    \"\"\"Generate audio from spectrograms for n utterances in batch\"\"\"\n",
    "    mel, mel_lens = y\n",
    "    bs = mel.size(0)\n",
    "    with torch.no_grad():\n",
    "        audios = vocoder(mel).cpu().squeeze(1).numpy() # [bsz, dim, samples ]only squeeze away dim (equals 1 for waveform)\n",
    "        mel_lens = mel_lens.cpu().numpy() - 1\n",
    "    audios_to_return = []\n",
    "    for audio, mel_len in zip(audios, mel_lens):\n",
    "        audio = audio[:mel_len * hop_length]\n",
    "        audio = audio / np.max(np.abs(audio))\n",
    "        audios_to_return.append(audio)\n",
    "        \n",
    "    return audios_to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f8651d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "do_vocode = True\n",
    "\n",
    "spelling2tts_mel = {}\n",
    "spelling2tts_mfcc = {}\n",
    "spelling2vocoded_audio = {}\n",
    "audios = None\n",
    "\n",
    "with torch.no_grad():\n",
    "    for b in tqdm(batches, \"synthesising respellings\"):\n",
    "        mel, mel_lens, *_ = fastpitch(b['text'])\n",
    "        \n",
    "        if do_vocode:\n",
    "            vocoder_inputs = (mel, mel_lens)\n",
    "            audios = generate_audio(vocoder_inputs, hifigan)\n",
    "            \n",
    "        # add samples in batch to dicts\n",
    "        bsz = len(mel)\n",
    "        for i in range(bsz):\n",
    "            respelling = b['respelling'][i]\n",
    "            mel_len = mel_lens[i]\n",
    "            spelling2tts_mel[respelling] = mel[i, :, :mel_len].cpu().numpy() # mel [bsz, feats, time]\n",
    "            spelling2tts_mfcc[respelling] = melspec2mfcc(spelling2tts_mel[respelling])\n",
    "            \n",
    "            if audios is not None:\n",
    "                spelling2vocoded_audio[respelling] = audios[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e72723b-3449-4ad1-bd62-a48b001d09fe",
   "metadata": {},
   "source": [
    "# create mapping from token_id to ground truth fastpitch mels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e267003-b7f8-4763-abbc-8a26af0db137",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#build mapping from token_id to path to its gt word aligned mel \n",
    "word_aligned_feats_path = '/home/s1785140/data/ljspeech_fastpitch/wordaligned_mels_lowercase_nopunc'\n",
    "token_id2fastpitch_gt_mel_path = {}\n",
    "\n",
    "for token_id in df['token_id'].unique():\n",
    "    wordtype = token_id.split('__')[0]\n",
    "    path = os.path.join(word_aligned_feats_path, wordtype, token_id + '.pt')\n",
    "    token_id2fastpitch_gt_mel_path[token_id] = path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da5d358-e3ba-4574-962a-a9a0bbfec2f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load mels from disk\n",
    "token_id2fastpitch_gt_mel = {}\n",
    "\n",
    "for token_id, gt_mel_path in token_id2fastpitch_gt_mel_path.items():\n",
    "    # remove len and file extension from path\n",
    "    # e.g. 'wordaligned_mels/unconstitutionality/unconstitutionality__LJ021-0191__occ1__len25760.wav' \n",
    "    # ---> 'wordaligned_mels/unconstitutionality/unconstitutionality__LJ021-0191__occ1\"\n",
    "    path_no_len_no_ext = gt_mel_path.split('__len')[0]\n",
    "    \n",
    "    # glob to fine the matching ground truth mel saved on disk\n",
    "    matches = glob.glob(path_no_len_no_ext + '*.pt')\n",
    "    assert len(matches) == 1\n",
    "    fastpitch_gt_mel = torch.load(matches[0]).transpose(0,1).cpu().numpy()\n",
    "    token_id2fastpitch_gt_mel[token_id] = fastpitch_gt_mel\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0768f426-16a5-47ea-9469-d94d385236e9",
   "metadata": {},
   "source": [
    "# Verify that predicted mel-spectrograms are normalised in same way as GT mel-spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708ff4a6-2b89-4541-a7e0-9761f46fd525",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_melspec(mel, title=\"\"):\n",
    "    if type(mel) == torch.Tensor:\n",
    "        mel = mel.cpu().numpy()\n",
    "    fig, ax = plt.subplots(figsize=(10, 2))\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.imshow(mel, aspect='auto', origin='lower', interpolation='none')\n",
    "    plt.colorbar()\n",
    "    \n",
    "def pad_along_axis(array: np.ndarray, target_length: int, axis: int = 0, constant_values=0) -> np.ndarray:\n",
    "    # Pad the data with NaN values at both ends\n",
    "    pad_size = target_length - array.shape[axis]\n",
    "    assert pad_size % 2 == 0\n",
    "\n",
    "    if pad_size <= 0:\n",
    "        return array\n",
    "\n",
    "    npad = [(0, 0)] * array.ndim\n",
    "    npad[axis] = (int(pad_size/2), int(pad_size/2))\n",
    "\n",
    "    return np.pad(array, pad_width=npad, mode='constant', constant_values=constant_values)\n",
    "\n",
    "def wma_smooth(mel, weights, dim_to_smooth=0, maintain_time_dim=False, extra_padding=0):\n",
    "    # mel should be shape [t, dim]\n",
    "    if dim_to_smooth == 0:\n",
    "        data = mel.transpose(0,1)\n",
    "    elif dim_to_smooth == 1:\n",
    "        data = mel\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "    weights = weights / np.sum(weights)\n",
    "    window = weights.shape[0]\n",
    "    dim = data.shape[1]\n",
    "        \n",
    "    # stack along feat dimension\n",
    "    stacked_weights = [weights for _ in range(dim)]\n",
    "    stacked_weights = np.stack(stacked_weights, axis=-1)\n",
    "    weights = stacked_weights\n",
    "\n",
    "    # pad data\n",
    "    target_len_after_padding = data.shape[0] + 2 * (window // 2) + 2 * extra_padding\n",
    "    padded_data = pad_along_axis(data, \n",
    "                                 target_length=target_len_after_padding, \n",
    "                                 axis=0, \n",
    "                                 constant_values=data.min()) # use data.min() as padding val because for log mel, 0 is actually large!!!\n",
    "\n",
    "    # Compute the weighted moving average \n",
    "    N = len(padded_data) - window + 1\n",
    "    wma = np.array([np.sum(weights * padded_data[i:i+window], axis=0) / np.sum(np.nan_to_num(weights), axis=0) for i in range(N)])\n",
    "    \n",
    "    if maintain_time_dim:\n",
    "        assert wma.shape == data.shape, f\"{wma.shape=} == {data.shape=}\"\n",
    "    else:\n",
    "        assert wma.shape[1] == data.shape[1], f\"{wma.shape[1]=} == {data.shape[1]=}\"\n",
    "    \n",
    "    return wma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73aad651-be15-4b67-84d5-372c229d179a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot GT mel spectrogram\n",
    "token_id = list(token_id2fastpitch_gt_mel.keys())[0]\n",
    "wordtype = token_id.split('__')[0]\n",
    "gt_mel = token_id2fastpitch_gt_mel[token_id]\n",
    "\n",
    "plot_melspec(gt_mel, token_id + \" - ORIGINAL!!! (no weighted moving average)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cd27a1-d66a-4ca0-93c8-86599d56abbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filter = [1,2,1]\n",
    "wma = wma_smooth(gt_mel.T, weights=filter).T\n",
    "plot_melspec(wma, token_id + f\" weights={filter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2eb4c4-a396-4a49-8e87-c8358156c436",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "temporal_filter = [1,2,1]\n",
    "feature_filter = [1,1,2,1,1]\n",
    "wma = wma_smooth(gt_mel.T, dim_to_smooth=0, weights=temporal_filter).T\n",
    "wma = wma_smooth(wma.T, dim_to_smooth=1, weights=feature_filter).T\n",
    "plot_melspec(wma, token_id + f\" weights={filter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d9b976-05b3-4395-b665-3ba2908ea471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot predicted mel spectrogram\n",
    "mel = spelling2tts_mel[wordtype]\n",
    "\n",
    "print(wordtype)\n",
    "plot_melspec(mel, wordtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0354f2-2420-4f02-8b25-8bcd77e4b731",
   "metadata": {},
   "source": [
    "# convert mel spec to Mel-Cepstrum then plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d310587d-3272-45bd-9a7a-3fb1c3a9f3ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_mfcc(mfcc, extra_title=\"\"):\n",
    "    # assume mfcc is a numpy array of shape (n_mfcc, n_frames)\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.imshow(mfcc, aspect='auto', origin='lower', cmap='coolwarm')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('MFCC')\n",
    "    title = 'MFCC'\n",
    "    if extra_title:\n",
    "        title += f\": {extra_title}\"\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f165b8cd-2f83-496b-ba2a-12e23870dcd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mfcc = melspec2mfcc(mel)\n",
    "plot_mfcc(mfcc, wordtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dca6c5-0b3a-4316-b98a-981b0e8e941a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gt_mfcc = melspec2mfcc(gt_mel)\n",
    "plot_mfcc(gt_mfcc, token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6adef8-4070-4f84-be7e-64a05a9b693d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# (optional) apply weighted moving average filters to GT mels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3795ab2c-6a07-426e-a3cb-7175104ce5a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# temporal_filter = []\n",
    "# feature_filter = []\n",
    "temporal_filter = [1,2,1]\n",
    "feature_filter = [1,1,2,1,1]\n",
    "\n",
    "for token_id, fastpitch_gt_mel in token_id2fastpitch_gt_mel.items():\n",
    "    if temporal_filter:\n",
    "        fastpitch_gt_mel = wma_smooth(fastpitch_gt_mel.T, dim_to_smooth=0, weights=temporal_filter).T\n",
    "    if feature_filter:\n",
    "        fastpitch_gt_mel = wma_smooth(fastpitch_gt_mel.T, dim_to_smooth=1, weights=feature_filter).T       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcc40c1-9008-429e-b0f4-19a4f6fa8a24",
   "metadata": {
    "tags": []
   },
   "source": [
    "# convert GT mels to MFCCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b715362-3cc2-44ec-9ad7-3bb046361998",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "token_id2fastpitch_gt_mfcc = {}\n",
    "for token_id, fastpitch_gt_mel in token_id2fastpitch_gt_mel.items():\n",
    "    token_id2fastpitch_gt_mfcc[token_id] = melspec2mfcc(fastpitch_gt_mel)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e692e0",
   "metadata": {},
   "source": [
    "# add acoustic distances between ground truth audio for word and synthesised respellings\n",
    "\n",
    "compare ranking by:\n",
    "- MCD-DTW\n",
    "- SoftDTW\n",
    "- L1, L2 distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c1db11-e05a-42d4-a47a-24461729b6c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import mcd\n",
    "from mcd import dtw\n",
    "\n",
    "def eucCost(x, y):\n",
    "    \"\"\"AKA l2 norm\"\"\"\n",
    "    return math.sqrt(np.inner(x - y, x - y))\n",
    "\n",
    "def manCost(x, y):\n",
    "    \"\"\"AKA l1 norm\"\"\"\n",
    "    return np.sum(np.inner(x - y, x - y))\n",
    "\n",
    "metric_registry = {\n",
    "}\n",
    "\n",
    "metric_type = \"mcd_dtw\"\n",
    "\n",
    "distance2costfn = {\n",
    "    \"euclidean\": eucCost,\n",
    "    \"manhattan\": manCost,\n",
    "}\n",
    "\n",
    "distance_type = \"euclidean\"\n",
    "\n",
    "distance_metric_col_name = f'{metric_type} ({distance_type})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fd5ddf-9c2c-46c6-978e-b7820d730ca7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculate MCD-DTW distance for each synthesised respelling in the dataframe\n",
    "# to the gt mfcc for the token_id that the respelling was generated from (using ASR)\n",
    "print(f\"calculating MCD-DTW distances between GT and predicted mels using {distance_type}\")\n",
    "\n",
    "for row_idx in tqdm(df.index):\n",
    "    respelling_mfcc = spelling2tts_mfcc[df.loc[row_idx, 'respelling']]\n",
    "    gt_mfcc = token_id2fastpitch_gt_mfcc[df.loc[row_idx, 'token_id']]\n",
    "    respelling_mfcc = respelling_mfcc.T\n",
    "    gt_mfcc = gt_mfcc.T\n",
    "    distance_score, _path = dtw.dtw(gt_mfcc, respelling_mfcc, distance2costfn[distance_type])\n",
    "    df.loc[row_idx, distance_metric_col_name] = distance_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fed0e47-6825-4772-b634-e0f3fe76cb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort df by distance then spelling\n",
    "df = df.sort_values(['wordtype', distance_metric_col_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3674b74a",
   "metadata": {},
   "source": [
    "# play audios for each word, ranked by certain metric (e.g. CER, acoustic distance, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33eb9bf1-7ebe-4e1e-9266-4cb046b36956",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wordtypes = df[\"wordtype\"].unique()\n",
    "print(len(wordtypes))\n",
    "wordtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a895a9aa-2469-4121-bc97-7fd82e539329",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# optionally choose to only display a subset of the wordtypes\n",
    "\n",
    "words_to_include = None # include all words\n",
    "# words_to_include = [\n",
    "#     \"presenting\",\n",
    "#     \"admittedly\",\n",
    "# ]\n",
    "\n",
    "if words_to_include is not None:\n",
    "    filtered_df = pd.DataFrame()\n",
    "    for wordtype in words_to_include:\n",
    "        filtered_df = pd.concat([filtered_df, df.loc[df[\"wordtype\"] == wordtype]]) \n",
    "else:\n",
    "    filtered_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206e9434-6820-42f6-9d3a-f1ec05d060a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally only display N-best ranked respellings per original input audio (excluding original spellings!)\n",
    "get_smallest = True\n",
    "n_spellings_per_token_id = 5\n",
    "if get_smallest:\n",
    "    lambda_fn = lambda x: x.nsmallest(n_spellings_per_token_id, distance_metric_col_name)\n",
    "else:\n",
    "    lambda_fn = lambda x: x.nlargest(n_spellings_per_token_id, distance_metric_col_name)\n",
    "    \n",
    "original_spellings_mask = filtered_df['wordtype'] == filtered_df['respelling']\n",
    "original_spellings_df = filtered_df[original_spellings_mask]\n",
    "inverted_mask = ~original_spellings_mask\n",
    "no_orig_spellings_df = filtered_df[inverted_mask]\n",
    "filtered_df = no_orig_spellings_df.groupby('wordtype').apply(lambda_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ebf00c-2183-4fe1-80bf-1fd1dce7eb3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add back original spellings and sort by distance again\n",
    "filtered_df = pd.concat([filtered_df, original_spellings_df], ignore_index=True)\n",
    "filtered_df = filtered_df.groupby('wordtype', group_keys=True).apply(lambda x: x.sort_values(distance_metric_col_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cffc8b-2e12-4022-8003-3e283c60db3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column to indicate whether it is original spelling or not\n",
    "new_col_idx = 1 + filtered_df.columns.get_loc('respelling') # insert after respelling column\n",
    "filtered_df.insert(new_col_idx, 'is_orig', filtered_df['wordtype'] == filtered_df['respelling'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ede93a8-da3c-47eb-9561-2ec3269fbefd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "current_wordtype = \"\"\n",
    "for token_id, wordtype, respelling, distance, orig_wav, is_orig in zip(filtered_df['token_id'], \n",
    "                                                              filtered_df['wordtype'], \n",
    "                                                              filtered_df['respelling'], \n",
    "                                                              filtered_df[distance_metric_col_name], \n",
    "                                                              filtered_df['orig_wav'],\n",
    "                                                              filtered_df['is_orig']):\n",
    "    # new wordtype, print GT token_ids and display audios\n",
    "    if current_wordtype != wordtype:\n",
    "        current_wordtype = wordtype\n",
    "        \n",
    "        # get all token_ids and orig_wavs for this wordtype\n",
    "        wordtype_df = filtered_df[filtered_df['wordtype'] == wordtype]\n",
    "        first_row_for_token_id = wordtype_df.groupby('token_id').nth(0)\n",
    "        \n",
    "        print(\"\\n===============================================================\")\n",
    "        print(f\"====== Original word aligned WAV(s) ===========================\")\n",
    "        print(\"===============================================================\")\n",
    "        for index, row in first_row_for_token_id.iterrows():\n",
    "            token_id = index # index is now equal to token_id as we used groupby above\n",
    "            print(f\"[GT - {token_id}]:\")\n",
    "            display(Audio(row['orig_wav'], rate=16000))\n",
    "        print(\"===============================================================\")\n",
    "    \n",
    "    # create string to print for this respelling\n",
    "    print_str = \"\"\n",
    "    if is_orig:\n",
    "        print_str += f\"{' '+wordtype+' ':*^35}\"\n",
    "    else:\n",
    "        print_str += f\" ---> {respelling=}\"\n",
    "    distance_str = f\" {distance:.2f} [{distance_metric_col_name}] [calc'ed vs GT: {token_id}]\"\n",
    "    print_str += distance_str\n",
    "    print(print_str)\n",
    "    \n",
    "    display(Audio(spelling2vocoded_audio[respelling], rate=22050))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d0751d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "6a8422eeb13ba8f92f71047f64b5c33152e234c2bbad3f45433feda7b6f3b4c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
