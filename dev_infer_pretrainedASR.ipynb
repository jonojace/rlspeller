{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4207e21b-d910-485b-8b9c-3fc7d5a59c5a",
   "metadata": {},
   "source": [
    "# Goal of this notebook\n",
    "\n",
    "Develop a training loop for finetuning ASR models using TTS loss by recreating RL training found in RL4LMs/rl4lms/envs/text_generation/training_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70687ad-9f13-4cdf-8ef7-07eecb34f496",
   "metadata": {},
   "source": [
    "# automatic reloading magic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738aac35-fdd1-4452-aff0-92caa77ec5ab",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c8a89c2-38ea-4778-8028-0c7854152301",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import List, Dict, Tuple, Any\n",
    "import hyperpyyaml\n",
    "from tqdm import tqdm\n",
    "from torchaudio.models.decoder import ctc_decoder\n",
    "from torch.nn.functional import softmax\n",
    "import random\n",
    "from jiwer import cer\n",
    "import numpy as np\n",
    "import speechbrain as sb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa2cd37",
   "metadata": {},
   "source": [
    "## check if gpu available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53032f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strickland.inf.ed.ac.uk\n"
     ]
    }
   ],
   "source": [
    "# print hostname to make sure we are on correct node\n",
    "import socket\n",
    "print(socket.gethostname())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f67b1b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6f67235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/disk/nfs/ostrom/s1785140/rlspeller'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b236619f-6c04-44b1-a697-a342417a6694",
   "metadata": {},
   "source": [
    "# HPARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8a11879-640c-433e-a192-9c8b551d8ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    \"softdtw_temp\": 0.01,\n",
    "    \"softdtw_bandwidth\": 120,\n",
    "    \"dist_func\": \"l1\",\n",
    "    \"sentencepiece_model_path\": \"/home/s1785140/speechbrain/templates/speech_recognition_CharTokens_NoLM/Tokenizer/save/0_char.model\",\n",
    "    'speechbrain_hparams_file': '/home/s1785140/rlspeller/infer_speechbrain.yaml',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e877615-a6ff-4755-a7e4-e6f72e542432",
   "metadata": {},
   "source": [
    "# TOKENIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0505c38d-bc92-4f84-9f1f-cfc942ad337d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "# load pretrained tokenizer used to tokenizer ASR training inputs \n",
    "import sentencepiece as spm \n",
    "spm_path = hparams[\"sentencepiece_model_path\"]\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(spm_path)\n",
    "print(sp.vocab_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4af21be-9224-4c45-87f6-b86a8d7c63ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 10 2 12 12 4 1 17 4 9 12 11 1 16 20 1 6 5 16 2 1 7 8 1 26 5 8 4 6\n"
     ]
    }
   ],
   "source": [
    "# test tokenizer\n",
    "s = \"hello world my name is jason\"\n",
    "# TODO pass string through text cleaners? \n",
    "encoded = sp.EncodeAsIds(s)\n",
    "assert 0 not in encoded, \"tried to encode an unknown character\"\n",
    "print(\" \".join(str(idx) for idx in encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe3c6048-79c5-47b4-9524-88d2e36ad8d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello world my name is jason'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.DecodeIds(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2996d1f1-d331-4919-8742-7907b46a46ce",
   "metadata": {},
   "source": [
    "# NEW! SIMPLE TOKENIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8951a84-e22e-4c62-a169-403a1b5269af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from speechbrain.tokenizers.SimpleTokenizer import SimpleTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfec85a8-d20d-445e-8483-cf27092e2962",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = SimpleTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c52e4b5-8d26-44c3-a1ad-c772b6feac62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello|my|name|is|jason\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[9, 6, 13, 13, 16, 1, 14, 26, 1, 15, 2, 14, 6, 1, 10, 20, 1, 11, 2, 20, 16, 15]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"hello my name is jason\"\n",
    "text = text.replace(' ', '|')\n",
    "print(text)\n",
    "ids = tokenizer.encode_as_ids(text)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5f1bf5e-500b-47ce-986c-3cbe059cdf41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello|my|name|is|jason'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode_ids(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5ea5f7-6224-4089-8136-5c4b8d47a2e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## test simple tokenizer with probability distribution, and see if CTC decoder successfully generates n-best lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a60ffb1b-6770-466c-90f6-6daa1dbe461b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create empty array of correct dimensions\n",
    "min_len, max_len = 50, 100\n",
    "bsz = 4\n",
    "lens = torch.randint(min_len, max_len, (bsz,))\n",
    "vocab_size = len(tokenizer.vocab)\n",
    "\n",
    "# randomly assign probaility distribution to each timestep\n",
    "\n",
    "# try to decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cac82712-8559-44d0-86d9-66cf1ddfbd5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "randn = torch.randn(bsz, max_len, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26e7e51c-bab1-4f2d-bb12-34f9f9c4454c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ctc_probs = softmax(randn, dim=1)\n",
    "# ctc_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f9abdc6-bd05-4c5c-a091-d04dadbaa9e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('sample 1, hyp 1/2', ['|kbwyjgpqvzacmvp|m|oynhmdurlxhwcrkivcakipybdogwdcnkbibhepapkrwlesfzthpvtncstewreifdgqbiu|dpjmy|'])\n",
      "('sample 1, hyp 2/2', ['|kbwyjgpqvzacmvp|m|oynhmdurlxhwcrkivcavipybdogwdcnkbibhepapkrwlesfzthpvtncstewreifdgqbiu|dpjmy|'])\n",
      "('sample 2, hyp 1/2', ['|awoiylgdaiupyh|vwtydjhxfrntlnbzslk|gorj|ulpmicqhhvbtipmgukaqdtnpczsrtfro|'])\n",
      "('sample 2, hyp 2/2', ['|awoiylgdaiupyh|vwtydjhxfrntlnbzslk|gorj|ulpmicqhhvbtipmgukaqdtnpcdsrtfro|'])\n",
      "('sample 3, hyp 1/2', ['|wsrusaqhwdbuoxpokjiezotyjuoznovygimucgbugpgnlbnzfxj|pxtf|efvplyvn|'])\n",
      "('sample 3, hyp 2/2', ['|wkrusaqhwdbuoxpokjiezotyjuoznovygimucgbugpgnlbnzfxj|pxtf|efvplyvn|'])\n",
      "('sample 4, hyp 1/2', ['|dlvmfuedstxpgihcwebyx|curwqtohicvhzkfalnrsisnuhibep|'])\n",
      "('sample 4, hyp 2/2', ['|dlvmfuedstxpgihcwebyx|curwqtohicvhzkflnrsisnuhibep|'])\n"
     ]
    }
   ],
   "source": [
    "ctc_beamsearch_decoder_test = ctc_decoder(\n",
    "    lexicon=None,\n",
    "    # tokens=\"/home/s1785140/rlspeller/templates/speech_recognition_CharTokens_NoLM/Tokenizer/save/tokens.txt\",\n",
    "    tokens=tokenizer.vocab,\n",
    "    nbest=2,\n",
    "    blank_token='-',\n",
    "    sil_token=\"|\",\n",
    ")\n",
    "\n",
    "predicted_ids = ctc_beamsearch_decoder_test(ctc_probs, lens)\n",
    "\n",
    "predicted_words = []\n",
    "for i, hyps in enumerate(predicted_ids):\n",
    "    for j, hyp in enumerate(hyps):\n",
    "        words = tokenizer.decode_ids(hyp.tokens.tolist()).split(\" \")\n",
    "        tup = (f\"sample {i+1}, hyp {j+1}/{len(hyps)}\", words)\n",
    "        predicted_words.append(tup)\n",
    "        print(tup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd650fd4",
   "metadata": {},
   "source": [
    "# LOAD ASR (PRETRAINED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56bf160c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from templates.speech_recognition_CharTokens_NoLM.ASR.train import ASR\n",
    "from templates.speech_recognition_CharTokens_NoLM.ASR.train import dataio_prepare\n",
    "from torch.utils.data import DataLoader\n",
    "from speechbrain.dataio.dataloader import LoopedLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36d8019b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/s1785140/speechbrain/templates/speech_recognition_CharTokens_NoLM/data/rirs_noises.zip exists. Skipping download\n"
     ]
    }
   ],
   "source": [
    "# Load hyperparameters file with command-line overrides\n",
    "speechbrain_hparams_file = hparams['speechbrain_hparams_file']\n",
    "with open(speechbrain_hparams_file) as f:\n",
    "    speechbrain_hparams = hyperpyyaml.load_hyperpyyaml(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26c15de1-25d9-428e-900f-967c3af9a8ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/s1785140/speechbrain/templates/speech_recognition_CharTokens_NoLM/ASR/results/CRDNN_CHAR_LJSpeech_halved/2602/save'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speechbrain_hparams['save_folder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4072195-fb2b-4c9a-b201-c84b7a7ae104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if on_evaluate_start() get runtime error, likely need to restart notebook kernel\n"
     ]
    }
   ],
   "source": [
    "# initialise trainer (we don't want to train, but model is tightly coupled with trainer)\n",
    "asr_brain = ASR(\n",
    "    modules=speechbrain_hparams[\"modules\"],\n",
    "    opt_class=speechbrain_hparams[\"opt_class\"],\n",
    "    hparams=speechbrain_hparams,\n",
    "    checkpointer=speechbrain_hparams[\"checkpointer\"],\n",
    ")\n",
    "\n",
    "def setup_asr_brain_for_infer(asr_brain):\n",
    "    asr_brain.on_evaluate_start(min_key=\"WER\") # We call the on_evaluate_start that will load the best model\n",
    "    asr_brain.modules.eval() # We set the model to eval mode (remove dropout etc)\n",
    "\n",
    "print(\"if on_evaluate_start() get runtime error, likely need to restart notebook kernel\")\n",
    "setup_asr_brain_for_infer(asr_brain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "909b3c39-3a03-4384-82ca-2305b3ed2272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset and dataloader for inference\n",
    "datasets = dataio_prepare(speechbrain_hparams)\n",
    "\n",
    "test_set = datasets['test']\n",
    "\n",
    "if not isinstance(test_set, DataLoader) or isinstance(test_set, LoopedLoader):\n",
    "    test_loader_kwargs=speechbrain_hparams[\"test_dataloader_opts\"]\n",
    "    test_set = asr_brain.make_dataloader(\n",
    "        test_set, stage=sb.Stage.TEST, **test_loader_kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69b4b4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ⁇ ', '', 'e', 't', 'o', 'a', 'n', 'i', 's', 'r', 'h', 'd', 'l', 'c', 'f', 'u', 'm', 'w', 'p', 'g', 'y', 'b', 'v', 'k', 'x', 'q', 'j', 'z']\n",
      "['-', '|', 'e', 't', 'o', 'a', 'n', 'i', 's', 'r', 'h', 'd', 'l', 'c', 'f', 'u', 'm', 'w', 'p', 'g', 'y', 'b', 'v', 'k', 'x', 'q', 'j', 'z']\n"
     ]
    }
   ],
   "source": [
    "# get vocab from tokenizer (needed for ctc decoding)\n",
    "vocab_size = len(asr_brain.hparams.tokenizer)\n",
    "vocab = []\n",
    "for i in range(vocab_size):\n",
    "    vocab.append(asr_brain.hparams.tokenizer.decode_ids([i]))\n",
    "print(vocab)\n",
    "\n",
    "# edit vocab to match default ctc decoder symbols for blank and silence\n",
    "vocab[0] = '-'\n",
    "vocab[1] = \"|\"\n",
    "\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6af49d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctc_beamsearch_decoder = ctc_decoder(\n",
    "    lexicon=None,\n",
    "    # tokens=\"/home/s1785140/rlspeller/templates/speech_recognition_CharTokens_NoLM/Tokenizer/save/tokens.txt\",\n",
    "    tokens=vocab,\n",
    "    nbest=100,\n",
    "    blank_token='-',\n",
    "    sil_token=\"|\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4d6501c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                               | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG batch: <speechbrain.dataio.batch.PaddedBatch object at 0x7fa97cf9e220>\n",
      "DEBUG use mel inputs: True\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ASR' object has no attribute 'prepare_features_mel'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 54\u001b[0m\n\u001b[1;32m     50\u001b[0m             transcripts\u001b[39m.\u001b[39mappend(predicted_words)\n\u001b[1;32m     52\u001b[0m     \u001b[39mreturn\u001b[39;00m transcripts, ctc_probs\n\u001b[0;32m---> 54\u001b[0m transcripts, ctc_probs \u001b[39m=\u001b[39m transcribe_dataset(asr_brain, test_set, greedy\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, num_batches_to_transcribe\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[24], line 12\u001b[0m, in \u001b[0;36mtranscribe_dataset\u001b[0;34m(asr_brain, dataset, greedy, num_batches_to_transcribe)\u001b[0m\n\u001b[1;32m      7\u001b[0m orig_transcriptions \u001b[39m=\u001b[39m batch\u001b[39m.\u001b[39mwords\n\u001b[1;32m      9\u001b[0m \u001b[39m# Make sure that your compute_forward returns the predictions !!!\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39m# In the case of the template, when stage = TEST, a beam search is applied \u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39m# in compute_forward(). \u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m predictions \u001b[39m=\u001b[39m asr_brain\u001b[39m.\u001b[39;49mcompute_forward(batch, stage\u001b[39m=\u001b[39;49msb\u001b[39m.\u001b[39;49mStage\u001b[39m.\u001b[39;49mTEST)\n\u001b[1;32m     14\u001b[0m ctc_probs \u001b[39m=\u001b[39m predictions[\u001b[39m'\u001b[39m\u001b[39mctc_logprobs\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m# FOR DEBUG\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39mif\u001b[39;00m greedy:\n",
      "File \u001b[0;32m/disk/nfs/ostrom/s1785140/speechbrain/templates/speech_recognition_CharTokens_NoLM/ASR/train.py:109\u001b[0m, in \u001b[0;36mASR.compute_forward\u001b[0;34m(self, batch, stage)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDEBUG use mel inputs: \u001b[39m\u001b[39m{\u001b[39;00muse_mel_inputs\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    108\u001b[0m \u001b[39mif\u001b[39;00m use_mel_inputs:\n\u001b[0;32m--> 109\u001b[0m     feats, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeat_lens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprepare_features_mel(stage, batch\u001b[39m.\u001b[39mmel)\n\u001b[1;32m    110\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m     \u001b[39m# NOTE CAREFUL!!! self.feat_lens are not mel lens but ratios from 0.0 to 1.0 (ratio of wav len to max wav len)\u001b[39;00m\n\u001b[1;32m    112\u001b[0m     feats, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeat_lens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_features(stage, batch\u001b[39m.\u001b[39msig, batch\u001b[39m.\u001b[39mwav_path)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ASR' object has no attribute 'prepare_features_mel'"
     ]
    }
   ],
   "source": [
    "# generate transcriptions for all batches in test set\n",
    "def transcribe_dataset(asr_brain, dataset, greedy=False, num_batches_to_transcribe=None):\n",
    "    # Now we iterate over the dataset and we simply compute_forward and decode\n",
    "    with torch.no_grad():\n",
    "        transcripts = []\n",
    "        for batch in tqdm(list(dataset)[:num_batches_to_transcribe], dynamic_ncols=True):\n",
    "            orig_transcriptions = batch.words\n",
    "\n",
    "            # Make sure that your compute_forward returns the predictions !!!\n",
    "            # In the case of the template, when stage = TEST, a beam search is applied \n",
    "            # in compute_forward(). \n",
    "            predictions = asr_brain.compute_forward(batch, stage=sb.Stage.TEST)\n",
    "            \n",
    "            ctc_probs = predictions['ctc_logprobs'] # FOR DEBUG\n",
    "\n",
    "            if greedy:\n",
    "                predicted_ids = sb.decoders.ctc_greedy_decode(\n",
    "                    predictions[\"ctc_logprobs\"], asr_brain.feat_lens, blank_id=asr_brain.hparams.blank_index\n",
    "                )\n",
    "                predicted_words = [\n",
    "                    asr_brain.tokenizer.decode_ids(ids).split(\" \")\n",
    "                    for ids in predicted_ids\n",
    "                ]\n",
    "            else:\n",
    "                # get mel lens from wav len ratios since torch ctc decoder requires lens in frames\n",
    "                batch_max_len = predictions[\"ctc_logprobs\"].size(1)\n",
    "                bsz = predictions[\"ctc_logprobs\"].size(0)\n",
    "                mel_lens = torch.zeros(bsz)\n",
    "                for i, len_ratio in enumerate(asr_brain.feat_lens):\n",
    "                    mel_lens[i] = int(torch.round(len_ratio * batch_max_len))\n",
    "                \n",
    "                predicted_ids = ctc_beamsearch_decoder(\n",
    "                    predictions[\"ctc_logprobs\"], lengths=mel_lens\n",
    "                )\n",
    "\n",
    "                predicted_words = []\n",
    "                for i, (utt_id, orig_text, hyps) in enumerate(zip(batch.utt_id, orig_transcriptions, predicted_ids)):\n",
    "                    print(f\"\\nsample {i+1} - ({utt_id}: '{orig_text}')\")\n",
    "                    sample_cers = []\n",
    "                    for j, hyp in enumerate(hyps):\n",
    "                        words = asr_brain.hparams.tokenizer.decode_ids(hyp.tokens.tolist()) # .split(\"|\")\n",
    "                        # words = tokenizer.decode_ids(hyp.tokens.tolist()) # .split(\"|\")\n",
    "                        hyp_cer = 100 * cer(orig_text, words)\n",
    "                        sample_cers.append(hyp_cer)\n",
    "                        print(f\"\\thyp {j+1}/{len(hyps)} (CER={hyp_cer:.1f}%): '{words}'\")\n",
    "                        predicted_words.append((f\"sample {i+1}, hyp {j+1}/{len(hyps)}\", words))\n",
    "                        \n",
    "                    print(f\"\\t=== Mean CER: {np.mean(sample_cers):.1f}%, Std CER: {np.std(sample_cers):.1f}% ===\")\n",
    "\n",
    "            transcripts.append(predicted_words)\n",
    "\n",
    "    return transcripts, ctc_probs\n",
    "\n",
    "transcripts, ctc_probs = transcribe_dataset(asr_brain, test_set, greedy=False, num_batches_to_transcribe=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b822e8c1",
   "metadata": {},
   "source": [
    "# LOAD WORD ALIGNED MELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e05be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imitate CLAs\n",
    "import sys\n",
    "import argparse\n",
    "import math\n",
    "import glob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc7e6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv = [\n",
    "    # speechbrain features\n",
    "    'train.py',\n",
    "    '--type', 'mel',\n",
    "    '--utt_id_list', '/home/s1785140/data/ljspeech_fastpitch/respeller_uttids.txt', \n",
    "    '--input_directory', '/home/s1785140/speechbrain/templates/speech_recognition_CharTokens_NoLM/data/ljspeech_dumped_feats',\n",
    "    '--alignments', '/home/s1785140/speechbrain/templates/speech_recognition_CharTokens_NoLM/data/LJSpeech-1.1/MFA_alignments_lowercase_nopunc', # newer alignments, lowercase no punctuation\n",
    "    '--output_directory', '/home/s1785140/speechbrain/templates/speech_recognition_CharTokens_NoLM/data/ljspeech_dumped_feats_word_aligned',\n",
    "    # '--mel-to-graphemes-ratio-lowest-threshold', '5.5',\n",
    "    # '--mel-to-graphemes-ratio-highest-threshold', '13.5',\n",
    "    '--clean-output-folder',\n",
    "]\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-t', '--type', type=str, default='hubert',\n",
    "                    help='type of input speech reps that we are using, i.e. hubert wav2vec2 etc.')\n",
    "parser.add_argument('--padding_idx_offset', type=int, default=0,\n",
    "                    help='add 1 to token id of discrete reps in order to allow for padding_idx==0')\n",
    "parser.add_argument('--utt_id_list', type=str, required=False, default=\"\",\n",
    "                    help='path to text file that contains list of utterance ids that we extract from')\n",
    "parser.add_argument('-s', '--input_directory', type=str, required=True,\n",
    "                    help='path to single non-nested folder containing speech representations (.pt files) or txt file (hubert)')\n",
    "parser.add_argument('-a', '--alignments', type=str, required=True,\n",
    "                    help='path to single non-nested folder containing MFA alignments (.TextGrid files)')\n",
    "parser.add_argument('-o', '--output_directory', type=str, required=True,\n",
    "                    help='where to write word-level data')\n",
    "parser.add_argument('--max-utts-to-generate', type=int, default=None,\n",
    "                    help='How many utts to extract word aligned speech reps for. If None, extract all utts.')\n",
    "parser.add_argument('--mel-to-graphemes-ratio-lowest-threshold', type=float, default=0.0,\n",
    "                    help='Lowest mel-to-graphemes ratio to consider. (lower ratio means fewer mel frames per grapheme)')\n",
    "parser.add_argument('--mel-to-graphemes-ratio-highest-threshold', type=float, default=math.inf,\n",
    "                    help='Lowest mel-to-graphemes ratio to consider. (higher ratio means more mel frames per grapheme)')\n",
    "parser.add_argument('--clean-output-folder', action=\"store_true\",\n",
    "                    help='Clean output folder before writing new data')\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f170f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "globbed 62116 mels from /home/s1785140/speechbrain/templates/speech_recognition_CharTokens_NoLM/data/ljspeech_dumped_feats_word_aligned.\n"
     ]
    }
   ],
   "source": [
    "# grab all mels and words in the output directory\n",
    "'/home/s1785140/speechbrain/templates/speech_recognition_CharTokens_NoLM/data/ljspeech_dumped_feats_word_aligned',\n",
    "\n",
    "# glob pytorch tensors from nested folders in output directory\n",
    "mel_paths = glob.glob(f'{args.output_directory}/**/*.pt', recursive=True)\n",
    "print(f\"globbed {len(mel_paths)} mels from {args.output_directory}.\")\n",
    "\n",
    "# TODO this cell takes a long time to glob all files in the output directory\n",
    "# perhaps save/load a list of all files in the output directory to a file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edab5ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 62116/62116 [00:00<00:00, 288881.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique word types: 13593\n",
      "Number of unique utterances: 6551\n",
      "Wordtypes with most common occurences: [('one', 450), ('quote', 426), ('oswald', 285), ('two', 281), ('end', 253), ('time', 219), ('would', 211), ('three', 176), ('made', 171), ('upon', 164), ('prisoners', 160), ('man', 159), ('first', 158), ('could', 158), ('president', 158), ('mister', 157), ('prison', 156), ('also', 148), ('newgate', 147), ('great', 140), ('many', 134), ('still', 134), ('house', 133), ('five', 132), ('eighteen', 125), ('service', 124), ('found', 124), ('might', 113), ('new', 113), ('said', 110), ('twenty', 107), ('long', 106), ('four', 104), ('may', 103), ('misess', 103), ('much', 102), ('life', 101), ('dallas', 101), ('years', 100), ('well', 100), ('left', 96), ('street', 95), ('work', 93), ('city', 92), ('secret', 92), ('jail', 90), ('without', 88), ('however', 88), ('police', 87), ('another', 85), ('although', 83), ('part', 83), ('hundred', 82), ('whole', 81), ('men', 80), ('every', 79), ('even', 78), ('like', 78), ('went', 78), ('last', 78), ('death', 77), ('case', 76), ('old', 76), ('good', 76), ('place', 75), ('way', 74), ('pounds', 74), ('six', 74), ('sixty', 73), ('came', 73), ('public', 72), ('taken', 71), ('known', 71), ('back', 70), ('right', 70), ('nineteen', 70), ('people', 69), ('never', 68), ('must', 68), ('thousand', 68), ('side', 68), ('state', 68), ('day', 68), ('number', 68), ('certain', 68), ('others', 67), ('seven', 66), ('thus', 65), ('rifle', 65), ('oswalds', 65), ('eight', 64), ('thirty', 64), ('system', 64), ('used', 63), ('took', 62), ('general', 62), ('body', 62), ('little', 61), ('name', 61), ('called', 60)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def filename_no_ext(path):\n",
    "    return path.split('/')[-1].split('.')[0]\n",
    "\n",
    "def parse_word_token_mel_path(word_token_mel_path):\n",
    "    filename = filename_no_ext(word_token_mel_path)\n",
    "    wordtype, utt_id, occurence_str = filename.split('__')\n",
    "    occurence = occurence_str.lstrip('occ')\n",
    "    return wordtype, utt_id, occurence\n",
    "\n",
    "# print some statistics about the data\n",
    "wordtypes = Counter()\n",
    "utt_ids = set()\n",
    "for mel_path in tqdm(mel_paths):\n",
    "    wordtype, utt_id, occurence = parse_word_token_mel_path(mel_path)\n",
    "    wordtypes[wordtype] += 1\n",
    "    utt_ids.add(utt_id)\n",
    "\n",
    "print(f\"Number of unique word types: {len(wordtypes)}\")\n",
    "print(f\"Number of unique utterances: {len(utt_ids)}\")\n",
    "print(f\"Wordtypes with most common occurences: {wordtypes.most_common(100)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e059da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wordtypes with least common occurences: [('lessening', 1), ('hornig', 1), ('donald', 1), ('increases', 1), ('symbolizes', 1), ('codify', 1), ('endorses', 1), ('preferable', 1), ('agrees', 1), ('experimented', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Wordtypes with least common occurences: {wordtypes.most_common()[:-10-1:-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69244bb3",
   "metadata": {},
   "source": [
    "## create datadicts\n",
    "\n",
    "which split the word token mel paths into train valid and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22471905",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1337\n",
    "if seed is not None:\n",
    "    random.seed(seed)\n",
    "random.shuffle(mel_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5e23f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62116 == 55904(train) + 3105(valid) + 3107(test)\n",
      "train_mel_paths:\n",
      "['/home/s1785140/speechbrain/templates/speech_recognition_CharTokens_NoLM/data/ljspeech_dumped_feats_word_aligned/crash/crash__LJ015-0154__occ1.pt', '/home/s1785140/speechbrain/templates/speech_recognition_CharTokens_NoLM/data/ljspeech_dumped_feats_word_aligned/paid/paid__LJ016-0225__occ1.pt', '/home/s1785140/speechbrain/templates/speech_recognition_CharTokens_NoLM/data/ljspeech_dumped_feats_word_aligned/charge/charge__LJ010-0273__occ1.pt', '/home/s1785140/speechbrain/templates/speech_recognition_CharTokens_NoLM/data/ljspeech_dumped_feats_word_aligned/certain/certain__LJ002-0184__occ1.pt', '/home/s1785140/speechbrain/templates/speech_recognition_CharTokens_NoLM/data/ljspeech_dumped_feats_word_aligned/mother/mother__LJ018-0234__occ1.pt']\n",
      "valid_mel_paths:\n",
      "['/home/s1785140/speechbrain/templates/speech_recognition_CharTokens_NoLM/data/ljspeech_dumped_feats_word_aligned/caused/caused__LJ013-0178__occ1.pt', '/home/s1785140/speechbrain/templates/speech_recognition_CharTokens_NoLM/data/ljspeech_dumped_feats_word_aligned/right/right__LJ016-0325__occ1.pt', '/home/s1785140/speechbrain/templates/speech_recognition_CharTokens_NoLM/data/ljspeech_dumped_feats_word_aligned/known/known__LJ026-0060__occ1.pt', '/home/s1785140/speechbrain/templates/speech_recognition_CharTokens_NoLM/data/ljspeech_dumped_feats_word_aligned/east/east__LJ039-0159__occ1.pt', '/home/s1785140/speechbrain/templates/speech_recognition_CharTokens_NoLM/data/ljspeech_dumped_feats_word_aligned/mans/mans__LJ014-0083__occ1.pt']\n",
      "test_mel_paths:\n",
      "['/home/s1785140/speechbrain/templates/speech_recognition_CharTokens_NoLM/data/ljspeech_dumped_feats_word_aligned/happened/happened__LJ028-0276__occ1.pt', '/home/s1785140/speechbrain/templates/speech_recognition_CharTokens_NoLM/data/ljspeech_dumped_feats_word_aligned/jeremy/jeremy__LJ006-0030__occ1.pt', '/home/s1785140/speechbrain/templates/speech_recognition_CharTokens_NoLM/data/ljspeech_dumped_feats_word_aligned/rather/rather__LJ027-0176__occ1.pt', '/home/s1785140/speechbrain/templates/speech_recognition_CharTokens_NoLM/data/ljspeech_dumped_feats_word_aligned/president/president__LJ046-0210__occ1.pt', '/home/s1785140/speechbrain/templates/speech_recognition_CharTokens_NoLM/data/ljspeech_dumped_feats_word_aligned/activities/activities__LJ048-0080__occ1.pt']\n"
     ]
    }
   ],
   "source": [
    "# Create train dev test splits\n",
    "def get_random_datasplits(\n",
    "    a_list, \n",
    "    ratios,  # [train, valid, test]\n",
    "):\n",
    "    assert sum(ratios) == 1\n",
    "    train_ratio, valid_ratio, test_ratio = ratios \n",
    "\n",
    "    N = len(a_list)\n",
    "    \n",
    "    train = a_list[:int(train_ratio*N)]\n",
    "    if test_ratio != 0.0:\n",
    "        valid = a_list[int(train_ratio*N):int(train_ratio*N)+int(valid_ratio*N)]\n",
    "        test = a_list[int(train_ratio*N)+int(valid_ratio*N):]\n",
    "    else:\n",
    "        valid = a_list[int(train_ratio*N):]\n",
    "        test = []\n",
    "    assert N == len(train) + len(valid) + len(test), f\"{N} == {len(train)} + {len(valid)} + {len(test)}\"\n",
    "    print(f\"{N} == {len(train)}(train) + {len(valid)}(valid) + {len(test)}(test)\")\n",
    "    return train, valid, test\n",
    "\n",
    "train_mel_paths, valid_mel_paths, test_mel_paths = get_random_datasplits(\n",
    "    mel_paths, \n",
    "    ratios=[0.9, 0.05, 0.05],\n",
    ")\n",
    "\n",
    "# print 10 samples from each split\n",
    "print(\"train_mel_paths:\")\n",
    "print(train_mel_paths[:5])\n",
    "print(\"valid_mel_paths:\")\n",
    "print(valid_mel_paths[:5])\n",
    "print(\"test_mel_paths:\")\n",
    "print(test_mel_paths[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c51719b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 55904/55904 [00:00<00:00, 270007.90it/s]\n",
      "100%|█████████████████████████████████████████████| 3105/3105 [00:00<00:00, 195960.12it/s]\n",
      "100%|█████████████████████████████████████████████| 3107/3107 [00:00<00:00, 202208.06it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"create datadicts\n",
    "{\n",
    "    \"<wordtype>__LJ001-0001__occ1\": {\n",
    "        \"word_token_mel_path\": \"/home/s1785140/speechbrain/templates/speech_recognition_CharTokens_NoLM/data/ljspeech_dumped_feats_word_aligned/<wordtype>__LJ001-0001__occ1.pt\",\n",
    "    },\n",
    "    \"<wordtype>__LJ001-0001__occ2\": {\n",
    "        \"word_token_mel_path\": \"/home/s1785140/speechbrain/templates/speech_recognition_CharTokens_NoLM/data/ljspeech_dumped_feats_word_aligned/<wordtype>__LJ001-0001__occ2.pt\",\n",
    "    },\n",
    "}\n",
    "\"\"\"\n",
    "datadicts = {}\n",
    "\n",
    "def create_datadict(mel_paths):\n",
    "    datadict = {}\n",
    "    for mel_path in tqdm(mel_paths):\n",
    "        key = filename_no_ext(mel_path)\n",
    "        datadict[key] = {\n",
    "            \"word_token_mel_path\": mel_path,\n",
    "            # \"num_frames\": mel_num_frames, # TODO\n",
    "        }\n",
    "    return datadict\n",
    "\n",
    "datadicts[\"train\"] = create_datadict(train_mel_paths)\n",
    "datadicts[\"valid\"] = create_datadict(valid_mel_paths)\n",
    "datadicts[\"test\"] = create_datadict(test_mel_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5370e4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bbaa3211",
   "metadata": {},
   "source": [
    "## create speechbrain dataset to load word aligned mels for all word types/word tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93adf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataio_prepare(hparams):\n",
    "    \"\"\"This function prepares the datasets to be used in the brain class.\n",
    "    It also defines the data processing pipeline through user-defined functions.\n",
    "\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    hparams : dict\n",
    "        This dictionary is loaded from the `train.yaml` file, and it includes\n",
    "        all the hyperparameters needed for dataset construction and loading.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    datasets : dict\n",
    "        Dictionary containing \"train\", \"valid\", and \"test\" keys that correspond\n",
    "        to the DynamicItemDataset objects.\n",
    "    \"\"\"\n",
    "    # Define audio pipeline. In this case, we simply read the path contained\n",
    "    # in the variable wav with the audio reader.\n",
    "    @sb.utils.data_pipeline.takes(\"word_token_mel_path\")\n",
    "    @sb.utils.data_pipeline.provides(\"words\", \"utt_id\", \"occurence\", \"mel\")\n",
    "    def audio_pipeline(word_token_mel_path):\n",
    "        \"\"\"Load the audio signal. This is done on the CPU in the `collate_fn`.\"\"\"\n",
    "        words, utt_id, occurence = parse_word_token_mel_path(word_token_mel_path)\n",
    "        yield words # NOTE just a wordtype actually, but call it words to be consistent for dataloaders for standard ASR training\n",
    "        yield utt_id\n",
    "        yield occurence\n",
    "        # yield num_frames # TODO \n",
    "\n",
    "        mel = torch.load(word_token_mel_path)\n",
    "        yield mel\n",
    "\n",
    "        # TODO also yield mel for calculating fastpitch softdtw loss\n",
    "\n",
    "\n",
    "    # Define datasets from json data manifest file\n",
    "    # Define datasets sorted by ascending lengths for efficiency\n",
    "    datasets = {}\n",
    "    splits = {\"train\", \"valid\", \"test\"}\n",
    "\n",
    "    for split in splits:\n",
    "        datasets[split] = sb.dataio.dataset.DynamicItemDataset(\n",
    "            data=datadicts[split],\n",
    "            dynamic_items=[audio_pipeline],\n",
    "            output_keys=[\n",
    "                \"words\", \"utt_id\", \"occurence\", \"mel\"\n",
    "            ],\n",
    "        )\n",
    "        hparams[f\"{split}_dataloader_opts\"][\"shuffle\"] = False\n",
    "\n",
    "    # TODO uncomment this!!!\n",
    "\n",
    "    # # Sorting training data with ascending order makes the code  much\n",
    "    # # faster  because we minimize zero-padding. In most of the cases, this\n",
    "    # # does not harm the performance.\n",
    "    # if hparams[\"sorting\"] == \"ascending\":\n",
    "    #     datasets[\"train\"] = datasets[\"train\"].filtered_sorted(sort_key=\"length\")\n",
    "    #     hparams[\"train_dataloader_opts\"][\"shuffle\"] = False\n",
    "\n",
    "    # elif hparams[\"sorting\"] == \"descending\":\n",
    "    #     datasets[\"train\"] = datasets[\"train\"].filtered_sorted(\n",
    "    #         sort_key=\"length\", reverse=True\n",
    "    #     )\n",
    "    #     hparams[\"train_dataloader_opts\"][\"shuffle\"] = False\n",
    "\n",
    "    # elif hparams[\"sorting\"] == \"random\":\n",
    "    #     hparams[\"train_dataloader_opts\"][\"shuffle\"] = True\n",
    "    #     pass\n",
    "\n",
    "    # else:\n",
    "    #     raise NotImplementedError(\n",
    "    #         \"sorting must be random, ascending or descending\"\n",
    "    #     )\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "datasets = dataio_prepare(speechbrain_hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15de5d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert from datasets to dataloaders\n",
    "split2stage = {\"train\": sb.Stage.TRAIN, \"valid\": sb.Stage.VALID, \"test\": sb.Stage.TEST}\n",
    "for split in [\"train\", \"valid\", \"test\"]:\n",
    "    if not isinstance(datasets[split], DataLoader) or isinstance(datasets[split], LoopedLoader):\n",
    "        dataloader_kwargs=speechbrain_hparams[f\"{split}_dataloader_opts\"]\n",
    "        datasets[split] = asr_brain.make_dataloader(\n",
    "            datasets[split], stage=split2stage[split], **dataloader_kwargs\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b952a0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                            | 0/6988 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'PaddedBatch' object has no attribute 'sig'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[333], line 60\u001b[0m\n\u001b[1;32m     56\u001b[0m             transcripts\u001b[39m.\u001b[39mappend(predicted_words)\n\u001b[1;32m     58\u001b[0m     \u001b[39mreturn\u001b[39;00m transcripts, ctc_probs\n\u001b[0;32m---> 60\u001b[0m transcripts, ctc_probs \u001b[39m=\u001b[39m transcribe_dataset(asr_brain, datasets[\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m], \n\u001b[1;32m     61\u001b[0m                                             greedy\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, num_batches_to_transcribe\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[333], line 18\u001b[0m, in \u001b[0;36mtranscribe_dataset\u001b[0;34m(asr_brain, dataset, greedy, num_batches_to_transcribe)\u001b[0m\n\u001b[1;32m     13\u001b[0m orig_transcriptions \u001b[39m=\u001b[39m batch\u001b[39m.\u001b[39mwords\n\u001b[1;32m     15\u001b[0m \u001b[39m# Make sure that your compute_forward returns the predictions !!!\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39m# In the case of the template, when stage = TEST, a beam search is applied \u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39m# in compute_forward(). \u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m predictions \u001b[39m=\u001b[39m asr_brain\u001b[39m.\u001b[39;49mcompute_forward(batch, stage\u001b[39m=\u001b[39;49msb\u001b[39m.\u001b[39;49mStage\u001b[39m.\u001b[39;49mTEST)\n\u001b[1;32m     20\u001b[0m ctc_probs \u001b[39m=\u001b[39m predictions[\u001b[39m'\u001b[39m\u001b[39mctc_logprobs\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m# FOR DEBUG\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39mif\u001b[39;00m greedy:\n",
      "File \u001b[0;32m/disk/nfs/ostrom/s1785140/speechbrain/templates/speech_recognition_CharTokens_NoLM/ASR/train.py:100\u001b[0m, in \u001b[0;36mASR.compute_forward\u001b[0;34m(self, batch, stage)\u001b[0m\n\u001b[1;32m     97\u001b[0m batch \u001b[39m=\u001b[39m batch\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     99\u001b[0m use_mel_inputs \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhparams, \u001b[39m\"\u001b[39m\u001b[39muse_mel_inputs\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    101\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhparams\u001b[39m.\u001b[39muse_mel_inputs:\n\u001b[1;32m    102\u001b[0m         use_mel_inputs \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PaddedBatch' object has no attribute 'sig'"
     ]
    }
   ],
   "source": [
    "# generate transcriptions for all batches in test set\n",
    "def transcribe_dataset(asr_brain, dataset, greedy=False, num_batches_to_transcribe=None):\n",
    "    # Now we iterate over the dataset and we simply compute_forward and decode\n",
    "    with torch.no_grad():\n",
    "        transcripts = []\n",
    "        n = 0 # number of batches transcribed\n",
    "        for batch in tqdm(dataset, dynamic_ncols=True):\n",
    "            # break out of loop if we have transcribed enough batches\n",
    "            if n >= num_batches_to_transcribe:\n",
    "                break\n",
    "            n += 1\n",
    "\n",
    "            orig_transcriptions = batch.words\n",
    "\n",
    "            # Make sure that your compute_forward returns the predictions !!!\n",
    "            # In the case of the template, when stage = TEST, a beam search is applied \n",
    "            # in compute_forward(). \n",
    "            predictions = asr_brain.compute_forward(batch, stage=sb.Stage.TEST)\n",
    "            \n",
    "            ctc_probs = predictions['ctc_logprobs'] # FOR DEBUG\n",
    "\n",
    "            if greedy:\n",
    "                predicted_ids = sb.decoders.ctc_greedy_decode(\n",
    "                    predictions[\"ctc_logprobs\"], asr_brain.feat_lens, blank_id=asr_brain.hparams.blank_index\n",
    "                )\n",
    "                predicted_words = [\n",
    "                    asr_brain.tokenizer.decode_ids(ids).split(\" \")\n",
    "                    for ids in predicted_ids\n",
    "                ]\n",
    "            else:\n",
    "                # get mel lens from wav len ratios since torch ctc decoder requires lens in frames\n",
    "                batch_max_len = predictions[\"ctc_logprobs\"].size(1)\n",
    "                bsz = predictions[\"ctc_logprobs\"].size(0)\n",
    "                mel_lens = torch.zeros(bsz)\n",
    "                for i, len_ratio in enumerate(asr_brain.feat_lens):\n",
    "                    mel_lens[i] = int(torch.round(len_ratio * batch_max_len))\n",
    "                \n",
    "                predicted_ids = ctc_beamsearch_decoder(\n",
    "                    predictions[\"ctc_logprobs\"], lengths=mel_lens\n",
    "                )\n",
    "\n",
    "                predicted_words = []\n",
    "                for i, (utt_id, orig_text, hyps) in enumerate(zip(batch.utt_id, orig_transcriptions, predicted_ids)):\n",
    "                    print(f\"\\nsample {i+1} - ({utt_id}: '{orig_text}')\")\n",
    "                    sample_cers = []\n",
    "                    for j, hyp in enumerate(hyps):\n",
    "                        words = asr_brain.hparams.tokenizer.decode_ids(hyp.tokens.tolist()) # .split(\"|\")\n",
    "                        # words = tokenizer.decode_ids(hyp.tokens.tolist()) # .split(\"|\")\n",
    "                        hyp_cer = 100 * cer(orig_text, words)\n",
    "                        sample_cers.append(hyp_cer)\n",
    "                        print(f\"\\thyp {j+1}/{len(hyps)} (CER={hyp_cer:.1f}%): '{words}'\")\n",
    "                        predicted_words.append((f\"sample {i+1}, hyp {j+1}/{len(hyps)}\", words))\n",
    "                        \n",
    "                    print(f\"\\t=== Mean CER: {np.mean(sample_cers):.1f}%, Std CER: {np.std(sample_cers):.1f}% ===\")\n",
    "\n",
    "            transcripts.append(predicted_words)\n",
    "\n",
    "    return transcripts, ctc_probs\n",
    "\n",
    "transcripts, ctc_probs = transcribe_dataset(asr_brain, datasets[\"train\"], \n",
    "                                            greedy=False, num_batches_to_transcribe=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272fbeeb",
   "metadata": {},
   "source": [
    "# TRANSCRIBE WORD ALIGNED MELS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "6a8422eeb13ba8f92f71047f64b5c33152e234c2bbad3f45433feda7b6f3b4c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
