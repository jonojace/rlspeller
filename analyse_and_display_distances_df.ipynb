{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "91a9abca-1158-4de9-99f2-0d4bdaad22d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "from IPython.display import Audio\n",
    "import pickle\n",
    "import glob\n",
    "import os.path\n",
    "from tqdm import tqdm\n",
    "import scipy\n",
    "from scipy.io.wavfile import write\n",
    "import shutil\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9b88694-9ef0-48cf-ab96-fc2d8ca192a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "GT_SAMPLE_RATE = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7682857b-1abe-40cd-adf4-10c2728cfe6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "HIFIGAN_SAMPLE_RATE = 22050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfdd00a2-7ae8-4dfb-af91-3a54b6f8553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63c6a945-cbab-431d-ba1d-b1c03a90a6ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded cache at /home/s1785140/rlspeller/caches/spelling2vocoded_audio_cache.pkl\n"
     ]
    }
   ],
   "source": [
    "def load_cache(cache_path):\n",
    "    if os.path.exists(cache_path):\n",
    "        with open(cache_path, \"rb\") as f:\n",
    "            cache = pickle.load(f)\n",
    "        print(\"loaded cache at\", cache_path)\n",
    "    else:\n",
    "        print(\"WARNING NO CACHE NOT FOUND AT\", cache_path, 'returning empty one')\n",
    "        cache = {}\n",
    "    return cache\n",
    "\n",
    "spelling2vocoded_audio = load_cache(\"/home/s1785140/rlspeller/caches/spelling2vocoded_audio_cache.pkl\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "737aafc5-d6e3-4a83-a996-41c1016f6480",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test_dump_feats',\n",
       " 'distances_df_16-03-2023_16:08:58.pkl',\n",
       " 'distances_df_16-03-2023_16:12:59.pkl',\n",
       " 'distances_df_16-Mar-2023_16:16:57.pkl',\n",
       " 'distances_df_16-Mar-2023_16:42:14.pkl',\n",
       " 'distances_df_16-Mar-2023_16:46:34.pkl',\n",
       " 'distances_df_16-Mar-2023_16:55:34.pkl',\n",
       " 'distances_df_16-Mar-2023_16:59:52.pkl',\n",
       " 'distances_df_16-Mar-2023_17:14:10.pkl',\n",
       " 'distances_df_17-Mar-2023_00:55:15.pkl',\n",
       " 'distances_df_27-Mar-2023_12:38:45.pkl',\n",
       " 'distances_df_27-Mar-2023_13:04:06.pkl',\n",
       " 'distances_df_27-Mar-2023_15:06:19.pkl',\n",
       " 'distances_df_27-Mar-2023_15:44:02.pkl',\n",
       " 'distances_df_28-Mar-2023_12:52:18.pkl',\n",
       " 'distances_df_28-Mar-2023_17:13:48.pkl',\n",
       " 'distances_df_29-Mar-2023_11:10:08.pkl',\n",
       " 'distances_df_31-Mar-2023_20:50:35.pkl',\n",
       " 'distances_df_01-Apr-2023_13:05:28.pkl',\n",
       " 'distances_df_experiment1.pkl']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_outdir = os.path.join(os.getcwd(), 'results')\n",
    "os.listdir(dataframe_outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91c75876-a9d0-4d95-934c-61bc2258fda5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/disk/nfs/ostrom/s1785140/rlspeller/results/distances_df_experiment1.pkl'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get latest created file in output folder \n",
    "def get_latest_file(in_dir, file_type):\n",
    "    files = glob.glob(os.path.join(in_dir, file_type))\n",
    "    return max(files, key=os.path.getctime)\n",
    "\n",
    "df_to_load = \"\"\n",
    "# df_to_load = '/disk/nfs/ostrom/s1785140/rlspeller/results/distances_df_17-Mar-2023_00:55:15.pkl' # first big test of pipeline\n",
    "# df_to_load = '/disk/nfs/ostrom/s1785140/rlspeller/results/distances_df_27-Mar-2023_13:04:06.pkl' # first test of 1000 spellings per wordtype (no dropout)\n",
    "# df_to_load = '/disk/nfs/ostrom/s1785140/rlspeller/results/distances_df_experiment1.pkl'\n",
    "\n",
    "if not df_to_load:\n",
    "    df_to_load = get_latest_file(dataframe_outdir, r'*.pkl')\n",
    "    \n",
    "df_to_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "353e7e3c-3013-479e-8d25-ed8bf686add3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataframe from disk\n",
    "df = pd.read_pickle(df_to_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "005c5c49-e00a-4951-aeee-a838f4499de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mfcc::Distance(Euclidean)\n",
      "mfcc::Distance(Euclidean)::w_SL_Penalty\n",
      "hubert-raw::Distance(Cosine)\n",
      "hubert-raw::Distance(Cosine)::w_SL_Penalty\n",
      "hubert-soft::Distance(Cosine)\n",
      "hubert-soft::Distance(Cosine)::w_SL_Penalty\n",
      "hubert-discrete-code::Distance(Levenshtein)\n",
      "hubert-discrete-code::Distance(Levenshtein)::w_SL_Penalty\n",
      "hubert-discrete-centroid::Distance(Cosine)\n",
      "hubert-discrete-centroid::Distance(Cosine)::w_SL_Penalty\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['mfcc==Distance(Euclidean)', 'mfcc==Distance(Euclidean)==w_SL_Penalty',\n",
       "       'hubert-raw==Distance(Cosine)',\n",
       "       'hubert-raw==Distance(Cosine)==w_SL_Penalty',\n",
       "       'hubert-soft==Distance(Cosine)',\n",
       "       'hubert-soft==Distance(Cosine)==w_SL_Penalty',\n",
       "       'hubert-discrete-code==Distance(Levenshtein)',\n",
       "       'hubert-discrete-code==Distance(Levenshtein)==w_SL_Penalty',\n",
       "       'hubert-discrete-centroid==Distance(Cosine)',\n",
       "       'hubert-discrete-centroid==Distance(Cosine)==w_SL_Penalty'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_columns = df.columns[df.columns.str.contains(\"::\")]\n",
    "for col in distance_columns:\n",
    "    print(col)\n",
    "    new_col = col.replace('::', '==')\n",
    "    df.rename(columns={col: new_col}, inplace=True)\n",
    "df.columns[df.columns.str.contains(\"==\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac4f0fec-9ccb-47d4-92a5-3a2f8c992d06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# exclude wordtypes \n",
    "# i.e. \n",
    "    # because they are not pronounced incorrectly by TTS\n",
    "    # bad alignment\n",
    "bad_alignments = [\n",
    "    'subpoenaed',\n",
    "    'deprecated',\n",
    "    'habitually',\n",
    "    'hogshead',\n",
    "    'schleiden',\n",
    "    'sabotage',\n",
    "    'schoeffer',\n",
    "] \n",
    "\n",
    "pronounced_ok_by_tts = [\n",
    "    # don't add any words here! we are ok if TTS pronounces original spelling ok, we might be able to find a better pronunciation with our spelling pipeline anyway\n",
    "]\n",
    "    \n",
    "wordtypes_to_exclude = bad_alignments + pronounced_ok_by_tts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b81e485-8f83-4653-b839-f2bb06c1fa46",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-filter df has 246 total wordtypes\n"
     ]
    }
   ],
   "source": [
    "print(\"pre-filter df has\", len(df['wordtype'].unique()), \"total wordtypes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d2adc74-5691-45a0-8a1c-07f4bc01bfed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df[~df['wordtype'].isin(wordtypes_to_exclude)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5f8ecbf-b8b6-4aed-92b2-99c8b2ea7df1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered df has 246 total wordtypes\n"
     ]
    }
   ],
   "source": [
    "print(\"filtered df has\", len(df['wordtype'].unique()), \"total wordtypes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe566f5a-28ec-4016-920d-9f96579cac76",
   "metadata": {},
   "source": [
    "# display n-best ranked spellings for a distance metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc6b23b0-b53a-4393-9c01-d01fb454e40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column to indicate whether it is original spelling or not\n",
    "new_col_idx = 1 + df.columns.get_loc('respelling') # insert after respelling column\n",
    "new_col_name = 'is_orig'\n",
    "df.insert(new_col_idx, new_col_name, df['wordtype'] == df['respelling'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28769b63-2c36-449f-9f2e-354b2e49fea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordtype2numspellings = {} #excludes original spelling\n",
    "for wordtype in df['wordtype'].unique():\n",
    "    count = len(df[(df['wordtype'] == wordtype) & (df['is_orig'] == False)])\n",
    "    wordtype2numspellings[wordtype] = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59b73a39-781a-4eb3-9092-05c0a099159a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['conducive', 'patriotic', 'exertion', 'exhaustion', 'soubriquet',\n",
       "       'borsippa', 'minorities', 'fatigues', 'narcotics', 'monitored',\n",
       "       'discouraging', 'spirituous', 'czolgosz', 'psychiatrist',\n",
       "       'brochure', 'juveniles', 'ostentatious', 'inveigled', 'subsidiary',\n",
       "       'nonpartisan', 'sepulchres', 'gonzalez', 'injuring', 'phosphorus',\n",
       "       'penknife', 'collared', 'jealousy', 'courtesy', 'shropshire',\n",
       "       'pecuniary', 'gloucester', 'erasures', 'usurious', 'silhouette',\n",
       "       'alleging', 'juvenile', 'executor', 'nourished', 'dastardly',\n",
       "       'exhorted', 'uncombed', 'overseer', 'etiquette', 'paradise',\n",
       "       'itinerary', 'nauseous', 'exertions', 'adhering', 'brougham',\n",
       "       'prerogative', 'exorbitant', 'vestiges', 'southern', 'hereford',\n",
       "       'beaumont', 'baronetcy', 'ferocious', 'horizontal', 'amusement',\n",
       "       'courtyards', 'brazilian', 'launched', 'morality', 'recalcitrant',\n",
       "       'punitive', 'nicholas', 'spiritual', 'tocqueville', 'aspirants',\n",
       "       'frenzied', 'attribute', 'khrushchev', 'longevity', 'sneering',\n",
       "       'vacillating', 'saccharine', 'exhortations', 'rothschilds',\n",
       "       'bonaventura', 'shredded', 'fastidious', 'alexanders', 'families',\n",
       "       'forgetfulness', 'barthelemy', 'exonerated', 'rouleaux',\n",
       "       'colleagues', 'devilish', 'laborers', 'ethelred', 'calamity',\n",
       "       'confided', 'exuberant', 'executors', 'bequeathed', 'appetite',\n",
       "       'exhorting', 'speeches', 'merionethshire', 'obscenity', 'cadillac',\n",
       "       'individuality', 'laughter', 'protests', 'matrices', 'terrestrial',\n",
       "       'boisterous', 'venetian', 'insecure', 'habitual', 'european',\n",
       "       'escritoire', 'marseilles', 'competitive', 'mesopotamia',\n",
       "       'persevered', 'turreted', 'seleucia', 'solemnly', 'erroneous',\n",
       "       'respited', 'shepherds', 'invoices', 'karswell', 'executing',\n",
       "       'intellectual', 'maryland', 'immature', 'exhortation', 'annulled',\n",
       "       'thinness', 'advertised', 'parasitic', 'bulwarks', 'hammurabi',\n",
       "       'giovanni', 'perished', 'charlotte', 'heterogeneous', 'monsieur',\n",
       "       'dovetail', 'momentary', 'interred', 'champagne', 'collegians',\n",
       "       'alexander', 'mortgages', 'theodore', 'colonial', 'prestige',\n",
       "       'minarets', 'pugnacious', 'burchell', 'sassanian', 'durability',\n",
       "       'presided', 'endorsed', 'loitering', 'staffordshire', 'engendered',\n",
       "       'caravans', 'priority', 'aligning', 'specimen', 'connived',\n",
       "       'decorated', 'delicate', 'outshone', 'debutant', 'therapist',\n",
       "       'flintshire', 'entourage', 'premising', 'susannah', 'napoleon',\n",
       "       'accelerator', 'harassed', 'cribbage', 'sepulchre', 'consignees',\n",
       "       'cupidity', 'peninsula', 'definitive', 'ehrenberg', 'monastery',\n",
       "       'unclothed', 'decorous', 'forenoon', 'calamitous', 'roughened',\n",
       "       'guiseppe', 'presides', 'macintosh', 'eventual', 'unearthed',\n",
       "       'oscillating', 'reverend', 'minority', 'maecenas', 'decorations',\n",
       "       'escalade', 'auditorium', 'boulogne', 'coughing', 'heiresses',\n",
       "       'exploited', 'parasitically', 'forgiveness', 'barouche',\n",
       "       'theodosius', 'asparagus', 'epithets', 'discourage', 'clangour',\n",
       "       'allegiance', 'adherence', 'colonization', 'phosphoric',\n",
       "       'battalion', 'deterred', 'analyzed', 'deutsche', 'charioteers',\n",
       "       'compromised', 'mediterranean', 'reservoir', 'disinterring',\n",
       "       'elliptical', 'effluvia', 'grimaldi', 'respiratory', 'portuguese',\n",
       "       'augmented', 'peninsular', 'cemetery', 'cherished', 'salubrious',\n",
       "       'peripheral', 'licensed', 'experimental', 'capillary', 'righteous',\n",
       "       'exhaustive', 'ridicule', 'gloucestershire'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordtypes = df[\"wordtype\"].unique()\n",
    "print(len(wordtypes))\n",
    "wordtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d55cfc95-e52c-4ff2-af56-2d95bd0c0b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using hubert-soft==Distance(Cosine) to rank spellings\n"
     ]
    }
   ],
   "source": [
    "# Select column that we sort by\n",
    "\n",
    "# distance_metric_col_name = \"mfcc==Distance(Euclidean)\"\n",
    "# distance_metric_col_name = \"hubert-raw==Distance(Cosine)\"\n",
    "distance_metric_col_name = \"hubert-soft==Distance(Cosine)\"\n",
    "# distance_metric_col_name = \"hubert-discrete-code==Distance(Levenshtein)\"\n",
    "# distance_metric_col_name = \"hubert-discrete-centroid==Distance(Cosine)\"\n",
    "\n",
    "use_SL_Penalty = False\n",
    "if use_SL_Penalty:\n",
    "    distance_metric_col_name += \"==w_SL_Penalty\"\n",
    "    \n",
    "print(f\"Using {distance_metric_col_name} to rank spellings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "56c79b30-f2fd-45c2-b78c-24b04b95bdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_best_df(df, distance_metric_col_name, n_spellings_per_token_id=1, show_smallest=True):\n",
    "    # Display N-best ranked respellings per original input audio (excluding original spellings!)\n",
    "    if show_smallest:\n",
    "        n_best_lambda_fn = lambda x: x.nsmallest(n_spellings_per_token_id, distance_metric_col_name)\n",
    "    else:\n",
    "        n_best_lambda_fn = lambda x: x.nlargest(n_spellings_per_token_id, distance_metric_col_name)\n",
    "\n",
    "    original_spellings_mask = df['wordtype'] == df['respelling']\n",
    "    original_spellings_df = df[original_spellings_mask]\n",
    "    inverted_mask = ~original_spellings_mask\n",
    "    no_orig_spellings_df = df[inverted_mask]\n",
    "    n_best_df = no_orig_spellings_df.groupby('wordtype', group_keys=False).apply(n_best_lambda_fn)\n",
    "    return n_best_df, original_spellings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "148aa592-e097-474a-9c5b-38ea2019edb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add back original spellings and sort by distance and also by gap between best spelling and original spelling\n",
    "n_best_df, original_spellings_df = get_n_best_df(df, distance_metric_col_name, n_spellings_per_token_id=1)\n",
    "n_best_df = pd.concat([n_best_df, original_spellings_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0bf20e01-8d3b-41f6-baac-ad8c5aebc8a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Produce a single dataframe where rows of the same wordtype are grouped\n",
    "# within a group they are ordered by the distance metric\n",
    "# and the order of the groups is determined by the gap between the distance value of the best spelling and the original spelling\n",
    "\n",
    "# add new col for distace_gap\n",
    "distance_gap_col_name = f\"{distance_metric_col_name}==distance_gap\"\n",
    "n_best_df.insert(1 + n_best_df.columns.get_loc(distance_metric_col_name), distance_gap_col_name, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2f698dd-2ef0-46d7-8d94-77ca8b2d7e2f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING invalid wordtype alexander\n",
      "WARNING invalid wordtype spiritual\n"
     ]
    }
   ],
   "source": [
    "wordtype2df = {}\n",
    "wordtype2maximum_distance_gap = {}\n",
    "\n",
    "# get each sub_df corresponding to wordtype\n",
    "wordtypes = n_best_df['wordtype'].unique()\n",
    "for wordtype in wordtypes:\n",
    "    wordtype_df = n_best_df[n_best_df['wordtype'] == wordtype]\n",
    "    \n",
    "    # for each spelling calculate its distance gap from the original wordtype (calc'd against same GT token_id!)\n",
    "    max_distance_gap = -math.inf\n",
    "    for i in wordtype_df[wordtype_df['wordtype'] == wordtype].index:\n",
    "        token_id = wordtype_df.loc[i, 'token_id']\n",
    "\n",
    "        # this should be the correct code! but we haven't guaranteed that we have calculated distance from original spelling to each respective token_id yet!!!\n",
    "        # original_spelling_distance = wordtype_df[(wordtype_df['wordtype'] == wordtype) & (wordtype_df['is_orig'] == True) & (wordtype_df['token_id'] == token_id)][distance_metric_col_name].item()\n",
    "\n",
    "        # temporary code, not ideal!\n",
    "        # print(wordtype_df[(wordtype_df['wordtype'] == wordtype) & (wordtype_df['is_orig'] == True)][distance_metric_col_name])\n",
    "        \n",
    "        # TODO fix code so this isn't needed\n",
    "        if len(wordtype_df[(wordtype_df['wordtype'] == wordtype) & (wordtype_df['is_orig'] == True)].head(1)[distance_metric_col_name]) != 1:\n",
    "            original_spelling_distance = -math.inf\n",
    "            print('WARNING invalid wordtype', wordtype)\n",
    "        else:\n",
    "            original_spelling_distance = wordtype_df[(wordtype_df['wordtype'] == wordtype) & (wordtype_df['is_orig'] == True)].head(1)[distance_metric_col_name].item()\n",
    "\n",
    "        distance_gap = original_spelling_distance - wordtype_df.loc[i, distance_metric_col_name]\n",
    "        max_distance_gap = max(distance_gap, max_distance_gap)\n",
    "        wordtype_df.loc[i, distance_gap_col_name] = distance_gap\n",
    "\n",
    "    # store df into dictionary temporarily\n",
    "    wordtype2df[wordtype] = wordtype_df\n",
    "\n",
    "    # track max distance gap for each wordtype\n",
    "    wordtype2maximum_distance_gap[wordtype] = max_distance_gap\n",
    "    \n",
    "# order wordtypes by distance\n",
    "# sorted(wordtypes, key=lambda x: wordtype2maximum_distance_gap[x])\n",
    "sorted_wordtype2maximum_distance_gap = dict(sorted(wordtype2maximum_distance_gap.items(), key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf788e94-dadc-486a-ba4f-46ddfd22367c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recombine dfs into big df\n",
    "df_list = [wordtype2df[wordtype] for wordtype in sorted_wordtype2maximum_distance_gap.keys()]\n",
    "combined_df = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a639b937-630c-40e7-8cab-58aa184be074",
   "metadata": {},
   "source": [
    "# print n-best spellings for each wordtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "577f4294-3359-499d-b21a-3e76b8e1a222",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# current_wordtype = \"\"\n",
    "# for token_id, wordtype, respelling, distance, orig_wav, is_orig in zip(\n",
    "#     combined_df['token_id'], \n",
    "#     combined_df['wordtype'], \n",
    "#     combined_df['respelling'], \n",
    "#     combined_df[distance_metric_col_name], \n",
    "#     combined_df['orig_wav'],\n",
    "#     combined_df['is_orig']\n",
    "# ):\n",
    "#     # new wordtype, print GT token_ids and display audios\n",
    "#     if current_wordtype != wordtype:\n",
    "#         current_wordtype = wordtype\n",
    "        \n",
    "#         # get all token_ids and orig_wavs for this wordtype\n",
    "#         wordtype_df = combined_df[combined_df['wordtype'] == wordtype]\n",
    "#         first_row_for_each_token_id = wordtype_df.groupby('token_id').nth(0)\n",
    "        \n",
    "#         str_symb_len = 75\n",
    "        \n",
    "#         print(str_symb_len * \"=\")\n",
    "#         print(f\" Original word aligned WAV(s) \".center(str_symb_len, '='))\n",
    "#         print(str_symb_len * \"=\")\n",
    "        \n",
    "#         for index, row in first_row_for_each_token_id.iterrows():\n",
    "#             token_id = index # index is now equal to token_id as we used groupby above\n",
    "#             print(f\"[GT - {token_id}]:\")\n",
    "#             display(Audio(row['orig_wav'], rate=GT_SAMPLE_RATE))\n",
    "#         print(str_symb_len * \"-\")\n",
    "#         print(f\" Respellings ({wordtype2numspellings[wordtype]} in total) \".center(str_symb_len, '-'))\n",
    "#         print(f\" Best distance gap: {wordtype2maximum_distance_gap[wordtype]:.3f} \".center(str_symb_len, '-'))\n",
    "#         print(str_symb_len * \"-\")\n",
    "    \n",
    "#     # create string to print for this respelling\n",
    "#     print_str = \"\"\n",
    "#     if is_orig:\n",
    "#         print_str += f\"{' '+wordtype+' ':*^35}\"\n",
    "#     else:\n",
    "#         print_str += f\" ---> {respelling=}\"\n",
    "#     distance_str = f\" {distance:.2f} [{distance_metric_col_name}] [calc'ed vs GT: {token_id}]\"\n",
    "#     print_str += distance_str\n",
    "#     print(print_str)\n",
    "    \n",
    "#     display(Audio(spelling2vocoded_audio[respelling], rate=HIFIGAN_SAMPLE_RATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "6fc05151-e7a9-4959-9c65-4a0aa8423454",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # CUSTOM FOR ONLY PRINTING GT AUDIO!!!!!!!!!!!!!!\n",
    "\n",
    "# current_wordtype = \"\"\n",
    "# for token_id, wordtype, respelling, distance, orig_wav, is_orig in zip(\n",
    "#     combined_df['token_id'], \n",
    "#     combined_df['wordtype'], \n",
    "#     combined_df['respelling'], \n",
    "#     combined_df[distance_metric_col_name], \n",
    "#     combined_df['orig_wav'],\n",
    "#     combined_df['is_orig']\n",
    "# ):\n",
    "#     # new wordtype, print GT token_ids and display audios\n",
    "#     if current_wordtype != wordtype:\n",
    "#         current_wordtype = wordtype\n",
    "        \n",
    "#         # get all token_ids and orig_wavs for this wordtype\n",
    "#         wordtype_df = combined_df[combined_df['wordtype'] == wordtype]\n",
    "#         first_row_for_each_token_id = wordtype_df.groupby('token_id').nth(0)\n",
    "        \n",
    "#         for index, row in first_row_for_each_token_id.iterrows():\n",
    "#             token_id = index # index is now equal to token_id as we used groupby above\n",
    "#             print(f\"[GT - {token_id}]:\")\n",
    "#             display(Audio(row['orig_wav'], rate=GT_SAMPLE_RATE))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5927e41e-2a59-44e4-8a4b-54afd2d011aa",
   "metadata": {},
   "source": [
    "# define list of dev set and test set wordtypes in reproducible manner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "867e5bc3-f99e-42ee-be7a-784f0cb0cd40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# filter so we only keep enough for dev and test sets\n",
    "dev_n = 100\n",
    "test_n = 100\n",
    "assert dev_n + test_n < len(df['wordtype'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe8901e4-59c3-42a7-830b-ce817b1e7fb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wordtype2per = {}\n",
    "for w in df['wordtype'].unique():\n",
    "    per = df[df['wordtype'] == w]['per'].head(1).values[0]\n",
    "    wordtype2per[w] = per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f621be10-fb7f-49b8-a0a9-b470c6bcd64c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sorted_wordtype2per = sorted(wordtype2per.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0ac3d297-b37c-4a2b-9afc-2918f506f8a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# odd is test set, even is dev set\n",
    "dev_set_wordtypes = set()\n",
    "test_set_wordtypes = set()\n",
    "for i, (w, _per) in enumerate(sorted_wordtype2per[:dev_n+test_n], 1):\n",
    "    if i % 2 == 0:\n",
    "        dev_set_wordtypes.add(w)\n",
    "    elif i % 2 == 1:\n",
    "        test_set_wordtypes.add(w)\n",
    "    else:\n",
    "        raise ValueError\n",
    "    \n",
    "assert len(dev_set_wordtypes) == dev_n, f\"{len(dev_set_wordtypes)} != {dev_n}\" \n",
    "assert len(test_set_wordtypes) == test_n, f\"{len(test_set_wordtypes)} != {test_n}\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "572395e1-8dd5-401f-8451-c536fec3e5be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'allegiance, alleging, annulled, attribute, auditorium, battalion, bequeathed, boulogne, brochure, brougham, bulwarks, burchell, cadillac, calamity, caravans, cemetery, charioteers, colleagues, collegians, colonial, competitive, conducive, coughing, courtesy, cribbage, czolgosz, decorated, deterred, effluvia, epithets, european, executing, executor, exertions, exhortations, exhorted, exhorting, exonerated, experimental, fastidious, flintshire, forenoon, forgetfulness, forgiveness, frenzied, giovanni, gloucester, gloucestershire, gonzalez, hammurabi, harassed, hereford, horizontal, individuality, invoices, juvenile, juveniles, macintosh, maryland, matrices, mediterranean, mesopotamia, minarets, monitored, monsieur, narcotics, nonpartisan, nourished, oscillating, ostentatious, paradise, parasitic, parasitically, penknife, persevered, phosphoric, portuguese, premising, prerogative, presided, pugnacious, punitive, respiratory, reverend, ridicule, seleucia, sepulchre, shredded, shropshire, soubriquet, specimen, speeches, spiritual, spirituous, staffordshire, tocqueville, turreted, unclothed, usurious, venetian'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\", \".join(sorted(dev_set_wordtypes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1c001d16-2081-487c-9098-584d4a428ccf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'adherence, adhering, advertised, alexander, alexanders, aligning, analyzed, aspirants, augmented, barouche, beaumont, boisterous, bonaventura, borsippa, capillary, champagne, clangour, collared, colonization, compromised, confided, dastardly, decorous, delicate, deutsche, discourage, ehrenberg, endorsed, engendered, entourage, erasures, erroneous, escalade, escritoire, ethelred, etiquette, eventual, executors, exertion, exhaustion, exhaustive, exhortation, exorbitant, fatigues, grimaldi, guiseppe, habitual, heiresses, heterogeneous, immature, insecure, intellectual, interred, inveigled, itinerary, jealousy, karswell, khrushchev, laughter, launched, licensed, maecenas, marseilles, merionethshire, minorities, minority, monastery, morality, mortgages, napoleon, nauseous, nicholas, obscenity, outshone, patriotic, pecuniary, peripheral, phosphorus, presides, prestige, priority, protests, psychiatrist, reservoir, respited, righteous, rothschilds, rouleaux, saccharine, sassanian, sepulchres, silhouette, solemnly, southern, subsidiary, theodosius, therapist, thinness, unearthed, vestiges'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\", \".join(sorted(test_set_wordtypes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90099254-00f4-45b3-9856-9b73bab7e2a1",
   "metadata": {},
   "source": [
    "# save files to disk to create listening test for experiment 1\n",
    "\n",
    "- save wavs for each ranking metric to its own folder \n",
    "- save GT wavs\n",
    "- save url lists (used by create_ab_url_lists_for_qualtreats.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "622aebaa-d488-4f21-9d71-67c7b8fefcee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 100/100 [08:01<00:00,  4.82s/it]\n"
     ]
    }
   ],
   "source": [
    "distances_to_save = [\n",
    "    \"mfcc==Distance(Euclidean)\",\n",
    "    \"hubert-raw==Distance(Cosine)\",\n",
    "    \"hubert-soft==Distance(Cosine)\",\n",
    "    \"hubert-discrete-code==Distance(Levenshtein)\",\n",
    "    \"hubert-discrete-centroid==Distance(Cosine)\",\n",
    "]\n",
    "\n",
    "# for creating url lists that are used to create qualtreats tests\n",
    "conditionname2wavlist = defaultdict(list)\n",
    "\n",
    "# save dev set spelling wavs to disk\n",
    "for i, w in enumerate(tqdm(sorted(dev_set_wordtypes)), 1):\n",
    "    for distance_metric_col_name in distances_to_save:\n",
    "        # retrieve best ranked spelling for wordtype\n",
    "        n_best_df, original_spellings_df = get_n_best_df(df, distance_metric_col_name, n_spellings_per_token_id=1)\n",
    "        best_ranked_spelling = n_best_df[n_best_df['wordtype'] == w]['respelling'].values[0]\n",
    "        wav = spelling2vocoded_audio[best_ranked_spelling]\n",
    "\n",
    "        # create outdir\n",
    "        outdir = os.path.join(\"listening_tests\", 'experiment1', distance_metric_col_name)\n",
    "        os.makedirs(outdir, exist_ok=True)\n",
    "        wavfilename = f\"{i}_{w}_{best_ranked_spelling}.wav\"\n",
    "        outpath = os.path.join(outdir, wavfilename)\n",
    "\n",
    "        # save in outdir\n",
    "        write(outpath, HIFIGAN_SAMPLE_RATE, wav)\n",
    "        \n",
    "        conditionname2wavlist[distance_metric_col_name].append(wavfilename)\n",
    "        \n",
    "    ## save GT audio for wordtypes to disk\n",
    "    # get path to GT wav from df \n",
    "    orig_wav_paths = df[df['wordtype'] == w]['orig_wav'].unique()\n",
    "    assert len(orig_wav_paths) == 1 # should be the case since we only have one token_id per wordtype\n",
    "    orig_wav_path = orig_wav_paths[0]\n",
    "    # use shell/bash to copy file to destination folder\n",
    "    outdir = os.path.join(\"listening_tests\", 'experiment1', 'groundtruth')\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    wavfilename = f\"{i}_{os.path.basename(orig_wav_path)}\"\n",
    "    outpath = os.path.join(outdir, wavfilename)\n",
    "    shutil.copy(orig_wav_path, outpath)\n",
    "    \n",
    "    conditionname2wavlist['groundtruth'].append(wavfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "2c773dfe-c37b-4d26-be11-0ac1650c1e7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save url_lists to disk, ready for qualtreats\n",
    "\n",
    "# example URL https://jonojace.github.io/IS22-speech-audio-corrector-samples/SAC-grapheme-input/1-how is afghan pronounced.wav\n",
    "url_folder = \"https://jonojace.github.io/SSW23-asr-speller-samples\"\n",
    "url_lists_outdir = os.path.join(\"listening_tests\", 'experiment1', 'url_lists')\n",
    "os.makedirs(url_lists_outdir, exist_ok=True)\n",
    "\n",
    "for condition_name, wavlist in conditionname2wavlist.items():\n",
    "    urls = []\n",
    "    for wavfilename in wavlist:\n",
    "        full_url = f\"{url_folder}/{condition_name}/{wavfilename}\"\n",
    "        urls.append(full_url)\n",
    "    \n",
    "    # write url list to disk\n",
    "    with open(os.path.join(url_lists_outdir, f\"{condition_name}.txt\"), 'w') as f:\n",
    "        f.write(\"\\n\".join(urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa16956-8dfb-4648-8b7f-b068d0bdbe25",
   "metadata": {},
   "source": [
    "## run the following command in bash from your laptop to transfer files to your local mac\n",
    "\n",
    "```bash\n",
    "SOURCE=s1785140@escience6.inf.ed.ac.uk:/home/s1785140/rlspeller/listening_tests/\n",
    "DEST=/Users/jonojace/asr_speller/listening_tests/\n",
    "mkdir -p $DEST\n",
    "rsync -avu $SOURCE $DEST\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded2c6ab-6657-44d7-9be4-2685f58ac04a",
   "metadata": {},
   "source": [
    "# save files to disk to create listening test for experiment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5c3285-5ce5-440b-9ba6-c7eaba369219",
   "metadata": {},
   "source": [
    "- GT audio\n",
    "- original_spellings\n",
    "- asr_1000_best_acoustic_autorank_top1\n",
    "- asr_1000_best_acoustic_autorank_top5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "16f1c415-c80a-44b7-a578-abe5ab991c61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### create all outdirs\n",
    "exp2_conditions = [\n",
    "    \"asr_1000_best_acoustic_autorank_top1\",\n",
    "    \"asr_1000_best_acoustic_autorank_top5\",\n",
    "]\n",
    "\n",
    "condition2outdir = {\n",
    "    condition: os.path.join(\"listening_tests\", 'experiment2', EXP2_DISTANCE_NAME, condition)\n",
    "    for condition in exp2_conditions\n",
    "}\n",
    "\n",
    "condition2outdir[\"original_spellings\"] = os.path.join(\"listening_tests\", 'experiment2', 'original_spellings')\n",
    "condition2outdir[\"groundtruth\"] = os.path.join(\"listening_tests\", 'experiment2', 'groundtruth')\n",
    "condition2outdir[\"asr_1_best\"] = os.path.join(\"listening_tests\", \"experiment2\", \"asr_1_best\")\n",
    "condition2outdir[\"asr_5_best_acoustic\"] = os.path.join(\"listening_tests\", \"experiment2\", \"asr_5_best_acoustic\")\n",
    "condition2outdir[\"asr_5_best_no_acoustic\"] = os.path.join(\"listening_tests\", \"experiment2\", \"asr_5_best_no_acoustic\")\n",
    "\n",
    "for condition, outdir in condition2outdir.items():\n",
    "    os.makedirs(outdir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dbe97847-dd7e-4a91-950b-205de03f5b10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 100/100 [02:11<00:00,  1.31s/it]\n"
     ]
    }
   ],
   "source": [
    "NUM_SPELLINGS_PER_WORDTYPE = 5\n",
    "EXP2_DISTANCE_NAME = \"hubert-raw==Distance(Cosine)\"\n",
    "\n",
    "for i, w in enumerate(tqdm(sorted(test_set_wordtypes)), 1):\n",
    "    n_best_df, original_spellings_df = get_n_best_df(df, EXP2_DISTANCE_NAME, n_spellings_per_token_id=NUM_SPELLINGS_PER_WORDTYPE)\n",
    "    best_ranked_spellings = n_best_df[n_best_df['wordtype'] == w]['respelling'].values\n",
    "    assert len(best_ranked_spellings) == NUM_SPELLINGS_PER_WORDTYPE\n",
    "    best_ranked_spelling = best_ranked_spellings[0] # since these are ranked in order of the distance!\n",
    "\n",
    "    ### save wavs for asr_1000_best_acoustic_autorank_top1 and top5\n",
    "    # top 1\n",
    "    wavfilename = f\"{i}_{w}_{best_ranked_spelling}.wav\"\n",
    "    outpath = os.path.join(condition2outdir['asr_1000_best_acoustic_autorank_top1'], wavfilename)\n",
    "    write(outpath, HIFIGAN_SAMPLE_RATE, spelling2vocoded_audio[best_ranked_spelling])\n",
    "    # top 5\n",
    "    for spelling in best_ranked_spellings:\n",
    "        wavfilename = f\"{i}_{w}_{spelling}.wav\"\n",
    "        outpath = os.path.join(condition2outdir['asr_1000_best_acoustic_autorank_top5'], wavfilename)\n",
    "        write(outpath, HIFIGAN_SAMPLE_RATE, spelling2vocoded_audio[spelling])\n",
    "\n",
    "    ### save wav original spelling\n",
    "    wavfilename = f\"{i}_{w}.wav\"\n",
    "    outpath = os.path.join(condition2outdir['original_spellings'], wavfilename)\n",
    "    write(outpath, HIFIGAN_SAMPLE_RATE, spelling2vocoded_audio[w])\n",
    "    \n",
    "    ### save wav for GT\n",
    "    orig_wav_paths = df[df['wordtype'] == w]['orig_wav'].unique()\n",
    "    assert len(orig_wav_paths) == 1 # should be the case since we only have one token_id per wordtype\n",
    "    orig_wav_path = orig_wav_paths[0]\n",
    "    # use shell/bash to copy file to destination folder\n",
    "    wavfilename = f\"{i}_{os.path.basename(orig_wav_path)}\"\n",
    "    outpath = os.path.join(condition2outdir[\"groundtruth\"], wavfilename)\n",
    "    shutil.copy(orig_wav_path, outpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69405c84-0af7-43c6-8c71-abf7030105c8",
   "metadata": {},
   "source": [
    "Audios for ASR nbest:\n",
    "- asr_1_best\n",
    "- asr_5_best_acoustic\n",
    "- asr_5_best_no_acoustic\n",
    "\n",
    "Below approach works since we can recover nbest rank for each respelling from its index in pandas df, since df \n",
    "conserves the order from which we added items into it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f4369a07-34d0-42b0-ae4c-b081988413ff",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 246/246 [00:47<00:00,  5.18it/s]\n"
     ]
    },
    {
     "ename": "IntCastingNaNError",
     "evalue": "Cannot convert non-finite values (NA or inf) to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIntCastingNaNError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, df_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwordtype\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m w]\u001b[38;5;241m.\u001b[39mindex[:LEN_OF_NBEST_LIST], \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      6\u001b[0m         df\u001b[38;5;241m.\u001b[39mloc[df_idx, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masr_nbest_rank\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m i\n\u001b[0;32m----> 7\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124masr_nbest_rank\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43masr_nbest_rank\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/speller/lib/python3.8/site-packages/pandas/core/generic.py:6240\u001b[0m, in \u001b[0;36mNDFrame.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m   6233\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   6234\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miloc[:, i]\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[1;32m   6235\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns))\n\u001b[1;32m   6236\u001b[0m     ]\n\u001b[1;32m   6238\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6239\u001b[0m     \u001b[38;5;66;03m# else, only a single dtype is given\u001b[39;00m\n\u001b[0;32m-> 6240\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor(new_data)\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6243\u001b[0m \u001b[38;5;66;03m# GH 33113: handle empty frame or series\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/speller/lib/python3.8/site-packages/pandas/core/internals/managers.py:448\u001b[0m, in \u001b[0;36mBaseBlockManager.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mastype\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, dtype, copy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, errors: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m--> 448\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mastype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/speller/lib/python3.8/site-packages/pandas/core/internals/managers.py:352\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[0;34m(self, f, align_keys, ignore_failures, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 352\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mNotImplementedError\u001b[39;00m):\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ignore_failures:\n",
      "File \u001b[0;32m~/miniconda3/envs/speller/lib/python3.8/site-packages/pandas/core/internals/blocks.py:526\u001b[0m, in \u001b[0;36mBlock.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;124;03mCoerce to the new dtype.\u001b[39;00m\n\u001b[1;32m    510\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;124;03mBlock\u001b[39;00m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    524\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m--> 526\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43mastype_array_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    528\u001b[0m new_values \u001b[38;5;241m=\u001b[39m maybe_coerce_values(new_values)\n\u001b[1;32m    529\u001b[0m newb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_block(new_values)\n",
      "File \u001b[0;32m~/miniconda3/envs/speller/lib/python3.8/site-packages/pandas/core/dtypes/astype.py:299\u001b[0m, in \u001b[0;36mastype_array_safe\u001b[0;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m values\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 299\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m \u001b[43mastype_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;66;03m# e.g. astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;66;03m#  trying to convert to float\u001b[39;00m\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/speller/lib/python3.8/site-packages/pandas/core/dtypes/astype.py:230\u001b[0m, in \u001b[0;36mastype_array\u001b[0;34m(values, dtype, copy)\u001b[0m\n\u001b[1;32m    227\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 230\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mastype_nansafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;66;03m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, np\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/speller/lib/python3.8/site-packages/pandas/core/dtypes/astype.py:140\u001b[0m, in \u001b[0;36mastype_nansafe\u001b[0;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot astype a timedelta from [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marr\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] to [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(arr\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39mfloating) \u001b[38;5;129;01mand\u001b[39;00m is_integer_dtype(dtype):\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_astype_float_to_int_nansafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_object_dtype(arr\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[1;32m    143\u001b[0m \n\u001b[1;32m    144\u001b[0m     \u001b[38;5;66;03m# if we have a datetime/timedelta array of objects\u001b[39;00m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;66;03m# then coerce to a proper dtype and recall astype_nansafe\u001b[39;00m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_datetime64_dtype(dtype):\n",
      "File \u001b[0;32m~/miniconda3/envs/speller/lib/python3.8/site-packages/pandas/core/dtypes/astype.py:182\u001b[0m, in \u001b[0;36m_astype_float_to_int_nansafe\u001b[0;34m(values, dtype, copy)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;124;03mastype with a check preventing converting NaN to an meaningless integer value.\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misfinite(values)\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m--> 182\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m IntCastingNaNError(\n\u001b[1;32m    183\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot convert non-finite values (NA or inf) to integer\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    184\u001b[0m     )\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;66;03m# GH#45151\u001b[39;00m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (values \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mall():\n",
      "\u001b[0;31mIntCastingNaNError\u001b[0m: Cannot convert non-finite values (NA or inf) to integer"
     ]
    }
   ],
   "source": [
    "LEN_OF_NBEST_LIST = 1000\n",
    "\n",
    "# for each word in dataframe, add a new column that indicates its ASR nbest ranking\n",
    "for w in tqdm(df['wordtype'].unique()):\n",
    "    for i, df_idx in enumerate(df[df['wordtype'] == w].index[:LEN_OF_NBEST_LIST], 1):\n",
    "        df.loc[df_idx, \"asr_nbest_rank\"] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "196dd478-0460-4e3b-a0c9-ace0d14ef99f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_wav(outpath, wav_data):\n",
    "    write(outpath, HIFIGAN_SAMPLE_RATE, wav_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0a9fd626-b314-4cd7-93af-9d70dd583f87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 100/100 [00:49<00:00,  2.03it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, w in enumerate(tqdm(sorted(test_set_wordtypes)), 1):\n",
    "    # get asr nbest from dataframe\n",
    "    n_best_df = df[df['wordtype'] == w][:NUM_SPELLINGS_PER_WORDTYPE]\n",
    "    assert n_best_df[\"asr_nbest_rank\"].max() == 5.0\n",
    "    assert n_best_df[\"asr_nbest_rank\"].min() == 1.0\n",
    "\n",
    "    # save wavs to disk\n",
    "    # for spelling in \n",
    "    for df_idx in n_best_df.index:\n",
    "        spelling = df.loc[df_idx, \"respelling\"]\n",
    "        wavfilename = f\"{i}_{w}_{spelling}.wav\"\n",
    "\n",
    "        outpath = os.path.join(condition2outdir['asr_5_best_acoustic'], wavfilename)\n",
    "        save_wav(outpath, spelling2vocoded_audio[spelling])\n",
    "        outpath = os.path.join(condition2outdir['asr_5_best_no_acoustic'], wavfilename)\n",
    "        save_wav(outpath, spelling2vocoded_audio[spelling])\n",
    "\n",
    "        if df.loc[df_idx, \"asr_nbest_rank\"] == 1.0:\n",
    "            outpath = os.path.join(condition2outdir['asr_1_best'], wavfilename)\n",
    "            save_wav(outpath, spelling2vocoded_audio[spelling])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9476ab8-77d7-4453-ba23-e3c6ca954baf",
   "metadata": {},
   "source": [
    "## copy wavs back to laptop for filtering of wavs (for asr_5_best_acoustic and asr_1000_best_acoustic_autorank_top5)\n",
    "\n",
    "```bash\n",
    "SOURCE=s1785140@escience6.inf.ed.ac.uk:/home/s1785140/rlspeller/listening_tests/\n",
    "DEST=/Users/jonojace/asr_speller/listening_tests/\n",
    "mkdir -p $DEST\n",
    "rsync -avu $SOURCE $DEST\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341f9f69-b6ac-4405-aba2-818dd3f523d2",
   "metadata": {},
   "source": [
    "## create url lists from folders of wav files\n",
    "\n",
    "before this step, ensure that you choose the best one for top 5 systems!!! (delete the ones that are not as good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "952e2f47-d2ff-491e-9dca-3b119481767c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|█▌                                                   | 3/100 [00:03<01:55,  1.19s/it]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "# save url_lists to disk, ready for qualtreats\n",
    "# example URL https://jonojace.github.io/IS22-speech-audio-corrector-samples/SAC-grapheme-input/1-how is afghan pronounced.wav\n",
    "url_folder = \"https://jonojace.github.io/SSW23-asr-speller-samples\"\n",
    "url_lists_outdir = os.path.join(\"listening_tests\", 'experiment1', 'url_lists')\n",
    "os.makedirs(url_lists_outdir, exist_ok=True)\n",
    "\n",
    "for condition_name, wavlist in conditionname2wavlist.items():\n",
    "    urls = []\n",
    "    for wavfilename in wavlist:\n",
    "        full_url = f\"{url_folder}/{condition_name}/{wavfilename}\"\n",
    "        urls.append(full_url)\n",
    "    \n",
    "    # write url list to disk\n",
    "    with open(os.path.join(url_lists_outdir, f\"{condition_name}.txt\"), 'w') as f:\n",
    "        f.write(\"\\n\".join(urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51505040-9753-4347-9d36-20cd3c505555",
   "metadata": {},
   "source": [
    "## run the following command in bash from your laptop to transfer files to your local mac\n",
    "\n",
    "```bash\n",
    "SOURCE=s1785140@escience6.inf.ed.ac.uk:/home/s1785140/rlspeller/listening_tests/\n",
    "DEST=/Users/jonojace/asr_speller/listening_tests/\n",
    "mkdir -p $DEST\n",
    "rsync -avu $SOURCE $DEST\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
