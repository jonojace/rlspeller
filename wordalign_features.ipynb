{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11f1f25c",
   "metadata": {},
   "source": [
    "# instructions\n",
    "\n",
    "use fastpitch conda env\n",
    "```bash\n",
    "conda activate fastpitch\n",
    "cd rlspeller/\n",
    "./jupyter_lab.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0453510-71c2-4a70-960a-970940bfc5d5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "docstring = \"\"\"\n",
    "(Use fastpitch conda env)\n",
    "\n",
    "Helper script that takes a folder of speech reps (wav2vec2, mel-spec, etc.)\n",
    "and aligns them at word-level using MFA alignments.\n",
    "\n",
    "Speech reps corresponding to word tokens in the corpus are then saved individually to an output folder\n",
    "with the following structure:\n",
    "- data_path\n",
    "    - word1\n",
    "        - word1_LJ010-0292_001.pt\n",
    "        - word1_LJ010-0292_002.pt\n",
    "        - ...\n",
    "    - word2\n",
    "        - word2_LJ001-0012_001.pt\n",
    "        - word2_LJ002-0024_001.pt\n",
    "        - ...\n",
    "    - ...\n",
    "\n",
    "- word1, word2, ... subfolders refer to a particular wordtype in the corpus.\n",
    "- .pt files contain speech representations that map to a particular example of a wordtype.\n",
    "  It is named as:\n",
    "    <wordtype>_<utt id>_<numbered occurrence in the utterance>.pt\n",
    "\n",
    "Example usage:\n",
    "    #hubert w/ padding offset\n",
    "    cd ~/fairseq\n",
    "    python examples/lexicon_learner/wordalign_speechreps.py \\\n",
    "        -t hubert \\\n",
    "        --padding_idx_offset 1 \\\n",
    "        -s /home/s1785140/fairseq/examples/lexicon_learner/lj_speech_quantized.txt \\\n",
    "        -a /home/s1785140/data/ljspeech_MFA_alignments \\\n",
    "        -o /home/s1785140/data/ljspeech_hubert_reps/hubert-base/layer-6/word_level_with_padding_idx_offset\n",
    "\n",
    "    #hubert w/o padding offset\n",
    "    cd ~/fairseq\n",
    "    python examples/lexicon_learner/wordalign_speechreps.py \\\n",
    "        -t hubert \\\n",
    "        --padding_idx_offset 0 \\\n",
    "        -s /home/s1785140/fairseq/examples/lexicon_learner/lj_speech_quantized.txt \\\n",
    "        -a /home/s1785140/data/ljspeech_MFA_alignments \\\n",
    "        -o /home/s1785140/data/ljspeech_hubert_reps/hubert-base/layer-6/word_level_without_padding_idx_offset\n",
    "\n",
    "    #wav2vec2\n",
    "    cd ~/fairseq\n",
    "    python examples/lexicon_learner/wordalign_speechreps.py \\\n",
    "        -t wav2vec2 \\\n",
    "        -s /home/s1785140/data/ljspeech_wav2vec2_reps/wav2vec2-large-960h/layer-15/utt_level \\\n",
    "        -a /home/s1785140/data/ljspeech_MFA_alignments \\\n",
    "        -o /home/s1785140/data/ljspeech_wav2vec2_reps/wav2vec2-large-960h/layer-15/word_level\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95da2491",
   "metadata": {},
   "source": [
    "Command line args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ed31824-8d7f-4e8c-a96b-937915cfc08f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# imitate CLAs\n",
    "import sys\n",
    "sys.argv = [\n",
    "    # fastpitch features\n",
    "    # 'train.py',\n",
    "    # '--type', 'mel',\n",
    "    # '--utt_id_list', '/home/s1785140/data/ljspeech_fastpitch/respeller_uttids.txt', \n",
    "    # '--input_directory', '/home/s1785140/data/ljspeech_fastpitch/mels',\n",
    "    # '--alignments', '/home/s1785140/data/ljspeech_fastpitch/aligns', # older alignments (without using improved tokenizer)\n",
    "    # '--output_directory', '/home/s1785140/data/ljspeech_fastpitch/wordaligned_mels',\n",
    "\n",
    "    # speechbrain features\n",
    "    'train.py',\n",
    "    '--type', 'mel',\n",
    "    '--utt_id_list', '/home/s1785140/data/ljspeech_fastpitch/respeller_uttids.txt', \n",
    "    '--input_directory', '/home/s1785140/speechbrain/templates/speech_recognition_CharTokens_NoLM/data/ljspeech_dumped_feats',\n",
    "    '--alignments', '/home/s1785140/speechbrain/templates/speech_recognition_CharTokens_NoLM/data/LJSpeech-1.1/MFA_alignments_lowercase_nopunc', # newer alignments, lowercase no punctuation\n",
    "    '--output_directory', '/home/s1785140/speechbrain/templates/speech_recognition_CharTokens_NoLM/data/ljspeech_dumped_feats_word_aligned',\n",
    "    # '--mel-to-graphemes-ratio-lowest-threshold', '5.5',\n",
    "    # '--mel-to-graphemes-ratio-highest-threshold', '13.5',\n",
    "    '--clean-output-folder',\n",
    "    # '--max-utts-to-generate', '10', # for testing!!!\n",
    "    \n",
    "    # FOR TESTING\n",
    "    # '--input_directory', '/home/s1785140/data/ljspeech_fastpitch/mels_test',\n",
    "    # '--alignments', '/home/s1785140/data/ljspeech_fastpitch/aligns_test', \n",
    "    # '--output_directory', '/home/s1785140/data/ljspeech_fastpitch/wordaligned_mels_test',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a486af3",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# install package from notebook\n",
    "import sys\n",
    "# !{sys.executable} -m pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1186da",
   "metadata": {},
   "source": [
    "# imports and globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3f0393f-64f2-4056-b219-a3e4f557516d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import tgt\n",
    "import string\n",
    "import librosa\n",
    "import glob\n",
    "import random\n",
    "from IPython.display import Audio\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import shutil\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "SKIP_NON_ASCII = False\n",
    "WORDS_TO_SKIP = [\"wdsu-tv\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1439b58",
   "metadata": {},
   "source": [
    "# Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ff2a0ad-4c05-476f-a430-ddd4e8e92e6e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-t', '--type', type=str, default='hubert',\n",
    "                    help='type of input speech reps that we are using, i.e. hubert wav2vec2 etc.')\n",
    "parser.add_argument('--padding_idx_offset', type=int, default=0,\n",
    "                    help='add 1 to token id of discrete reps in order to allow for padding_idx==0')\n",
    "parser.add_argument('--utt_id_list', type=str, required=False, default=\"\",\n",
    "                    help='path to text file that contains list of utterance ids that we extract from')\n",
    "parser.add_argument('-s', '--input_directory', type=str, required=True,\n",
    "                    help='path to single non-nested folder containing speech representations (.pt files) or txt file (hubert)')\n",
    "parser.add_argument('-a', '--alignments', type=str, required=True,\n",
    "                    help='path to single non-nested folder containing MFA alignments (.TextGrid files)')\n",
    "parser.add_argument('-o', '--output_directory', type=str, required=True,\n",
    "                    help='where to write word-level data')\n",
    "parser.add_argument('--max-utts-to-generate', type=int, default=None,\n",
    "                    help='How many utts to extract word aligned speech reps for. If None, extract all utts.')\n",
    "parser.add_argument('--mel-to-graphemes-ratio-lowest-threshold', type=float, default=0.0,\n",
    "                    help='Lowest mel-to-graphemes ratio to consider. (lower ratio means fewer mel frames per grapheme)')\n",
    "parser.add_argument('--mel-to-graphemes-ratio-highest-threshold', type=float, default=math.inf,\n",
    "                    help='Lowest mel-to-graphemes ratio to consider. (higher ratio means more mel frames per grapheme)')\n",
    "parser.add_argument('--clean-output-folder', action=\"store_true\",\n",
    "                    help='Clean output folder before writing new data')\n",
    "args = parser.parse_args()\n",
    "\n",
    "if \"speechbrain\" in args.input_directory:\n",
    "    args.corpus_name = \"speechbrain\"\n",
    "    args.transpose_mel = False\n",
    "    SAMPLING_RATE = 16000 # hz\n",
    "    HOP_LENGTH_IN_MS = 10 # in ms\n",
    "    WIN_LENGTH_IN_MS = 25 # in ms\n",
    "    HOP_LENGTH = int(HOP_LENGTH_IN_MS * SAMPLING_RATE / 1000) # convert HOP_LENGTH to samples\n",
    "    WIN_LENGTH = int(WIN_LENGTH_IN_MS * SAMPLING_RATE / 1000) # convert WIN_LENGTH to samples\n",
    "elif \"ljspeech\" in args.input_directory:\n",
    "    args.corpus_name = \"ljspeech\"\n",
    "    args.transpose_mel = True\n",
    "    SAMPLING_RATE = 22050 # hz\n",
    "    HOP_LENGTH = 256 # in samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81205290",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "050c2ce5-b47b-4bf5-96a1-fd8bedc6330a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def save_to_disk(tensor, word, utt_id, count, output_directory):\n",
    "    output_directory = os.path.join(output_directory, word)\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    save_path = os.path.join(output_directory, f'{word}__{utt_id}__occ{count}.pt')\n",
    "    torch.save(tensor, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8affea0-41bb-4739-b9ee-496642017f14",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# load speech reps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "225bb850-5a9e-4d90-bc1e-3c44c9d8048d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully loaded subset of utts from file /home/s1785140/data/ljspeech_fastpitch/respeller_uttids.txt\n",
      "loading mels from disk in dir /home/s1785140/speechbrain/templates/speech_recognition_CharTokens_NoLM/data/ljspeech_dumped_feats for 6551 utts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 6551/6551 [04:41<00:00, 23.30it/s]\n"
     ]
    }
   ],
   "source": [
    "debug = True\n",
    "if debug:\n",
    "    # load fewer utts\n",
    "    max_utts = args.max_utts_to_generate\n",
    "else:\n",
    "    max_utts = None\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "map_location = 'cuda' if cuda else 'cpu'\n",
    "\n",
    "if args.type == \"hubert\":\n",
    "    with open(args.input_directory, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    num_of_utts = len(lines)\n",
    "    utt_id2speechreps = {l.split('|')[0]:l.split('|')[1] for l in lines}\n",
    "    utt_ids = sorted(utt_id2speechreps.keys()) # ensure we always process utts in same alphabetical order\n",
    "elif args.type == \"wav2vec2\":\n",
    "    num_of_utts = len(os.listdir(args.input_directory))\n",
    "    utt_ids = sorted(file.split('.')[0] for file in os.listdir(args.input_directory))\n",
    "elif args.type == \"mel\":\n",
    "    if args.utt_id_list:\n",
    "        # we specified a subset of utt ids\n",
    "        with open(args.utt_id_list, 'r') as f:\n",
    "            utt_ids = f.read().splitlines()\n",
    "        print(f\"successfully loaded subset of utts from file {args.utt_id_list}\")\n",
    "    else:\n",
    "        # all files in directory\n",
    "        utt_ids = list(sorted(file.split('.')[0] for file in os.listdir(args.input_directory)))\n",
    "    num_of_utts = len(utt_ids)\n",
    "    utt_ids = utt_ids[:max_utts]\n",
    "    utt_id2speechreps = {}\n",
    "    print(f\"loading mels from disk in dir {args.input_directory} for {len(utt_ids)} utts\")\n",
    "    for utt_id in tqdm(utt_ids):\n",
    "        # load mel data\n",
    "        mel_path = os.path.join(args.input_directory, f'{utt_id}.pt')\n",
    "        mel = torch.load(mel_path, map_location=map_location)\n",
    "        if args.transpose_mel:\n",
    "            mel = mel.transpose(0,1)\n",
    "\n",
    "        # mels should be of shape (T, D) now\n",
    "        utt_id2speechreps[utt_id] = mel\n",
    "else:\n",
    "    raise ValueError(f\"invalid input type {args.type}\")\n",
    "\n",
    "# sanity check - assert that each utt has a corresponding alignment\n",
    "alignment_files = set(os.listdir(args.alignments))\n",
    "for utt_id in utt_ids:\n",
    "    assert f\"{utt_id}.TextGrid\" in alignment_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a8a42a-79ba-468d-9cb8-cae8ef38cb95",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# perform splitting of mel specs using MFA alignments and save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8392ea9e-a6ac-4084-9558-5e3c1921b96a",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def parse_textgrid(tier, sampling_rate, hop_length, ignore_all_pauses=True):\n",
    "    # latest MFA replaces silence phones with \"\" in output TextGrids\n",
    "    sil_phones = [\"sil\", \"sp\", \"spn\", \"\"]\n",
    "    utt_start_time = tier[0].start_time\n",
    "    utt_end_time = tier[-1].end_time\n",
    "    phones = []\n",
    "    durations = [] # NOTE includes durations of silences\n",
    "    start_frames = []\n",
    "    end_frames = []\n",
    "    for i, t in enumerate(tier._objects):\n",
    "        s, e, p = t.start_time, t.end_time, t.text\n",
    "        if p not in sil_phones:\n",
    "            phones.append(p)\n",
    "            start_frames.append(int(np.ceil(s * sampling_rate / hop_length)))\n",
    "            end_frames.append(int(np.ceil(e * sampling_rate / hop_length)))\n",
    "            durations.append(int(np.ceil(e * sampling_rate / hop_length)\n",
    "                                 - np.ceil(s * sampling_rate / hop_length)))\n",
    "        else:\n",
    "            if not ignore_all_pauses:\n",
    "                if (i == 0) or (i == len(tier) - 1):\n",
    "                    # leading or trailing silence\n",
    "                    phones.append(\"sil\")\n",
    "                else:\n",
    "                    # short pause between words\n",
    "                    phones.append(\"sp\")\n",
    "\n",
    "    n_samples = utt_end_time * sampling_rate\n",
    "    n_frames = n_samples / hop_length\n",
    "    # fix occasional length mismatches at the end of utterances when\n",
    "    # duration in samples is an integer multiple of hop_length\n",
    "    if n_frames.is_integer():\n",
    "        durations[-1] += 1\n",
    "    return phones, durations, start_frames, end_frames, utt_start_time, utt_end_time\n",
    "\n",
    "def extract_reprs_with_timestamps(total_num_frames, start_time, end_time, utt_duration):\n",
    "    \"\"\"\n",
    "    extract subsequence of 'repr' that corresponds to a particular word\n",
    "    function expects input to be of dimension 2: (timesteps, hidden_size)\n",
    "    \"\"\"\n",
    "    start_fraction = start_time / utt_duration\n",
    "    end_fraction = end_time / utt_duration\n",
    "    # start_idx = math.floor(start_fraction * total_num_frames)\n",
    "    # end_idx = math.ceil(end_fraction * total_num_frames)\n",
    "    start_idx = int(start_fraction * total_num_frames)\n",
    "    end_idx = int(end_fraction * total_num_frames)\n",
    "    # start_idx = math.floor(start_fraction * total_num_frames)\n",
    "    # end_idx = math.ceil(end_fraction * total_num_frames)\n",
    "    num_frames = end_idx - start_idx\n",
    "    return start_idx, end_idx, num_frames\n",
    "\n",
    "def parse_textgrid2(tier, mel_spectrogram, ignore_all_pauses=True):\n",
    "    # latest MFA replaces silence phones with \"\" in output TextGrids\n",
    "    total_num_frames = mel_spectrogram.size(0)\n",
    "    sil_phones = [\"sil\", \"sp\", \"spn\", \"\"]\n",
    "    utt_start_time = tier[0].start_time\n",
    "    utt_end_time = tier[-1].end_time\n",
    "    words = []\n",
    "    word_durations = [] \n",
    "    sil_durations = []\n",
    "    start_frames = []\n",
    "    end_frames = []\n",
    "    for i, t in enumerate(tier._objects):\n",
    "        start, end, token = t.start_time, t.end_time, t.text\n",
    "        start_idx, end_idx, num_frames = extract_reprs_with_timestamps(total_num_frames, start, end, utt_end_time)\n",
    "    \n",
    "        if token not in sil_phones:\n",
    "            words.append(token)\n",
    "            start_frames.append(start_idx)\n",
    "            end_frames.append(end_idx)\n",
    "            word_durations.append(num_frames)\n",
    "        else:\n",
    "            sil_durations.append(num_frames)\n",
    "\n",
    "    return words, word_durations, sil_durations, start_frames, end_frames, utt_start_time, utt_end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40b8c9c7-1987-4d94-bc47-7aff00b204ae",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(stop_words)=179\n",
      "len(stop_words)=197\n",
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'b', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'c', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'e', 'each', 'f', 'few', 'for', 'from', 'further', 'g', 'h', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it's\", 'its', 'itself', 'j', 'just', 'k', 'l', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'n', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'p', 'q', 'r', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she's\", 'should', \"should've\", 'shouldn', \"shouldn't\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'u', 'under', 'until', 'up', 'v', 've', 'very', 'w', 'was', 'wasn', \"wasn't\", 'we', 'were', 'weren', \"weren't\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'x', 'y', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves', 'z']\n"
     ]
    }
   ],
   "source": [
    "# get list of wordnet nltk stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(f\"{len(stop_words)=}\")\n",
    "\n",
    "# add to stop_words all letters in alphabet, since tokenisation might create these edge cases \n",
    "# for example, \"wasn't\" might get tokenized as \"wasn\" and \"t\"\n",
    "stop_words.update(string.ascii_lowercase)\n",
    "print(f\"{len(stop_words)=}\")\n",
    "print(sorted(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "904ce0a3-78de-4401-bc11-3d936f06a540",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def allowed_word(word: str) -> bool:\n",
    "    if len(word) <= 1:\n",
    "        return False\n",
    "    if word == '--':\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d30b8ce-c345-40e2-8ae5-bb1fc4f468bf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning output folder... '/home/s1785140/speechbrain/templates/speech_recognition_CharTokens_NoLM/data/ljspeech_dumped_feats_word_aligned'\n",
      "finished cleaning output folder!\n",
      "split speech reps using word alignments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|█████████████████████████████████████████████████████████                | 5121/6551 [1:19:09<30:24,  1.28s/it]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if args.clean_output_folder:\n",
    "    print(f\"cleaning output folder... '{args.output_directory}'\")\n",
    "    shutil.rmtree(args.output_directory, ignore_errors=True)\n",
    "    print(\"finished cleaning output folder!\")\n",
    "\n",
    "longest_word = ''\n",
    "longest_word_utt_id = ''\n",
    "longest_word_num_frames = 0\n",
    "buffer_frames = 0\n",
    "num_tokens_skipped_due_to_mel_to_graphemes_ratio = 0\n",
    "num_stopwords_skipped = 0\n",
    "skip_stopwords = True\n",
    "filter_by_len_ratio = False\n",
    "\n",
    "# glob recursively number of torch tensor files in output directory\n",
    "orig_num_files = len(glob.glob(os.path.join(args.output_directory, '**', '*.pt'), recursive=True))\n",
    "\n",
    "# split each speech reps file using the word-level alignments\n",
    "print(\"split speech reps using word alignments\")\n",
    "for utt_id in tqdm(utt_ids):\n",
    "    # load speech reps\n",
    "    if args.type == \"hubert\":\n",
    "        reps = utt_id2speechreps[utt_id]\n",
    "        reps = [int(s)+args.padding_idx_offset for s in reps.split(' ')] # NOTE add 1 to each index so that 0 is available as a padding_idx\n",
    "        reps = torch.tensor(reps)\n",
    "        reps.requires_grad = False\n",
    "\n",
    "        # check dimensions\n",
    "        if reps.dim() == 1:\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(\"speech representations have an incorrect number of dimensions\")\n",
    "    elif args.type == \"mel\":\n",
    "        reps = utt_id2speechreps[utt_id]\n",
    "        reps.requires_grad = False\n",
    "\n",
    "        # check dimensions\n",
    "        if reps.dim() == 2:\n",
    "            if args.corpus_name == 'speechbrain' and reps.size(1) == 40:\n",
    "                pass\n",
    "            elif args.corpus_name == 'ljspeech' and reps.size(1) == 80:\n",
    "                pass\n",
    "            else:\n",
    "                raise ValueError(f\"feat dimension is wrong size for corpus {reps.size(1)=}\")    \n",
    "        else:\n",
    "            raise ValueError(f\"speech representations have an incorrect number of dimensions {reps.dim()=}\")\n",
    "    else:\n",
    "        raise ValueError(f\"invalid input type {args.type}\")\n",
    "\n",
    "    tg_path = f\"{args.alignments}/{utt_id}.TextGrid\"\n",
    "    tg = tgt.io.read_textgrid(tg_path, include_empty_intervals=True)\n",
    "\n",
    "    words, word_durs, sil_durations, start_frames, end_frames, utt_start, utt_end = parse_textgrid2(\n",
    "        tg.get_tier_by_name('words'), reps, ignore_all_pauses=True\n",
    "    )\n",
    "    \n",
    "    word_occ_in_utt_counter = Counter()\n",
    "    mel = utt_id2speechreps[utt_id]\n",
    "\n",
    "    # verify that MFA frame durations match up with the extracted mels\n",
    "    assert mel.size(0) == (sum(word_durs) + sum(sil_durations)), f\"{mel.size(0)=} != {sum(word_durs)=} + {sum(sil_durations)=}\" \n",
    "\n",
    "    # iterate over words in utterance\n",
    "    for j, (word, dur, start_frame, end_frame) in enumerate(zip(words, word_durs, start_frames, end_frames)):\n",
    "        if skip_stopwords and word in stop_words:\n",
    "            num_stopwords_skipped += 1\n",
    "            continue\n",
    "\n",
    "        if not allowed_word(word):\n",
    "            continue\n",
    "\n",
    "        skip_word = False\n",
    "        normalise_non_ascii = False\n",
    "        for c in word:\n",
    "            if c not in string.ascii_lowercase:\n",
    "                s = f'WARNING: char {c} in word {word}'\n",
    "                if SKIP_NON_ASCII or word in WORDS_TO_SKIP:\n",
    "                    s += '. skipping!...'\n",
    "                    skip_word = True\n",
    "                else:\n",
    "                    normalise_non_ascii = True\n",
    "\n",
    "                print(s)\n",
    "\n",
    "        if skip_word:\n",
    "            continue\n",
    "                \n",
    "        if normalise_non_ascii: # normalise word\n",
    "            prenorm_word = word\n",
    "            # remove trailing '-'\n",
    "            word = word.rstrip('-')\n",
    "            # convert diacritics to ascii\n",
    "            word = unidecode.unidecode(word)\n",
    "            print(f\"\\tnormalised '{prenorm_word}' to '{word}'\")\n",
    "        \n",
    "        # check if word is the longest word we have seen so far\n",
    "        word_dur = end_frame - start_frame \n",
    "        if word_dur > longest_word_num_frames:\n",
    "            longest_word_num_frames = word_dur\n",
    "            longest_word = word\n",
    "            longest_word_utt_id = utt_id\n",
    "\n",
    "        # extract mel\n",
    "        a = max(0, start_frame - buffer_frames)\n",
    "        b = min(mel.size(0), end_frame + buffer_frames)\n",
    "        wordaligned_mel = mel[a:b]\n",
    "\n",
    "        # get mel to graphemes ratio\n",
    "        mel_to_graphemes_ratio = word_dur / len(word)\n",
    "        if (mel_to_graphemes_ratio < args.mel_to_graphemes_ratio_lowest_threshold or \n",
    "            mel_to_graphemes_ratio > args.mel_to_graphemes_ratio_highest_threshold):\n",
    "            num_tokens_skipped_due_to_mel_to_graphemes_ratio += 1\n",
    "            # likely that alignment is poor, so skip this word\n",
    "            continue\n",
    "        else:\n",
    "            # save extracted mel to disk\n",
    "            word_occ_in_utt_counter[word] += 1\n",
    "            extracted_timesteps = wordaligned_mel.size(0)\n",
    "\n",
    "            assert dur == extracted_timesteps - (start_frame - a) - (b - end_frame) == word_dur, f\"{dur=}, {extracted_timesteps=}, {word_dur=}\"\n",
    "            save_to_disk(wordaligned_mel, word, utt_id, word_occ_in_utt_counter[word], args.output_directory)\n",
    "\n",
    "print(f\"wordtype with longest num of timesteps is '{longest_word}' from\", longest_word_utt_id, \"with len\", longest_word_num_frames)\n",
    "print(\"you can set transformer max_source_positions to this\")\n",
    "\n",
    "new_num_files = len(glob.glob(os.path.join(args.output_directory, '**', '*.pt'), recursive=True))\n",
    "\n",
    "print(f\"Added {new_num_files - orig_num_files} files to {args.output_directory}, \\nnow contains {new_num_files} files, used to contain {orig_num_files} files\")\n",
    "\n",
    "print(f\"Skipped {num_tokens_skipped_due_to_mel_to_graphemes_ratio} word tokens due to mel to graphemes ratio being outside of [{args.mel_to_graphemes_ratio_lowest_threshold}, {args.mel_to_graphemes_ratio_highest_threshold}]\")\n",
    "print(f\"Skipped {num_stopwords_skipped} word tokens as they were stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b402ed",
   "metadata": {},
   "source": [
    "## sanity check alignments by generating wordaligned spectrograms using griffin-lim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b97e976",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█████████▌                                                                | 7996/62116 [10:01<54:18, 16.61it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 38%|███████████████████████████▋                                             | 23550/62116 [33:10<30:21, 21.18it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 60%|███████████████████████████████████████████▌                             | 37118/62116 [56:22<36:51, 11.31it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 76%|██████████████████████████████████████████████████████▎                | 47509/62116 [1:14:20<29:02,  8.38it/s]"
     ]
    }
   ],
   "source": [
    "# grab all mels and words in the output directory\n",
    "\n",
    "# glob pytorch tensors from nested folders in output directory\n",
    "mel_paths = glob.glob(f'{args.output_directory}/**/*.pt', recursive=True)\n",
    "\n",
    "# load mels into list\n",
    "mels = []\n",
    "words = []\n",
    "for mel_path in tqdm(mel_paths):\n",
    "    mel = torch.load(mel_path)\n",
    "    mels.append(mel)\n",
    "\n",
    "    # also get word from path\n",
    "    word = mel_path.split('/')[-2]\n",
    "    words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77f83a1",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def griffin_lim_synthesise(mel, n_iter=100):\n",
    "    \"\"\"Synthesises audio from mel spectrogram using Griffin-Lim algorithm.\n",
    "    Args:\n",
    "        mel (torch.Tensor): Mel spectrogram (B, C, T).\n",
    "        n_iter (int): Number of iterations for Griffin-Lim algorithm.\n",
    "    Returns:\n",
    "        torch.Tensor: Audio waveform (B, T).\n",
    "    \"\"\"\n",
    "    mel = mel.detach().cpu().numpy()\n",
    "    mel = librosa.feature.inverse.mel_to_audio(\n",
    "        mel, sr=SAMPLING_RATE, n_fft=400, hop_length=HOP_LENGTH, win_length=WIN_LENGTH,\n",
    "        window='hamming', center=True, pad_mode='constant', power=1.0, n_iter=n_iter,\n",
    "        )\n",
    "    mel = torch.from_numpy(mel).float()\n",
    "    return mel\n",
    "\n",
    "def reshape_mel_for_librosa(mel):\n",
    "    mel = mel.unsqueeze(0) # make batch dimension\n",
    "    mel = mel.transpose(1, 2)\n",
    "    return mel\n",
    "\n",
    "def synthesise_and_play_Audio(mel, n_iter=100):\n",
    "    mel = reshape_mel_for_librosa(mel)\n",
    "    audio = griffin_lim_synthesise(mel, n_iter)\n",
    "    return Audio(audio, rate=SAMPLING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6415644b",
   "metadata": {},
   "source": [
    "## listen to generated audio for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3276ab",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# words_to_synth = \"stopwords\"\n",
    "words_to_synth = \"functionwords\"\n",
    "\n",
    "tuples = list(zip(mels, words))\n",
    "# filter out words that are not in the wordnet nltk stopwords list\n",
    "if words_to_synth == \"stopwords\":\n",
    "    tuples = [t for t in tuples if t[1] in stop_words]\n",
    "# filter out words that are not in the function word list\n",
    "elif words_to_synth == \"functionwords\":\n",
    "    tuples = [t for t in tuples if t[1] not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372bd11f",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# NOTE these might sound very bad for two reasons\n",
    "# 1. stft -> mel spec is not lossless\n",
    "# 2. (speechbrain) ASR features are fewer bins (i.e. 40 rather than 80 mel spec bins)\n",
    "# but still good enough for sanity checking alignments!\n",
    "NUM_TO_LISTEN = None\n",
    "# NUM_TO_LISTEN = 20\n",
    "\n",
    "# generate wavs from mels and then concatenate them into a single wav separated by silences and play in notebook\n",
    "# NOTE this is not lossless, but still good enough for sanity checking alignments!\n",
    "def generate_wav_from_mel(mel, n_iter=100):\n",
    "    mel = reshape_mel_for_librosa(mel)\n",
    "    audio = griffin_lim_synthesise(mel, n_iter)\n",
    "    audio = audio.squeeze(0)\n",
    "    audio = audio.numpy()\n",
    "    return audio\n",
    "\n",
    "def generate_wav_from_mels_with_silences(mels, n_iter=100, silence_duration=0.5):\n",
    "    wavs = []\n",
    "    for mel in mels:\n",
    "        wavs.append(generate_wav_from_mel(mel, n_iter))\n",
    "        wavs.append(np.zeros(int(silence_duration * SAMPLING_RATE)))\n",
    "    wav = np.concatenate(wavs)\n",
    "    return wav\n",
    "\n",
    "def generate_wav_from_mels_with_silences_and_play(mels, n_iter=100, silence_duration=0.5):\n",
    "    wav = generate_wav_from_mels_with_silences(mels, n_iter, silence_duration)\n",
    "    return Audio(wav, rate=SAMPLING_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819de895",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "generate_all = False\n",
    "if generate_all:\n",
    "    # Generate all words in tuples\n",
    "    mels_to_gen = [t[0] for t in tuples[:NUM_TO_LISTEN]]\n",
    "    words_to_gen = [t[1] for t in tuples[:NUM_TO_LISTEN]]\n",
    "\n",
    "    # sort mels and words by alphabetical order of words\n",
    "    mels_to_gen, words_to_gen = zip(*sorted(zip(mels_to_gen, words_to_gen), key=lambda x: x[1]))\n",
    "\n",
    "    print(\"Generating audio for following words:\", words_to_gen)\n",
    "    generate_wav_from_mels_with_silences_and_play(mels_to_gen, silence_duration=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1884179c",
   "metadata": {},
   "source": [
    "# Filter poor alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db75ca43",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" filter out alignments based on certain criteria \n",
    "- where ratio of frames to graphemes is too low\n",
    "- graphemes is too short\n",
    "\n",
    "some problematic ones:\n",
    "    - surpassed\n",
    "\n",
    "from the below code, seems that mel-to-graphemes ratio of \n",
    "    - > 15.0 includes many examples of poor alignments\n",
    "    - < \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f51d521",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# get mel to graphemes ratio for each word token\n",
    "mel_to_graphemes_ratio = []\n",
    "for mel, word in tqdm(tuples):\n",
    "    mel_to_graphemes_ratio.append((mel, word, mel.shape[0] / len(word)))\n",
    "\n",
    "# sort by ratio\n",
    "mel_to_graphemes_ratio = sorted(mel_to_graphemes_ratio, key=lambda x: x[2])\n",
    "\n",
    "NUM_TO_LISTEN = 30\n",
    "\n",
    "# synthesise words with highest ratios (lowest to highest ratio)\n",
    "mels_to_gen = [t[0] for t in mel_to_graphemes_ratio[-NUM_TO_LISTEN:]]\n",
    "words_to_gen = [t[1] for t in mel_to_graphemes_ratio[-NUM_TO_LISTEN:]]\n",
    "ratios = [t[2] for t in mel_to_graphemes_ratio[-NUM_TO_LISTEN:]]\n",
    "\n",
    "print(\"Generating audio for following words:\", list(zip(words_to_gen, ratios)))\n",
    "generate_wav_from_mels_with_silences_and_play(mels_to_gen, silence_duration=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c18bff7",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# synthesise words with lowest ratios (lowest to highest ratio)\n",
    "mels_to_gen = [t[0] for t in mel_to_graphemes_ratio[:NUM_TO_LISTEN]]\n",
    "words_to_gen = [t[1] for t in mel_to_graphemes_ratio[:NUM_TO_LISTEN]]\n",
    "ratios = [t[2] for t in mel_to_graphemes_ratio[:NUM_TO_LISTEN]]\n",
    "\n",
    "print(\"Generating audio for following words:\", list(zip(words_to_gen, ratios)))\n",
    "generate_wav_from_mels_with_silences_and_play(mels_to_gen, silence_duration=0.4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ad23a0",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# calculate mel to graphemes ratio threshold for bottom 5% and top 5%\n",
    "# NOTE: this is not a good way to do this, since the distribution is not normal\n",
    "# but it's good enough for now\n",
    "bottom_5_percent = int(len(mel_to_graphemes_ratio) * 0.05)\n",
    "top_5_percent = int(len(mel_to_graphemes_ratio) * 0.95)\n",
    "\n",
    "bottom_5_percent_mel_to_graphemes_ratio = mel_to_graphemes_ratio[bottom_5_percent][2]\n",
    "top_5_percent_mel_to_graphemes_ratio = mel_to_graphemes_ratio[top_5_percent][2]\n",
    "\n",
    "print(f\"{bottom_5_percent_mel_to_graphemes_ratio=}\")\n",
    "print(f\"{top_5_percent_mel_to_graphemes_ratio=}\")\n",
    "print(\"Use these ratios to filter what speech reps get saved to disk!!!\")\n",
    "\n",
    "# filter out words with mel to graphemes ratio below threshold\n",
    "# NOTE: this is not a good way to do this, since the distribution is not normal\n",
    "# but it's good enough for now\n",
    "mels_to_gen = [t[0] for t in mel_to_graphemes_ratio if t[2] > bottom_5_percent_mel_to_graphemes_ratio]\n",
    "words_to_gen = [t[1] for t in mel_to_graphemes_ratio if t[2] > bottom_5_percent_mel_to_graphemes_ratio]\n",
    "ratios = [t[2] for t in mel_to_graphemes_ratio if t[2] > bottom_5_percent_mel_to_graphemes_ratio]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd935f97",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "raise ValueError(\"stop before creating datasplits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd5c7db-860e-4daf-aa2e-52f6d6a12cff",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# create train,dev,test datasplits for training respeller\n",
    "\n",
    "We hold out WORDTYPES from training for the dev and test splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1d3344-f2ed-44f3-bf87-0f0b038c0421",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835dc300-35a9-4f21-a5d3-584919022662",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "\n",
    "random.seed(1337)\n",
    "\n",
    "train_ratio, dev_ratio, test_ratio = [0.9, 0.05, 0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fc263e-51ae-4d31-b7c8-b5b75454bf24",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# get oov wordtypes list (words that are not seen in tts training)\n",
    "oov_wordlist_path = '/home/s1785140/data/ljspeech_fastpitch/oov_list.json'\n",
    "with open(oov_wordlist_path, 'r') as f:\n",
    "    oovs_and_freqs = json.load(f)\n",
    "    \n",
    "all_wordtypes = set(oovs_and_freqs.keys())\n",
    "print(f'original before cleaning/sampling {len(all_wordtypes)=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c6d733-557d-49bf-8ef2-63983caba003",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# clean/remove words that do not have speech reps\n",
    "words_with_aligned_mels = set(os.listdir(args.output_directory))\n",
    "words_no_mels = all_wordtypes - words_with_aligned_mels\n",
    "print(f'{len(words_no_mels)}=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a134aa68-17fb-425e-b5ee-0a4bd19407bd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "print(\"list of words to be excluded from respeller training as they do not have mels (likely due to how normalisation is different between mfa and our own data processing):\")\n",
    "words_no_mels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e5f5f6",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cae348a-0333-484a-8245-383537c4877f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# remove these problematic words from respeller training dev test\n",
    "for w in words_no_mels:\n",
    "    del oovs_and_freqs[w]\n",
    "    \n",
    "all_wordtypes = set(oovs_and_freqs.keys())\n",
    "print(f'original after cleaning {len(all_wordtypes)=}')\n",
    "\n",
    "dev_N = int(dev_ratio * len(all_wordtypes))\n",
    "test_N = int(test_ratio * len(all_wordtypes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ce65fc-a973-4fbc-8b82-e63dd0131ee8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def sample_and_remove(s: set, N: int):\n",
    "    \"\"\"sample N words from set s\n",
    "    then remove these words from the set\"\"\"\n",
    "    sampled = random.sample(s, N)\n",
    "    for item in sampled:\n",
    "        s.remove(item)\n",
    "    return set(sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3e4eed-a336-43c1-8634-8b54707034fd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#get dev and test splits\n",
    "oov_singletons = set(wordtype for wordtype, freq in oovs_and_freqs.items() if freq == 1)\n",
    "assert len(oov_singletons) > dev_N + test_N, \"not enough OOV singletons to create dev and test sets\" \n",
    "print(f'before sampling dev and test {len(oov_singletons)=}')\n",
    "\n",
    "dev = sample_and_remove(oov_singletons, dev_N)\n",
    "print(f'after sampling dev {len(oov_singletons)=}, {len(dev)=}')\n",
    "\n",
    "test = sample_and_remove(oov_singletons, test_N)\n",
    "print(f'after sampling test {len(oov_singletons)=}, {len(test)=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69f58b0-b7c7-4c54-9333-adee647b291b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "list(dev)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb7c6d1-0c33-479d-8cbf-859fba31d7bd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "list(test)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01092fe6-fd9f-42db-a206-01097ef558e9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#get train split\n",
    "print(f'before removing dev and test wordtypes {len(all_wordtypes)=}')\n",
    "for word in dev | test:\n",
    "    all_wordtypes.remove(word)\n",
    "print(f'after removing dev and test wordtypes {len(all_wordtypes)=}')\n",
    "\n",
    "train = set(all_wordtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa0bce1-1baa-422d-a6eb-fb53e8d05964",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# sanity checks\n",
    "assert len(dev.intersection(test)) == 0\n",
    "assert len(train.intersection(dev)) == 0\n",
    "assert len(train.intersection(test)) == 0\n",
    "print(\"Good! No overlapping words between train, dev, and test!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455c0255-bf59-480b-9d83-6b43376125f0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# write to disk\n",
    "def save_wordlist(path, words):\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(sorted(list(words)), f, indent=4)\n",
    "        \n",
    "train_path = '/home/s1785140/data/ljspeech_fastpitch/respeller_train_words.json'\n",
    "dev_path = '/home/s1785140/data/ljspeech_fastpitch/respeller_dev_words.json'\n",
    "test_path = '/home/s1785140/data/ljspeech_fastpitch/respeller_test_words.json'\n",
    "\n",
    "save_wordlist(train_path, train)\n",
    "save_wordlist(dev_path, dev)\n",
    "save_wordlist(test_path, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61f2ebd-5d54-4e81-ae06-295add409a5e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## G2P selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c63569d-3f12-45c0-ba6a-a4379235d35f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "vscode": {
   "interpreter": {
    "hash": "2ed8b345ca459c03f8526b7bad908d345c967956aebabae0df909223b22f2255"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
