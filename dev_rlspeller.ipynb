{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4207e21b-d910-485b-8b9c-3fc7d5a59c5a",
   "metadata": {},
   "source": [
    "# Goal of this notebook\n",
    "\n",
    "Develop a training loop for finetuning ASR models using TTS loss by recreating RL training found in RL4LMs/rl4lms/envs/text_generation/training_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70687ad-9f13-4cdf-8ef7-07eecb34f496",
   "metadata": {},
   "source": [
    "# automatic reloading magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9618cff7-0dc3-4a44-a4f7-7dbe732ded51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738aac35-fdd1-4452-aff0-92caa77ec5ab",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c8a89c2-38ea-4778-8028-0c7854152301",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import List, Dict, Tuple, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3a0b2d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b236619f-6c04-44b1-a697-a342417a6694",
   "metadata": {},
   "source": [
    "# HPARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8a11879-640c-433e-a192-9c8b551d8ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    \"softdtw_temp\": 0.01,\n",
    "    \"softdtw_bandwidth\": 120,\n",
    "    \"dist_func\": \"l1\",\n",
    "    \"sentencepiece_model_path\": \"/home/s1785140/speechbrain/templates/speech_recognition_CharTokens_NoLM/Tokenizer/save/0_char.model\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e877615-a6ff-4755-a7e4-e6f72e542432",
   "metadata": {},
   "source": [
    "# TOKENIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0505c38d-bc92-4f84-9f1f-cfc942ad337d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "# load pretrained tokenizer used to tokenizer ASR training inputs \n",
    "import sentencepiece as spm \n",
    "spm_path = hparams[\"sentencepiece_model_path\"]\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(spm_path)\n",
    "print(sp.vocab_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4af21be-9224-4c45-87f6-b86a8d7c63ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 10 2 12 12 4 1 17 4 9 12 11 1 16 20 1 6 5 16 2 1 7 8 1 26 5 8 4 6\n"
     ]
    }
   ],
   "source": [
    "# test tokenizer\n",
    "s = \"hello world my name is jason\"\n",
    "# TODO pass string through text cleaners? \n",
    "encoded = sp.EncodeAsIds(s)\n",
    "assert 0 not in encoded, \"tried to encode an unknown character\"\n",
    "print(\" \".join(str(idx) for idx in encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe3c6048-79c5-47b4-9524-88d2e36ad8d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello world my name is jason'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.DecodeIds(encoded)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd650fd4",
   "metadata": {},
   "source": [
    "# LOAD YOUR PRETRAINED ASR MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56bf160c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from templates.speech_recognition_CharTokens_NoLM.ASR.train import ASR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "155258ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'modules'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# initialise trainer (we don't want to train, but model is tightly coupled with trainer)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m asr_brain \u001b[39m=\u001b[39m ASR(\n\u001b[0;32m----> 3\u001b[0m     modules\u001b[39m=\u001b[39mhparams[\u001b[39m\"\u001b[39;49m\u001b[39mmodules\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m      4\u001b[0m     opt_class\u001b[39m=\u001b[39mhparams[\u001b[39m\"\u001b[39m\u001b[39mopt_class\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m      5\u001b[0m     hparams\u001b[39m=\u001b[39mhparams,\n\u001b[1;32m      6\u001b[0m     run_opts\u001b[39m=\u001b[39mrun_opts,\n\u001b[1;32m      7\u001b[0m     checkpointer\u001b[39m=\u001b[39mhparams[\u001b[39m\"\u001b[39m\u001b[39mcheckpointer\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;31mKeyError\u001b[0m: 'modules'"
     ]
    }
   ],
   "source": [
    "# initialise trainer (we don't want to train, but model is tightly coupled with trainer)\n",
    "asr_brain = ASR(\n",
    "    modules=hparams[\"modules\"],\n",
    "    opt_class=hparams[\"opt_class\"],\n",
    "    hparams=hparams,\n",
    "    run_opts=run_opts,\n",
    "    checkpointer=hparams[\"checkpointer\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a387be4-9390-459d-92a5-2389ae883ed4",
   "metadata": {},
   "source": [
    "# DATAPOOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c8214f-8ecc-4dd2-9e2e-0437d2376261",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load SpellerDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf97be5-efdb-4295-852a-9975ce615bba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from rlspeller.dataset import SpellerDataset\n",
    "\n",
    "def load_dataset(split):\n",
    "    wordaligned_speechreps_dir = '/home/s1785140/data/ljspeech_speechbrain/wordaligned_mels'\n",
    "    wordlists = {\n",
    "        \"train\": '/home/s1785140/data/ljspeech_fastpitch/respeller_train_words.json',\n",
    "        \"val\": '/home/s1785140/data/ljspeech_fastpitch/respeller_val_words.json',\n",
    "        \"test\": '/home/s1785140/data/ljspeech_fastpitch/respeller_test_words.json',\n",
    "    }\n",
    "    \n",
    "    return SpellerDataset(wordaligned_speechreps_dir, wordlists[split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfd206e-a80e-4490-99dc-6a077f829ecc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define Sample and Datapool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13ff58d-9f56-4c45-aed4-85ed9ddddff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from abc import abstractclassmethod\n",
    "\n",
    "@dataclass(init=True)\n",
    "class Sample:\n",
    "    id: str # \n",
    "    gt_mel_path: str # full path to mel spectrogram\n",
    "    gt_text: str # original spelling of word\n",
    "    meta_data: Dict[str, Any] = None\n",
    "\n",
    "\n",
    "class ASRPool:\n",
    "    def __init__(self, samples: List[Sample]):\n",
    "        self._samples = samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._samples)\n",
    "\n",
    "    def __getitem__(self, ix: int) -> Sample:\n",
    "        if ix >= len(self):\n",
    "            raise StopIteration\n",
    "        sample = self._samples[ix]\n",
    "        return sample, 1.0\n",
    "\n",
    "    def sample(self) -> Sample:\n",
    "        random_sample = random.choice(self._samples)\n",
    "        return random_sample\n",
    "\n",
    "    @abstractclassmethod\n",
    "    def prepare(cls, **args) -> 'TextGenPool':\n",
    "        \"\"\"\n",
    "        A factory method to instantiate data pool\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def split(self, split_ratios: List[float]) -> List['TextGenPool']:\n",
    "        start_ix = 0\n",
    "        pools = []\n",
    "        for ratio in split_ratios:\n",
    "            count = int(len(self) * ratio)\n",
    "            end_ix = start_ix + count\n",
    "            pools.append(type(self)(self._samples[start_ix: end_ix]))\n",
    "            start_ix = end_ix\n",
    "        return pools\n",
    "    \n",
    "class LJSpeech(ASRPool):\n",
    "    @classmethod\n",
    "    def prepare(cls, split: str, **args) -> 'ASRPool':\n",
    "        ds = load_dataset(split)\n",
    "        samples = []\n",
    "        for idx, item in tqdm(enumerate(ds)):\n",
    "            # sample = Sample(\n",
    "            #     id: f\"{split}_{idx}\" \n",
    "            #     gt_mel_path: str \n",
    "            #     gt_text: str \n",
    "            # )\n",
    "            samples.append(sample)\n",
    "\n",
    "        pool_instance = cls(samples)\n",
    "        return pool_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a7e707-e639-4aed-b069-220006aba329",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising respeller dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7479 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/s1785140/data/ljspeech_speechbrain/wordaligned_mels/abandon'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_datapool \u001b[39m=\u001b[39m LJSpeech\u001b[39m.\u001b[39;49mprepare(\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[10], line 49\u001b[0m, in \u001b[0;36mLJSpeech.prepare\u001b[0;34m(cls, split, **args)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m     48\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprepare\u001b[39m(\u001b[39mcls\u001b[39m, split: \u001b[39mstr\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39margs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m'\u001b[39m\u001b[39mASRPool\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m---> 49\u001b[0m     ds \u001b[39m=\u001b[39m load_dataset(split)\n\u001b[1;32m     50\u001b[0m     samples \u001b[39m=\u001b[39m []\n\u001b[1;32m     51\u001b[0m     \u001b[39mfor\u001b[39;00m idx, item \u001b[39min\u001b[39;00m tqdm(\u001b[39menumerate\u001b[39m(ds)):\n\u001b[1;32m     52\u001b[0m         \u001b[39m# sample = Sample(\u001b[39;00m\n\u001b[1;32m     53\u001b[0m         \u001b[39m#     id: f\"{split}_{idx}\" \u001b[39;00m\n\u001b[1;32m     54\u001b[0m         \u001b[39m#     gt_mel_path: str \u001b[39;00m\n\u001b[1;32m     55\u001b[0m         \u001b[39m#     gt_text: str \u001b[39;00m\n\u001b[1;32m     56\u001b[0m         \u001b[39m# )\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 11\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(split)\u001b[0m\n\u001b[1;32m      4\u001b[0m wordaligned_speechreps_dir \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/home/s1785140/data/ljspeech_speechbrain/wordaligned_mels\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      5\u001b[0m wordlists \u001b[39m=\u001b[39m {\n\u001b[1;32m      6\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m'\u001b[39m\u001b[39m/home/s1785140/data/ljspeech_fastpitch/respeller_train_words.json\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      7\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mval\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m'\u001b[39m\u001b[39m/home/s1785140/data/ljspeech_fastpitch/respeller_val_words.json\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      8\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m'\u001b[39m\u001b[39m/home/s1785140/data/ljspeech_fastpitch/respeller_test_words.json\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      9\u001b[0m }\n\u001b[0;32m---> 11\u001b[0m \u001b[39mreturn\u001b[39;00m SpellerDataset(wordaligned_speechreps_dir, wordlists[split])\n",
      "File \u001b[0;32m/disk/nfs/ostrom/s1785140/rlspeller/rlspeller/dataset.py:56\u001b[0m, in \u001b[0;36mSpellerDataset.__init__\u001b[0;34m(self, wordaligned_speechreps_dir, wordlist, max_examples_per_wordtype, text_cleaners, symbol_set, add_spaces, eos_symbol, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m tqdm(wordlist):\n\u001b[1;32m     54\u001b[0m     \u001b[39m# find all word aligned mels for the word\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     word_dir \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(wordaligned_speechreps_dir, word)\n\u001b[0;32m---> 56\u001b[0m     mel_files \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mlistdir(word_dir)\n\u001b[1;32m     57\u001b[0m     \u001b[39mif\u001b[39;00m max_examples_per_wordtype:\n\u001b[1;32m     58\u001b[0m         mel_files \u001b[39m=\u001b[39m mel_files[:max_examples_per_wordtype]\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/s1785140/data/ljspeech_speechbrain/wordaligned_mels/abandon'"
     ]
    }
   ],
   "source": [
    "train_datapool = LJSpeech.prepare('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f666a7-6b7e-4bb3-a723-af922d4e5565",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_datapool._samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe55d99-e0c0-4208-9020-2b297517972d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_get_datapool_by_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_datapool \u001b[38;5;241m=\u001b[39m \u001b[43m_get_datapool_by_split\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m val_datapool \u001b[38;5;241m=\u001b[39m _get_datapool_by_split(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m test_datapool \u001b[38;5;241m=\u001b[39m _get_datapool_by_split(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name '_get_datapool_by_split' is not defined"
     ]
    }
   ],
   "source": [
    "train_datapool = _get_datapool_by_split(\"train\")\n",
    "val_datapool = _get_datapool_by_split(\"val\")\n",
    "test_datapool = _get_datapool_by_split(\"test\")\n",
    "\n",
    "samples_by_split = {\n",
    "    \"train\": [(sample, weight)\n",
    "              for sample, weight in train_datapool],\n",
    "    \"val\": [sample for sample, _ in val_datapool],\n",
    "    \"test\": [sample for sample, _ in test_datapool]\n",
    "}\n",
    "return samples_by_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674722a6-d6ee-4feb-9f63-d8b8b893e619",
   "metadata": {},
   "source": [
    "# REWARD FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835c2708-462e-46a8-a945-bda08b859ed8",
   "metadata": {},
   "source": [
    "## funcs to load pretrained fastpitch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945147f5-5dc5-4367-affe-08ae0291e6d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "from fastpitch import models as fastpitch_model\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Fastpitch Model Config Parser', allow_abbrev=False)\n",
    "parser = fastpitch_model.parse_model_args('FastPitch', parser)\n",
    "args, unk_args = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1df11c-91d2-4aeb-b966-1dbc9e465def",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"WARNING!!! unknown args:\", unk_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ce3ef1-1359-4f8e-bfae-ee91a6bc3a54",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training command for no punctuation fastpitch:\n",
    "\n",
    "```bash\n",
    "cd \n",
    "source activate_respeller.sh\n",
    "\n",
    "cd ~/respeller/fastpitch\n",
    "\n",
    "EXP_NAME=halved_ljspeech_data_nospaces_noeos_pad_lowercase_nopunc\n",
    "\n",
    "DATA_ROOT=~/data/ljspeech_fastpitch\n",
    "CHECKPOINT_DIR=exps\n",
    "mkdir $CHECKPOINT_DIR\n",
    "HIFIGAN_CHKPT=~/pretrained_models/hifigan/ljspeech/LJ_V1/generator_v1\n",
    "HIFIGAN_CFG=~/pretrained_models/hifigan/ljspeech/LJ_V1/config.json\n",
    "MASTER_ADDR=`hostname -s`\n",
    "FILELIST_STEM=wav_text_filelist\n",
    "\n",
    "./sbatch.sh python train.py \\\n",
    "  --dataset-path $DATA_ROOT \\\n",
    "  --output $CHECKPOINT_DIR/$EXP_NAME \\\n",
    "  --training-files $DATA_ROOT/train_meta_half.txt \\\n",
    "  --validation-files $DATA_ROOT/val_meta_half.txt \\\n",
    "  --pitch-mean-std-file $DATA_ROOT/pitches_stats__${FILELIST_STEM}.json \\\n",
    "  --input-type char \\\n",
    "  --symbol-set english_pad_lowercase_nopunc \\\n",
    "  --text-cleaners lowercase_no_punc \\\n",
    "  --epochs 1000 \\\n",
    "  --epochs-per-checkpoint 10 \\\n",
    "  --batch-size 16 \\\n",
    "  --use-mas \\\n",
    "  --cuda \\\n",
    "  --hifigan $HIFIGAN_CHKPT \\\n",
    "  --hifigan-config $HIFIGAN_CFG \\\n",
    "  --use-sepconv \\\n",
    "  --master-addr $MASTER_ADDR \\\n",
    "  --checkpoint-path /home/s1785140/respeller/fastpitch/exps/halved_ljspeech_data_nospaces_noeos_pad_lowercase_nopunc/FastPitch_checkpoint_290.pt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e59671f-126c-46a1-899c-e3ff81adb541",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# change values of some args to match the config of the pretrained model \n",
    "args.local_rank = 0\n",
    "args.use_mas = True\n",
    "args.use_sepconv = True\n",
    "args.cuda = torch.cuda.is_available()\n",
    "args.input_type = 'char'\n",
    "args.symbol_set = 'english_pad_lowercase_nopunc'\n",
    "args.n_speakers = 1\n",
    "args.fastpitch_chkpt = \"/home/s1785140/respeller/fastpitch/exps/halved_ljspeech_data_nospaces_noeos_pad_lowercase_nopunc/FastPitch_checkpoint_1000.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e53c29d-d9ce-4e99-ae38-cac96fd5b007",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_checkpoint(args, model, filepath):\n",
    "    if args.local_rank == 0:\n",
    "        print(f'Loading model and optimizer state from {filepath}')\n",
    "    checkpoint = torch.load(filepath, map_location='cpu')\n",
    "    sd = {k.replace('module.', ''): v\n",
    "          for k, v in checkpoint['state_dict'].items()}\n",
    "    getattr(model, 'module', model).load_state_dict(sd)\n",
    "    return model\n",
    "\n",
    "def load_pretrained_fastpitch(args):\n",
    "    # load chkpt\n",
    "    device = torch.device('cuda' if args.cuda else 'cpu')\n",
    "    model_config = fastpitch_model.get_model_config('FastPitch', args)\n",
    "    fastpitch = fastpitch_model.get_model('FastPitch', model_config, device, forward_is_infer=True)\n",
    "    load_checkpoint(args, fastpitch, args.fastpitch_chkpt)\n",
    "    # get information about grapheme embedding table\n",
    "    n_symbols = fastpitch.encoder.word_emb.weight.size(0)\n",
    "    embedding_dim = fastpitch.encoder.word_emb.weight.size(1)\n",
    "    return fastpitch, model_config, n_symbols, embedding_dim\n",
    "\n",
    "# from fastpitch.fastpitch.transformer import FFTransformer\n",
    "fastpitch, model_config, n_symbols, embedding_dim = load_pretrained_fastpitch(args)\n",
    "print(\"Finished loading TTS model!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46e4101-61dc-4e18-8ab2-40ee587d2a64",
   "metadata": {},
   "source": [
    "## TTSMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8540b6e0-950c-4c46-ade2-07870b934c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TTSMetric:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path,\n",
    "    ):\n",
    "        self.tts_model = load_fastpitch(model_path)\n",
    "        self.softdtw_loss = criterion = SoftDTW(\n",
    "            use_cuda=torch.cuda.is_available(), \n",
    "            gamma=hparams[\"softdtw_temp\"], \n",
    "            bandwidth=hparams[\"softdtw_bandwidth\"],\n",
    "            dist_func=hparams[\"dist_func\"],\n",
    "        )\n",
    "    \n",
    "    def __call__(\n",
    "        self,\n",
    "        predicted_texts: List[str], # [bsz]\n",
    "        reference_mels: torch.Tensor, # [bsz, seqlen, dim]\n",
    "    ) -> float:\n",
    "        \"\"\"return softdtw loss between two batches of mel-spectrograms\n",
    "        averaged across batch dimension\"\"\"\n",
    "        predicted_mels = self.tts_model(predicted_texts)\n",
    "        return self.softdtw_loss(predicted_mels, reference_mels).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e306d2-b402-4974-977f-a5205ab7f3ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ttsmetric = TTSMetric(hparams[\"tts_model_path\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def187a5-028a-42a6-8684-b05975969dbd",
   "metadata": {},
   "source": [
    "## TTSRewardFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e002d148-d914-49c8-8569-de1e7e86134d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TTSRewardFunction:\n",
    "    \"\"\"TTS reward function\"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_path: str,\n",
    "        shaping_fn: str = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._metric = TTSMetric(model_path)\n",
    "        \n",
    "    def __call__(\n",
    "        self,\n",
    "        current_observation: Observation,\n",
    "        action: int,\n",
    "        next_observation: Observation,\n",
    "        done: bool,\n",
    "        meta_info: Dict[str, Any] = None,\n",
    "    ):\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcffa0fb-8585-42ba-a462-2fd42ce3af17",
   "metadata": {},
   "source": [
    "# ENVIRONMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1166f6f4-446f-4b6b-9960-28add60e8705",
   "metadata": {},
   "source": [
    "## create custom env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd7ecda-44d0-4d9c-8940-65c707b4916b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "\n",
    "class ASREnv(gym.Env):\n",
    "    \"\"\"Custom Environment that follows gym interface.\"\"\"\n",
    "    \n",
    "    # below taken from Gym code https://github.com/openai/gym/blob/master/gym/core.py\n",
    "    r\"\"\"The main OpenAI Gym class.\n",
    "    It encapsulates an environment with arbitrary behind-the-scenes dynamics.\n",
    "    An environment can be partially or fully observed.\n",
    "    The main API methods that users of this class need to know are:\n",
    "    - :meth:`step` - Takes a step in the environment using an action returning the next observation, reward,\n",
    "      if the environment terminated and observation information.\n",
    "    - :meth:`reset` - Resets the environment to an initial state, returning the initial observation and observation information.\n",
    "    - :meth:`render` - Renders the environment observation with modes depending on the output\n",
    "    - :meth:`close` - Closes the environment, important for rendering where pygame is imported\n",
    "    And set the following attributes:\n",
    "    - :attr:`action_space` - The Space object corresponding to valid actions\n",
    "    - :attr:`observation_space` - The Space object corresponding to valid observations\n",
    "    - :attr:`reward_range` - A tuple corresponding to the minimum and maximum possible rewards\n",
    "    - :attr:`spec` - An environment spec that contains the information used to initialise the environment from `gym.make`\n",
    "    - :attr:`metadata` - The metadata of the environment, i.e. render modes\n",
    "    - :attr:`np_random` - The random number generator for the environment\n",
    "    Note: a default reward range set to :math:`(-\\infty,+\\infty)` already exists. Set it if you want a narrower range.\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\"render.modes\": [\"human\"]}\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        tokenizer, \n",
    "        reward_function,\n",
    "        samples,\n",
    "        \n",
    "    ):\n",
    "        \"\"\"Generic RL environment to generate ASR hypotheses from input audio\"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self._vocab_size = tokenizer.vocab_size\n",
    "        self.reward_function = reward_function\n",
    "        for sample, weight in samples:\n",
    "            self.sampler_for_replaying.add(sample, weight)\n",
    "        \n",
    "        # Define action and observation space\n",
    "        # They must be gym.spaces objects\n",
    "        self.action_space = spaces.Discrete(n=self._vocab_size)\n",
    "        self.observation_space = DictSpace(\n",
    "            {\n",
    "                # we have to provide fixed sized inputs (padded) because sb3 support for DictObsersevation is limited\n",
    "                # while creating rollout buffers, observations are concatenated for each key\n",
    "                \"prompt_or_input_encoded_pt\": spaces.Box(\n",
    "                    low=0, high=self._vocab_size, shape=(self._max_text_length,)\n",
    "                ),\n",
    "                \"prompt_or_input_attention_mask_pt\": spaces.Box(\n",
    "                    low=0, high=1, shape=(self._max_text_length,)\n",
    "                ),\n",
    "                \"context_encoded_pt\": spaces.Box(\n",
    "                    low=0, high=self._vocab_size, shape=(self.max_steps,)\n",
    "                ),\n",
    "                \"context_attention_mask_pt\": spaces.Box(\n",
    "                    low=0, high=1, shape=(self.max_steps,)\n",
    "                ),\n",
    "                \"input_encoded_pt\": spaces.Box(\n",
    "                    low=0,\n",
    "                    high=self._vocab_size,\n",
    "                    shape=(self._max_text_length + self.max_steps,),\n",
    "                ),\n",
    "                \"input_attention_mask_pt\": spaces.Box(\n",
    "                    low=0, high=1, shape=(self._max_text_length + self.max_steps,)\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def step(self, action):\n",
    "        self.__time_step += 1\n",
    "\n",
    "        # previous obs\n",
    "        previous_obs = self.__current_obs\n",
    "\n",
    "        # just update the context tensor and gets the new observation\n",
    "        self.__current_obs = self.__current_obs.update(action, self.tokenizer)\n",
    "\n",
    "        # decide if the episode is finished or not\n",
    "        done = (action == self.tokenizer.eos_token_id and self._terminate_on_eos) or (\n",
    "            self.__time_step == self.max_steps\n",
    "        )\n",
    "\n",
    "        # compute reward\n",
    "        if not isinstance(self.reward_function, BatchedRewardFunction):\n",
    "            reward = (\n",
    "                None\n",
    "                if self.reward_function is None\n",
    "                else self.reward_function(\n",
    "                    previous_obs,\n",
    "                    action,\n",
    "                    self.__current_obs,\n",
    "                    done,\n",
    "                    self.__current_obs.meta_info,\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            reward = -inf  # will be overridden later\n",
    "\n",
    "        # populate additional info\n",
    "        info = {\n",
    "            \"output\": self.__current_obs.context_text,\n",
    "            \"action_history\": self.__current_obs.action_history,\n",
    "            \"reference_text\": self.__current_obs.target_or_reference_texts,\n",
    "            \"prompt_text\": self.__current_obs.prompt_or_input_text,\n",
    "            \"prev_output\": previous_obs.context_text,\n",
    "            \"meta_info\": previous_obs.meta_info,\n",
    "        }\n",
    "\n",
    "        dict_observation = self.__current_obs.to_dict()\n",
    "        return dict_observation, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the environment and starts a new episode\n",
    "        \"\"\"\n",
    "        # gets a new sample if not provided\n",
    "        if sample is None:\n",
    "            sample = self.sampler_for_replaying.sample(size=1)[0]\n",
    "        self.__current_sample = sample\n",
    "\n",
    "        # init the observation\n",
    "        self.__current_obs = Observation.init_from_sample(\n",
    "            sample,\n",
    "            self.tokenizer,\n",
    "            self._max_text_length,\n",
    "            self.max_steps,\n",
    "            self._prompt_truncation_side,\n",
    "            self._context_start_token,\n",
    "            sample.meta_data,\n",
    "        )\n",
    "\n",
    "        # start the time step counter\n",
    "        self.__time_step = 0\n",
    "\n",
    "        dict_observation = self.__current_obs.to_dict()\n",
    "        return dict_observation\n",
    "\n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6497b419-fadb-4e3e-a546-d4effe989983",
   "metadata": {
    "tags": []
   },
   "source": [
    "##Â check that env follows Gym interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525eacca-aed7-4ef9-b7dc-f9c3aa6719e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "env = CustomEnv(arg1, ...)\n",
    "# It will check your custom environment and output additional warnings if needed\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dece3d-9a3b-49dc-a5c3-a6edfaf1d366",
   "metadata": {},
   "source": [
    "# POLICY/ALGORITHM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12d79c3-b288-40b5-9d19-8d5c504ef21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Dict, List, Optional, Tuple, Type, Union\n",
    "\n",
    "from gym import spaces\n",
    "import torch as th\n",
    "from torch import nn\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "\n",
    "\n",
    "class PPO(OnPolicyAlgorithm):\n",
    "    \"\"\"\n",
    "    Created with reference to Seq2SeqLMActorCriticPolicy\n",
    "    \n",
    "    Custom network for policy and value function.\n",
    "    It receives as input the features extracted by the features extractor.\n",
    "\n",
    "    :param feature_dim: dimension of the features extracted with the features_extractor (e.g. features from a CNN)\n",
    "    :param last_layer_dim_pi: (int) number of units for the last layer of the policy network\n",
    "    :param last_layer_dim_vf: (int) number of units for the last layer of the value network\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_dim: int,\n",
    "        last_layer_dim_pi: int = 64,\n",
    "        last_layer_dim_vf: int = 64,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # IMPORTANT:\n",
    "        # Save output dimensions, used to create the distributions\n",
    "        self.latent_dim_pi = last_layer_dim_pi\n",
    "        self.latent_dim_vf = last_layer_dim_vf\n",
    "\n",
    "        # Policy network\n",
    "        self.policy_net = nn.Sequential(\n",
    "            nn.Linear(feature_dim, last_layer_dim_pi), nn.ReLU()\n",
    "        )\n",
    "        # Value network\n",
    "        self.value_net = nn.Sequential(\n",
    "            nn.Linear(feature_dim, last_layer_dim_vf), nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, features: th.Tensor) -> Tuple[th.Tensor, th.Tensor]:\n",
    "        \"\"\"\n",
    "        :return: (th.Tensor, th.Tensor) latent_policy, latent_value of the specified network.\n",
    "            If all layers are shared, then ``latent_policy == latent_value``\n",
    "        \"\"\"\n",
    "        return self.forward_actor(features), self.forward_critic(features)\n",
    "\n",
    "    def forward_actor(self, features: th.Tensor) -> th.Tensor:\n",
    "        return self.policy_net(features)\n",
    "\n",
    "    def forward_critic(self, features: th.Tensor) -> th.Tensor:\n",
    "        return self.value_net(features)\n",
    "\n",
    "\n",
    "class CustomActorCriticPolicy(ActorCriticPolicy):\n",
    "    def __init__(\n",
    "        self,\n",
    "        observation_space: spaces.Space,\n",
    "        action_space: spaces.Space,\n",
    "        lr_schedule: Callable[[float], float],\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        super().__init__(\n",
    "            observation_space,\n",
    "            action_space,\n",
    "            lr_schedule,\n",
    "            # Pass remaining arguments to base class\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )\n",
    "        # Disable orthogonal initialization\n",
    "        self.ortho_init = False\n",
    "\n",
    "    def _build_mlp_extractor(self) -> None:\n",
    "        self.mlp_extractor = CustomNetwork(self.features_dim)\n",
    "\n",
    "\n",
    "model = PPO(CustomActorCriticPolicy, \"CartPole-v1\", verbose=1)\n",
    "model.learn(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddd7e9d-1caf-4cba-b431-92a8e3e9bb51",
   "metadata": {},
   "source": [
    "# collect rollouts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1cb4a9-b229-46e6-9700-4b0bf71e434b",
   "metadata": {},
   "source": [
    "# create rollout buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdfc397-9e16-4f15-b1c5-b2571e45265a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "6a8422eeb13ba8f92f71047f64b5c33152e234c2bbad3f45433feda7b6f3b4c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
