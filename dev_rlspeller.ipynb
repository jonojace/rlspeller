{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4207e21b-d910-485b-8b9c-3fc7d5a59c5a",
   "metadata": {},
   "source": [
    "# Goal of this notebook\n",
    "\n",
    "Develop a training loop for finetuning ASR models using TTS loss by recreating RL training found in RL4LMs/rl4lms/envs/text_generation/training_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738aac35-fdd1-4452-aff0-92caa77ec5ab",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e877615-a6ff-4755-a7e4-e6f72e542432",
   "metadata": {},
   "source": [
    "# tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674722a6-d6ee-4feb-9f63-d8b8b893e619",
   "metadata": {},
   "source": [
    "# reward function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a387be4-9390-459d-92a5-2389ae883ed4",
   "metadata": {},
   "source": [
    "# datapool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcffa0fb-8585-42ba-a462-2fd42ce3af17",
   "metadata": {},
   "source": [
    "# environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1166f6f4-446f-4b6b-9960-28add60e8705",
   "metadata": {},
   "source": [
    "## create custom env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cd7ecda-44d0-4d9c-8940-65c707b4916b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "\n",
    "class ASREnv(gym.Env):\n",
    "    \"\"\"Custom Environment that follows gym interface.\"\"\"\n",
    "    \n",
    "    # below taken from Gym code https://github.com/openai/gym/blob/master/gym/core.py\n",
    "    r\"\"\"The main OpenAI Gym class.\n",
    "    It encapsulates an environment with arbitrary behind-the-scenes dynamics.\n",
    "    An environment can be partially or fully observed.\n",
    "    The main API methods that users of this class need to know are:\n",
    "    - :meth:`step` - Takes a step in the environment using an action returning the next observation, reward,\n",
    "      if the environment terminated and observation information.\n",
    "    - :meth:`reset` - Resets the environment to an initial state, returning the initial observation and observation information.\n",
    "    - :meth:`render` - Renders the environment observation with modes depending on the output\n",
    "    - :meth:`close` - Closes the environment, important for rendering where pygame is imported\n",
    "    And set the following attributes:\n",
    "    - :attr:`action_space` - The Space object corresponding to valid actions\n",
    "    - :attr:`observation_space` - The Space object corresponding to valid observations\n",
    "    - :attr:`reward_range` - A tuple corresponding to the minimum and maximum possible rewards\n",
    "    - :attr:`spec` - An environment spec that contains the information used to initialise the environment from `gym.make`\n",
    "    - :attr:`metadata` - The metadata of the environment, i.e. render modes\n",
    "    - :attr:`np_random` - The random number generator for the environment\n",
    "    Note: a default reward range set to :math:`(-\\infty,+\\infty)` already exists. Set it if you want a narrower range.\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\"render.modes\": [\"human\"]}\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        tokenizer, \n",
    "        reward_function,\n",
    "        samples,\n",
    "        \n",
    "    ):\n",
    "        \"\"\"Generic RL environment to generate ASR hypotheses from input audio\"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self._vocab_size = tokenizer.vocab_size\n",
    "        self.reward_function = reward_function\n",
    "        for sample, weight in samples:\n",
    "            self.sampler_for_replaying.add(sample, weight)\n",
    "        \n",
    "        # Define action and observation space\n",
    "        # They must be gym.spaces objects\n",
    "        self.action_space = spaces.Discrete(n=self._vocab_size)\n",
    "        self.observation_space = DictSpace(\n",
    "            {\n",
    "                # we have to provide fixed sized inputs (padded) because sb3 support for DictObsersevation is limited\n",
    "                # while creating rollout buffers, observations are concatenated for each key\n",
    "                \"prompt_or_input_encoded_pt\": spaces.Box(\n",
    "                    low=0, high=self._vocab_size, shape=(self._max_text_length,)\n",
    "                ),\n",
    "                \"prompt_or_input_attention_mask_pt\": spaces.Box(\n",
    "                    low=0, high=1, shape=(self._max_text_length,)\n",
    "                ),\n",
    "                \"context_encoded_pt\": spaces.Box(\n",
    "                    low=0, high=self._vocab_size, shape=(self.max_steps,)\n",
    "                ),\n",
    "                \"context_attention_mask_pt\": spaces.Box(\n",
    "                    low=0, high=1, shape=(self.max_steps,)\n",
    "                ),\n",
    "                \"input_encoded_pt\": spaces.Box(\n",
    "                    low=0,\n",
    "                    high=self._vocab_size,\n",
    "                    shape=(self._max_text_length + self.max_steps,),\n",
    "                ),\n",
    "                \"input_attention_mask_pt\": spaces.Box(\n",
    "                    low=0, high=1, shape=(self._max_text_length + self.max_steps,)\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def step(self, action):\n",
    "        self.__time_step += 1\n",
    "\n",
    "        # previous obs\n",
    "        previous_obs = self.__current_obs\n",
    "\n",
    "        # just update the context tensor and gets the new observation\n",
    "        self.__current_obs = self.__current_obs.update(action, self.tokenizer)\n",
    "\n",
    "        # decide if the episode is finished or not\n",
    "        done = (action == self.tokenizer.eos_token_id and self._terminate_on_eos) or (\n",
    "            self.__time_step == self.max_steps\n",
    "        )\n",
    "\n",
    "        # compute reward\n",
    "        if not isinstance(self.reward_function, BatchedRewardFunction):\n",
    "            reward = (\n",
    "                None\n",
    "                if self.reward_function is None\n",
    "                else self.reward_function(\n",
    "                    previous_obs,\n",
    "                    action,\n",
    "                    self.__current_obs,\n",
    "                    done,\n",
    "                    self.__current_obs.meta_info,\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            reward = -inf  # will be overridden later\n",
    "\n",
    "        # populate additional info\n",
    "        info = {\n",
    "            \"output\": self.__current_obs.context_text,\n",
    "            \"action_history\": self.__current_obs.action_history,\n",
    "            \"reference_text\": self.__current_obs.target_or_reference_texts,\n",
    "            \"prompt_text\": self.__current_obs.prompt_or_input_text,\n",
    "            \"prev_output\": previous_obs.context_text,\n",
    "            \"meta_info\": previous_obs.meta_info,\n",
    "        }\n",
    "\n",
    "        dict_observation = self.__current_obs.to_dict()\n",
    "        return dict_observation, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the environment and starts a new episode\n",
    "        \"\"\"\n",
    "        # gets a new sample if not provided\n",
    "        if sample is None:\n",
    "            sample = self.sampler_for_replaying.sample(size=1)[0]\n",
    "        self.__current_sample = sample\n",
    "\n",
    "        # init the observation\n",
    "        self.__current_obs = Observation.init_from_sample(\n",
    "            sample,\n",
    "            self.tokenizer,\n",
    "            self._max_text_length,\n",
    "            self.max_steps,\n",
    "            self._prompt_truncation_side,\n",
    "            self._context_start_token,\n",
    "            sample.meta_data,\n",
    "        )\n",
    "\n",
    "        # start the time step counter\n",
    "        self.__time_step = 0\n",
    "\n",
    "        dict_observation = self.__current_obs.to_dict()\n",
    "        return dict_observation\n",
    "\n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6497b419-fadb-4e3e-a546-d4effe989983",
   "metadata": {
    "tags": []
   },
   "source": [
    "##Â check that env follows Gym interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525eacca-aed7-4ef9-b7dc-f9c3aa6719e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "env = CustomEnv(arg1, ...)\n",
    "# It will check your custom environment and output additional warnings if needed\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dece3d-9a3b-49dc-a5c3-a6edfaf1d366",
   "metadata": {},
   "source": [
    "# policy algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12d79c3-b288-40b5-9d19-8d5c504ef21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Dict, List, Optional, Tuple, Type, Union\n",
    "\n",
    "from gym import spaces\n",
    "import torch as th\n",
    "from torch import nn\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "\n",
    "\n",
    "class PPO(OnPolicyAlgorithm):\n",
    "    \"\"\"\n",
    "    Created with reference to Seq2SeqLMActorCriticPolicy\n",
    "    \n",
    "    Custom network for policy and value function.\n",
    "    It receives as input the features extracted by the features extractor.\n",
    "\n",
    "    :param feature_dim: dimension of the features extracted with the features_extractor (e.g. features from a CNN)\n",
    "    :param last_layer_dim_pi: (int) number of units for the last layer of the policy network\n",
    "    :param last_layer_dim_vf: (int) number of units for the last layer of the value network\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_dim: int,\n",
    "        last_layer_dim_pi: int = 64,\n",
    "        last_layer_dim_vf: int = 64,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # IMPORTANT:\n",
    "        # Save output dimensions, used to create the distributions\n",
    "        self.latent_dim_pi = last_layer_dim_pi\n",
    "        self.latent_dim_vf = last_layer_dim_vf\n",
    "\n",
    "        # Policy network\n",
    "        self.policy_net = nn.Sequential(\n",
    "            nn.Linear(feature_dim, last_layer_dim_pi), nn.ReLU()\n",
    "        )\n",
    "        # Value network\n",
    "        self.value_net = nn.Sequential(\n",
    "            nn.Linear(feature_dim, last_layer_dim_vf), nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, features: th.Tensor) -> Tuple[th.Tensor, th.Tensor]:\n",
    "        \"\"\"\n",
    "        :return: (th.Tensor, th.Tensor) latent_policy, latent_value of the specified network.\n",
    "            If all layers are shared, then ``latent_policy == latent_value``\n",
    "        \"\"\"\n",
    "        return self.forward_actor(features), self.forward_critic(features)\n",
    "\n",
    "    def forward_actor(self, features: th.Tensor) -> th.Tensor:\n",
    "        return self.policy_net(features)\n",
    "\n",
    "    def forward_critic(self, features: th.Tensor) -> th.Tensor:\n",
    "        return self.value_net(features)\n",
    "\n",
    "\n",
    "class CustomActorCriticPolicy(ActorCriticPolicy):\n",
    "    def __init__(\n",
    "        self,\n",
    "        observation_space: spaces.Space,\n",
    "        action_space: spaces.Space,\n",
    "        lr_schedule: Callable[[float], float],\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        super().__init__(\n",
    "            observation_space,\n",
    "            action_space,\n",
    "            lr_schedule,\n",
    "            # Pass remaining arguments to base class\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )\n",
    "        # Disable orthogonal initialization\n",
    "        self.ortho_init = False\n",
    "\n",
    "    def _build_mlp_extractor(self) -> None:\n",
    "        self.mlp_extractor = CustomNetwork(self.features_dim)\n",
    "\n",
    "\n",
    "model = PPO(CustomActorCriticPolicy, \"CartPole-v1\", verbose=1)\n",
    "model.learn(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddd7e9d-1caf-4cba-b431-92a8e3e9bb51",
   "metadata": {},
   "source": [
    "# collect rollouts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1cb4a9-b229-46e6-9700-4b0bf71e434b",
   "metadata": {},
   "source": [
    "# create rollout buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdfc397-9e16-4f15-b1c5-b2571e45265a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
