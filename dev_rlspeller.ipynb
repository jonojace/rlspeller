{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4207e21b-d910-485b-8b9c-3fc7d5a59c5a",
   "metadata": {},
   "source": [
    "# Goal of this notebook\n",
    "\n",
    "Develop a training loop for finetuning ASR models using TTS loss by recreating RL training found in RL4LMs/rl4lms/envs/text_generation/training_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70687ad-9f13-4cdf-8ef7-07eecb34f496",
   "metadata": {},
   "source": [
    "# automatic reloading magic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738aac35-fdd1-4452-aff0-92caa77ec5ab",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53032f3b",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strickland.inf.ed.ac.uk\n"
     ]
    }
   ],
   "source": [
    "# print hostname to make sure we are on correct node\n",
    "import socket\n",
    "print(socket.gethostname())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6f67235",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/disk/nfs/ostrom/s1785140/rlspeller'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c8a89c2-38ea-4778-8028-0c7854152301",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import List, Dict, Tuple, Any\n",
    "import hyperpyyaml\n",
    "from tqdm import tqdm\n",
    "from torchaudio.models.decoder import ctc_decoder\n",
    "from torch.nn.functional import softmax\n",
    "import random\n",
    "from jiwer import cer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f67b1b68",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "723e84d2",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import speechbrain as sb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0221e93b-576c-4781-913f-07fe6e657d7a",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b236619f-6c04-44b1-a697-a342417a6694",
   "metadata": {},
   "source": [
    "# HPARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8a11879-640c-433e-a192-9c8b551d8ca2",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    \"softdtw_temp\": 0.01,\n",
    "    \"softdtw_bandwidth\": 120,\n",
    "    \"dist_func\": \"l1\",\n",
    "    \"sentencepiece_model_path\": \"/home/s1785140/speechbrain/templates/speech_recognition_CharTokens_NoLM/Tokenizer/save/0_char.model\",\n",
    "    'speechbrain_hparams_file': '/home/s1785140/rlspeller/infer_speechbrain.yaml',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e877615-a6ff-4755-a7e4-e6f72e542432",
   "metadata": {},
   "source": [
    "# TOKENIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0505c38d-bc92-4f84-9f1f-cfc942ad337d",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "# load pretrained tokenizer used to tokenizer ASR training inputs \n",
    "import sentencepiece as spm \n",
    "spm_path = hparams[\"sentencepiece_model_path\"]\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(spm_path)\n",
    "print(sp.vocab_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4af21be-9224-4c45-87f6-b86a8d7c63ab",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 10 2 12 12 4 1 17 4 9 12 11 1 16 20 1 6 5 16 2 1 7 8 1 26 5 8 4 6\n"
     ]
    }
   ],
   "source": [
    "# test tokenizer\n",
    "s = \"hello world my name is jason\"\n",
    "# TODO pass string through text cleaners? \n",
    "encoded = sp.EncodeAsIds(s)\n",
    "assert 0 not in encoded, \"tried to encode an unknown character\"\n",
    "print(\" \".join(str(idx) for idx in encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe3c6048-79c5-47b4-9524-88d2e36ad8d3",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello world my name is jason'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.DecodeIds(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2996d1f1-d331-4919-8742-7907b46a46ce",
   "metadata": {},
   "source": [
    "# NEW! SIMPLE TOKENIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8951a84-e22e-4c62-a169-403a1b5269af",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from speechbrain.tokenizers.SimpleTokenizer import SimpleTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfec85a8-d20d-445e-8483-cf27092e2962",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = SimpleTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c52e4b5-8d26-44c3-a1ad-c772b6feac62",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello|my|name|is|jason\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[9, 6, 13, 13, 16, 1, 14, 26, 1, 15, 2, 14, 6, 1, 10, 20, 1, 11, 2, 20, 16, 15]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"hello my name is jason\"\n",
    "text = text.replace(' ', '|')\n",
    "print(text)\n",
    "ids = tokenizer.encode_as_ids(text)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5f1bf5e-500b-47ce-986c-3cbe059cdf41",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello|my|name|is|jason'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode_ids(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5ea5f7-6224-4089-8136-5c4b8d47a2e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## test simple tokenizer with probability distribution, and see if CTC decoder successfully generates n-best lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a60ffb1b-6770-466c-90f6-6daa1dbe461b",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# create empty array of correct dimensions\n",
    "min_len, max_len = 50, 100\n",
    "bsz = 4\n",
    "lens = torch.randint(min_len, max_len, (bsz,))\n",
    "vocab_size = len(tokenizer.vocab)\n",
    "\n",
    "# randomly assign probaility distribution to each timestep\n",
    "\n",
    "# try to decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cac82712-8559-44d0-86d9-66cf1ddfbd5c",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "randn = torch.randn(bsz, max_len, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26e7e51c-bab1-4f2d-bb12-34f9f9c4454c",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "ctc_probs = softmax(randn, dim=1)\n",
    "# ctc_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f9abdc6-bd05-4c5c-a091-d04dadbaa9e5",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('sample 1, hyp 1/2', ['|ydjno|dumnf|ramucrtgolkjfsyipquyvuctvghwnbctnijveywrzgzy|'])\n",
      "('sample 1, hyp 2/2', ['|ydjno|dumnf|ramucrtgolk|jfsyipquyvuctvghwnbctnijveywrzgzy|'])\n",
      "('sample 2, hyp 1/2', ['|yiewfjhwqouhdvmewtrlekncpkceorxdsonvqhnspgcgac|gaxm|'])\n",
      "('sample 2, hyp 2/2', ['|yiewfjhwqouhdvmewtrlekncpkceorxdsonvqhnspcgac|gaxm|'])\n",
      "('sample 3, hyp 1/2', ['|plmqd|ublcbwdeodn|bhpfptegqwleomvsudveyarodysrt|ta|i|'])\n",
      "('sample 3, hyp 2/2', ['|plmqd|ublcbweodn|bhpfptegqwleomvsudveyarodysrt|ta|i|'])\n",
      "('sample 4, hyp 1/2', ['|lhpbsmrzrcpnjmnwpvrxepqwuamizxafkmvmluinhlifbsnbkqyrqnuyq|lfuqhkcvczmnvqgoiyzbrtzids|'])\n",
      "('sample 4, hyp 2/2', ['|lhpbsmrzrcpnjmnwpvrxepqwuamizxafkmvmluinhlifbsnbkqyrqnuyq|lfuqhkcvczmnvqpoiyzbrtzids|'])\n"
     ]
    }
   ],
   "source": [
    "ctc_beamsearch_decoder_test = ctc_decoder(\n",
    "    lexicon=None,\n",
    "    # tokens=\"/home/s1785140/rlspeller/templates/speech_recognition_CharTokens_NoLM/Tokenizer/save/tokens.txt\",\n",
    "    tokens=tokenizer.vocab,\n",
    "    nbest=2,\n",
    "    blank_token='-',\n",
    "    sil_token=\"|\",\n",
    ")\n",
    "\n",
    "predicted_ids = ctc_beamsearch_decoder_test(ctc_probs, lens)\n",
    "\n",
    "predicted_words = []\n",
    "for i, hyps in enumerate(predicted_ids):\n",
    "    for j, hyp in enumerate(hyps):\n",
    "        words = tokenizer.decode_ids(hyp.tokens.tolist()).split(\" \")\n",
    "        tup = (f\"sample {i+1}, hyp {j+1}/{len(hyps)}\", words)\n",
    "        predicted_words.append(tup)\n",
    "        print(tup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd650fd4",
   "metadata": {},
   "source": [
    "# LOAD ASR (PRETRAINED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56bf160c",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from templates.speech_recognition_CharTokens_NoLM.ASR.train import ASR\n",
    "from templates.speech_recognition_CharTokens_NoLM.ASR.train import dataio_prepare\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08e81b55",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from speechbrain.dataio.dataloader import LoopedLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36d8019b",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/s1785140/speechbrain/templates/speech_recognition_CharTokens_NoLM/data/rirs_noises.zip exists. Skipping download\n"
     ]
    }
   ],
   "source": [
    "# Load hyperparameters file with command-line overrides\n",
    "speechbrain_hparams_file = hparams['speechbrain_hparams_file']\n",
    "with open(speechbrain_hparams_file) as f:\n",
    "    speechbrain_hparams = hyperpyyaml.load_hyperpyyaml(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26c15de1-25d9-428e-900f-967c3af9a8ad",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/s1785140/speechbrain/templates/speech_recognition_CharTokens_NoLM/ASR/results/CRDNN_CHAR_LJSpeech_halved/2602/save'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speechbrain_hparams['save_folder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4072195-fb2b-4c9a-b201-c84b7a7ae104",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if on_evaluate_start() get runtime error, likely need to restart notebook kernel\n"
     ]
    }
   ],
   "source": [
    "# initialise trainer (we don't want to train, but model is tightly coupled with trainer)\n",
    "asr_brain = ASR(\n",
    "    modules=speechbrain_hparams[\"modules\"],\n",
    "    opt_class=speechbrain_hparams[\"opt_class\"],\n",
    "    hparams=speechbrain_hparams,\n",
    "    checkpointer=speechbrain_hparams[\"checkpointer\"],\n",
    ")\n",
    "\n",
    "def setup_asr_brain_for_infer(asr_brain):\n",
    "    asr_brain.on_evaluate_start(min_key=\"WER\") # We call the on_evaluate_start that will load the best model\n",
    "    asr_brain.modules.eval() # We set the model to eval mode (remove dropout etc)\n",
    "\n",
    "print(\"if on_evaluate_start() get runtime error, likely need to restart notebook kernel\")\n",
    "setup_asr_brain_for_infer(asr_brain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "909b3c39-3a03-4384-82ca-2305b3ed2272",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# create dataset and dataloader for inference\n",
    "datasets = dataio_prepare(speechbrain_hparams)\n",
    "\n",
    "test_set = datasets['test']\n",
    "\n",
    "if not isinstance(test_set, DataLoader) or isinstance(test_set, LoopedLoader):\n",
    "    test_loader_kwargs=speechbrain_hparams[\"test_dataloader_opts\"]\n",
    "    test_set = asr_brain.make_dataloader(\n",
    "        test_set, stage=sb.Stage.TEST, **test_loader_kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "69b4b4b1",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ⁇ ', '', 'e', 't', 'o', 'a', 'n', 'i', 's', 'r', 'h', 'd', 'l', 'c', 'f', 'u', 'm', 'w', 'p', 'g', 'y', 'b', 'v', 'k', 'x', 'q', 'j', 'z']\n",
      "['-', '|', 'e', 't', 'o', 'a', 'n', 'i', 's', 'r', 'h', 'd', 'l', 'c', 'f', 'u', 'm', 'w', 'p', 'g', 'y', 'b', 'v', 'k', 'x', 'q', 'j', 'z']\n"
     ]
    }
   ],
   "source": [
    "# get vocab from tokenizer (needed for ctc decoding)\n",
    "vocab_size = len(asr_brain.hparams.tokenizer)\n",
    "vocab = []\n",
    "for i in range(vocab_size):\n",
    "    vocab.append(asr_brain.hparams.tokenizer.decode_ids([i]))\n",
    "print(vocab)\n",
    "\n",
    "# edit vocab to match default ctc decoder symbols for blank and silence\n",
    "vocab[0] = '-'\n",
    "vocab[1] = \"|\"\n",
    "\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6af49d3e",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "ctc_beamsearch_decoder = ctc_decoder(\n",
    "    lexicon=None,\n",
    "    # tokens=\"/home/s1785140/rlspeller/templates/speech_recognition_CharTokens_NoLM/Tokenizer/save/tokens.txt\",\n",
    "    tokens=vocab,\n",
    "    nbest=100,\n",
    "    blank_token='-',\n",
    "    sil_token=\"|\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b4d6501c",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                        | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG INSIDE PREPARE FEATURES, feats.shape=torch.Size([8, 627, 40]) wav_lens.shape=torch.Size([8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 1/1 [00:08<00:00,  8.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sample 1 - (LJ039-0175: 'for the first four attempts the firers missed the second shot by several inches')\n",
      "\thyp 1/48 (CER=0.0%): 'for the first four attempts the firers missed the second shot by several inches '\n",
      "\thyp 2/48 (CER=1.3%): 'for the first four attempts the firers mised the second shot by several inches '\n",
      "\thyp 3/48 (CER=1.3%): 'for the fist four attempts the firers missed the second shot by several inches '\n",
      "\thyp 4/48 (CER=1.3%): 'for the first four attempths the firers missed the second shot by several inches '\n",
      "\thyp 5/48 (CER=2.5%): 'for the first four attempts the firerers missed the second shot by several inches '\n",
      "\thyp 6/48 (CER=2.5%): 'for the fist four attempts the firers mised the second shot by several inches '\n",
      "\thyp 7/48 (CER=1.3%): 'fr the first four attempts the firers missed the second shot by several inches '\n",
      "\thyp 8/48 (CER=1.3%): 'for the first four attempts the firers  missed the second shot by several inches '\n",
      "\thyp 9/48 (CER=1.3%): 'for the first four atempts the firers missed the second shot by several inches '\n",
      "\thyp 10/48 (CER=1.3%): 'for the first four attempts the firers missed the second shot by several nches '\n",
      "\thyp 11/48 (CER=1.3%): 'for the first four attemts the firers missed the second shot by several inches '\n",
      "\thyp 12/48 (CER=2.5%): 'for the first four attempths the firers mised the second shot by several inches '\n",
      "\thyp 13/48 (CER=1.3%): 'for the first four attempts the firers misse the second shot by several inches '\n",
      "\thyp 14/48 (CER=1.3%): 'for the first four attempts the firers missed the second shot by several inchies '\n",
      "\thyp 15/48 (CER=1.3%): 'for the first four attempts the firers missed the secod shot by several inches '\n",
      "\thyp 16/48 (CER=2.5%): 'for the first four attempts the firers m missed the second shot by several inches '\n",
      "\thyp 17/48 (CER=1.3%): 'for the first four attempts the firers missed the second schot by several inches '\n",
      "\thyp 18/48 (CER=1.3%): 'for the first four attempts the firers nmissed the second shot by several inches '\n",
      "\thyp 19/48 (CER=2.5%): 'for the first four attempts the firers a missed the second shot by several inches '\n",
      "\thyp 20/48 (CER=2.5%): 'for the first four attempts the firers h missed the second shot by several inches '\n",
      "\thyp 21/48 (CER=1.3%): 'for the first four attemmpts the firers missed the second shot by several inches '\n",
      "\thyp 22/48 (CER=2.5%): 'for the first four attempts the firers n missed the second shot by several inches '\n",
      "\thyp 23/48 (CER=1.3%): 'for the first four attempts the firers missed the second shot by several inces '\n",
      "\thyp 24/48 (CER=1.3%): 'for the first four attemnpts the firers missed the second shot by several inches '\n",
      "\thyp 25/48 (CER=3.8%): 'for the first four attempts the firerers mised the second shot by several inches '\n",
      "\thyp 26/48 (CER=1.3%): 'for the first four attempts the firers missed the sepcond shot by several inches '\n",
      "\thyp 27/48 (CER=2.5%): 'fr the first four attempts the firers mised the second shot by several inches '\n",
      "\thyp 28/48 (CER=1.3%): 'for the first four attemptes the firers missed the second shot by several inches '\n",
      "\thyp 29/48 (CER=2.5%): 'for the first four attempts the firers d missed the second shot by several inches '\n",
      "\thyp 30/48 (CER=1.3%): 'for the first four attempts the firers missed the second shot by several einches '\n",
      "\thyp 31/48 (CER=2.5%): 'for the first four attempts the firers mised the second shot by several nches '\n",
      "\thyp 32/48 (CER=2.5%): 'for the first four attempts the firers missed the second shot by several incheies '\n",
      "\thyp 33/48 (CER=2.5%): 'for the first four attempts the firers mised the second shot by several inchies '\n",
      "\thyp 34/48 (CER=1.3%): 'for the first four attempts the firers missed the second shot by several inchaes '\n",
      "\thyp 35/48 (CER=1.3%): 'for the first four attempts the firers missed the second shot by several inchees '\n",
      "\thyp 36/48 (CER=1.3%): 'for the first four attempts the firers missed the second shot by several inchess '\n",
      "\thyp 37/48 (CER=1.3%): 'for the first four attempts the firers missed the second shot by several inchoes '\n",
      "\thyp 38/48 (CER=3.8%): 'for the first four attempts the firers mised the second shot by several incheies '\n",
      "\thyp 39/48 (CER=2.5%): 'for the first four attempts the firers mised the second shot by several inchess '\n",
      "\thyp 40/48 (CER=0.0%): 'for the first four attempts the firers missed the second shot by several inches '\n",
      "\thyp 41/48 (CER=1.3%): 'for the first four attempts the firers mised the second shot by several inches '\n",
      "\thyp 42/48 (CER=1.3%): 'for the first four attempts the firers missed the second shot by several inchesf '\n",
      "\thyp 43/48 (CER=2.5%): 'for the first four attempts the firers mised the second shot by several inchesf '\n",
      "\thyp 44/48 (CER=1.3%): 'for the fist four attempts the firers missed the second shot by several inches '\n",
      "\thyp 45/48 (CER=1.3%): 'for the first four attempts the firers missed the second shot by several inchesm '\n",
      "\thyp 46/48 (CER=1.3%): 'for the first four attempths the firers missed the second shot by several inches '\n",
      "\thyp 47/48 (CER=2.5%): 'for the first four attempts the firerers missed the second shot by several inches '\n",
      "\thyp 48/48 (CER=2.5%): 'for the fist four attempts the firers mised the second shot by several inches '\n",
      "\t=== Mean CER: 1.7%, Std CER: 0.8% ===\n",
      "\n",
      "sample 2 - (LJ046-0118: 'in addition to this function prs is responsible for such tasks')\n",
      "\thyp 1/49 (CER=0.0%): 'in addition to this function prs is responsible for such tasks '\n",
      "\thyp 2/49 (CER=1.6%): 'in addition to this function prss is responsible for such tasks '\n",
      "\thyp 3/49 (CER=1.6%): 'in additio to this function prs is responsible for such tasks '\n",
      "\thyp 4/49 (CER=3.2%): 'in addition to this function prs is responsible for such tas '\n",
      "\thyp 5/49 (CER=1.6%): 'in additin to this function prs is responsible for such tasks '\n",
      "\thyp 6/49 (CER=1.6%): 'in addition to this function prs is responsible for such taskts '\n",
      "\thyp 7/49 (CER=3.2%): 'in additio to this function prss is responsible for such tasks '\n",
      "\thyp 8/49 (CER=1.6%): 'in addition to this functieon prs is responsible for such tasks '\n",
      "\thyp 9/49 (CER=4.8%): 'in addition to this function prss is responsible for such tas '\n",
      "\thyp 10/49 (CER=4.8%): 'in additio to this function prs is responsible for such tas '\n",
      "\thyp 11/49 (CER=3.2%): 'in additin to this function prss is responsible for such tasks '\n",
      "\thyp 12/49 (CER=3.2%): 'in addition to this function prss is responsible for such taskts '\n",
      "\thyp 13/49 (CER=3.2%): 'in additio to this function prs is responsible for such taskts '\n",
      "\thyp 14/49 (CER=1.6%): 'in addition to this function pr is responsible for such tasks '\n",
      "\thyp 15/49 (CER=1.6%): 'in addition to this function prs is responsible fo such tasks '\n",
      "\thyp 16/49 (CER=1.6%): 'in addition to tis function prs is responsible for such tasks '\n",
      "\thyp 17/49 (CER=1.6%): 'in addition to this function prs is responsible for such tasts '\n",
      "\thyp 18/49 (CER=1.6%): 'in addition to this function prs is responsible for such taesks '\n",
      "\thyp 19/49 (CER=4.8%): 'in additin to this function prs is responsible for such tas '\n",
      "\thyp 20/49 (CER=1.6%): 'in addition to this function prs is responsible for such teasks '\n",
      "\thyp 21/49 (CER=3.2%): 'in addition to this functieon prss is responsible for such tasks '\n",
      "\thyp 22/49 (CER=3.2%): 'in additio to this functieon prs is responsible for such tasks '\n",
      "\thyp 23/49 (CER=1.6%): 'in addition to this function prs is responsible for such tastks '\n",
      "\thyp 24/49 (CER=6.5%): 'in additio to this function prss is responsible for such tas '\n",
      "\thyp 25/49 (CER=1.6%): 'in additioan to this function prs is responsible for such tasks '\n",
      "\thyp 26/49 (CER=3.2%): 'in additin to this function prs is responsible for such taskts '\n",
      "\thyp 27/49 (CER=4.8%): 'in addition to this functieon prs is responsible for such tas '\n",
      "\thyp 28/49 (CER=1.6%): 'in addition to this function prs is responsible for such tass '\n",
      "\thyp 29/49 (CER=1.6%): 'in addition to this function prs is responsibe for such tasks '\n",
      "\thyp 30/49 (CER=1.6%): 'in addition to this function prs is responsible for such taskss '\n",
      "\thyp 31/49 (CER=1.6%): 'in addition to this function prs is responsible for such taskst '\n",
      "\thyp 32/49 (CER=3.2%): 'in addition to this function prss is responsible for such taskss '\n",
      "\thyp 33/49 (CER=3.2%): 'in additio to this function prs is responsible for such taskss '\n",
      "\thyp 34/49 (CER=3.2%): 'in additin to this function prs is responsible for such taskss '\n",
      "\thyp 35/49 (CER=3.2%): 'in addition to this function prs is responsible for such tasktss '\n",
      "\thyp 36/49 (CER=3.2%): 'in addition to this function prss is responsible for such taskst '\n",
      "\thyp 37/49 (CER=3.2%): 'in additio to this function prs is responsible for such taskst '\n",
      "\thyp 38/49 (CER=4.8%): 'in additio to this function prss is responsible for such taskss '\n",
      "\thyp 39/49 (CER=3.2%): 'in addition to this functieon prs is responsible for such taskss '\n",
      "\thyp 40/49 (CER=3.2%): 'in addition to this function prs is responsible for such tast '\n",
      "\thyp 41/49 (CER=3.2%): 'in addition to this function prss is responsible for such tass '\n",
      "\thyp 42/49 (CER=3.2%): 'in additio to this function prs is responsible for such tass '\n",
      "\thyp 43/49 (CER=3.2%): 'in additin to this function prs is responsible for such taskst '\n",
      "\thyp 44/49 (CER=4.8%): 'in additin to this function prss is responsible for such taskss '\n",
      "\thyp 45/49 (CER=1.6%): 'in addition to this function prs is responsible for such tasksn '\n",
      "\thyp 46/49 (CER=3.2%): 'in addition to this function prs is responsible for such tasktst '\n",
      "\thyp 47/49 (CER=4.8%): 'in addition to this function prss is responsible for such tasktss '\n",
      "\thyp 48/49 (CER=4.8%): 'in additio to this function prs is responsible for such tasktss '\n",
      "\thyp 49/49 (CER=4.8%): 'in additio to this function prss is responsible for such taskst '\n",
      "\t=== Mean CER: 2.9%, Std CER: 1.3% ===\n",
      "\n",
      "sample 3 - (LJ049-0095: 'for all offenses within its jurisdiction as are fbi agents and federal marshals')\n",
      "\thyp 1/50 (CER=1.3%): 'for all offenses within its jurisdiction as are fbi agents and federal marfhals '\n",
      "\thyp 2/50 (CER=2.5%): 'for all ofenses within its jurisdiction as are fbi agents and federal marfhals '\n",
      "\thyp 3/50 (CER=2.5%): 'for all offences within its jurisdiction as are fbi agents and federal marfhals '\n",
      "\thyp 4/50 (CER=1.3%): 'for all offenses within its jurisdiction as are fbi agents and federal marhals '\n",
      "\thyp 5/50 (CER=2.5%): 'for all offenses within its jurisdiction as are fbi agents and federal martfhals '\n",
      "\thyp 6/50 (CER=1.3%): 'for all offenses within its jurisdiction as are fbi agents and federal marsfhals '\n",
      "\thyp 7/50 (CER=3.8%): 'for all ofences within its jurisdiction as are fbi agents and federal marfhals '\n",
      "\thyp 8/50 (CER=2.5%): 'for all ofenses within its jurisdiction as are fbi agents and federal marhals '\n",
      "\thyp 9/50 (CER=3.8%): 'for all ofenses within its jurisdiction as are fbi agents and federal martfhals '\n",
      "\thyp 10/50 (CER=2.5%): 'for all offences within its jurisdiction as are fbi agents and federal marhals '\n",
      "\thyp 11/50 (CER=2.5%): 'for all offenses within its jurisdition as are fbi agents and federal marfhals '\n",
      "\thyp 12/50 (CER=3.8%): 'for all offences within its jurisdiction as are fbi agents and federal martfhals '\n",
      "\thyp 13/50 (CER=1.3%): 'for all offenses within its jurisdiction as are fbi agents and federal marthals '\n",
      "\thyp 14/50 (CER=2.5%): 'for all offencses within its jurisdiction as are fbi agents and federal marfhals '\n",
      "\thyp 15/50 (CER=2.5%): 'for all ofenses within its jurisdiction as are fbi agents and federal marsfhals '\n",
      "\thyp 16/50 (CER=1.3%): 'for all offenses within its jurisdiction as are fbi agents and federal marchals '\n",
      "\thyp 17/50 (CER=2.5%): 'for all offenses with in its jurisdiction as are fbi agents and federal marfhals '\n",
      "\thyp 18/50 (CER=2.5%): 'for all offences within its jurisdiction as are fbi agents and federal marsfhals '\n",
      "\thyp 19/50 (CER=0.0%): 'for all offenses within its jurisdiction as are fbi agents and federal marshals '\n",
      "\thyp 20/50 (CER=3.8%): 'for all ofences within its jurisdiction as are fbi agents and federal marhals '\n",
      "\thyp 21/50 (CER=2.5%): 'for all offenses within its jurisdiction as are fbi agents and federal marfials '\n",
      "\thyp 22/50 (CER=2.5%): 'for all offenses within its jurisdicion as are fbi agents and federal marfhals '\n",
      "\thyp 23/50 (CER=3.8%): 'for all ofenses within its jurisdition as are fbi agents and federal marfhals '\n",
      "\thyp 24/50 (CER=2.5%): 'for al offenses within its jurisdiction as are fbi agents and federal marfhals '\n",
      "\thyp 25/50 (CER=5.1%): 'for all ofences within its jurisdiction as are fbi agents and federal martfhals '\n",
      "\thyp 26/50 (CER=2.5%): 'for all ofenses within its jurisdiction as are fbi agents and federal marthals '\n",
      "\thyp 27/50 (CER=3.8%): 'for all ofencses within its jurisdiction as are fbi agents and federal marfhals '\n",
      "\thyp 28/50 (CER=3.8%): 'for all offences within its jurisdition as are fbi agents and federal marfhals '\n",
      "\thyp 29/50 (CER=2.5%): 'for all offenses within its jurisdition as are fbi agents and federal marhals '\n",
      "\thyp 30/50 (CER=2.5%): 'for all offences within its jurisdiction as are fbi agents and federal marthals '\n",
      "\thyp 31/50 (CER=2.5%): 'for all offenses within its jurisdiction as are fbi agents and federal marcfhals '\n",
      "\thyp 32/50 (CER=2.5%): 'for all offencses within its jurisdiction as are fbi agents and federal marhals '\n",
      "\thyp 33/50 (CER=2.5%): 'for all ofenses within its jurisdiction as are fbi agents and federal marchals '\n",
      "\thyp 34/50 (CER=3.8%): 'for all offenses within its jurisdition as are fbi agents and federal martfhals '\n",
      "\thyp 35/50 (CER=2.5%): 'for all offenses within its jurisdiction as are fbi agents and federal marfhalss '\n",
      "\thyp 36/50 (CER=3.8%): 'for all ofenses within its jurisdiction as are fbi agents and federal marfhalss '\n",
      "\thyp 37/50 (CER=2.5%): 'for all offenses within its jurisdiction as are fbi agents and federal marfhalst '\n",
      "\thyp 38/50 (CER=3.8%): 'for all offences within its jurisdiction as are fbi agents and federal marfhalss '\n",
      "\thyp 39/50 (CER=2.5%): 'for all offenses within its jurisdiction as are fbi agents and federal marhalss '\n",
      "\thyp 40/50 (CER=3.8%): 'for all offenses within its jurisdiction as are fbi agents and federal martfhalss '\n",
      "\thyp 41/50 (CER=2.5%): 'for all offenses within its jurisdiction as are fbi agents and federal marsfhalss '\n",
      "\thyp 42/50 (CER=3.8%): 'for all ofenses within its jurisdiction as are fbi agents and federal marfhalst '\n",
      "\thyp 43/50 (CER=5.1%): 'for all ofences within its jurisdiction as are fbi agents and federal marfhalss '\n",
      "\thyp 44/50 (CER=3.8%): 'for all ofenses within its jurisdiction as are fbi agents and federal marhalss '\n",
      "\thyp 45/50 (CER=2.5%): 'for all offenses within its jurisdiction as are fbi agents and federal marfhalsc '\n",
      "\thyp 46/50 (CER=5.1%): 'for all ofenses within its jurisdiction as are fbi agents and federal martfhalss '\n",
      "\thyp 47/50 (CER=3.8%): 'for all offences within its jurisdiction as are fbi agents and federal marfhalst '\n",
      "\thyp 48/50 (CER=2.5%): 'for all offenses within its jurisdiction as are fbi agents and federal marhalst '\n",
      "\thyp 49/50 (CER=3.8%): 'for all offences within its jurisdiction as are fbi agents and federal marhalss '\n",
      "\thyp 50/50 (CER=3.8%): 'for all offenses within its jurisdition as are fbi agents and federal marfhalss '\n",
      "\t=== Mean CER: 2.9%, Std CER: 1.0% ===\n",
      "\n",
      "sample 4 - (LJ042-0234: 'quote except in the us the living standard is a little higher')\n",
      "\thyp 1/50 (CER=3.3%): 'quote except in the u s the living standard is a lidtle higher '\n",
      "\thyp 2/50 (CER=4.9%): 'quote except in the u s the living standard is a lidtle highere '\n",
      "\thyp 3/50 (CER=4.9%): 'quote except in the u s the living standard is a lidle higher '\n",
      "\thyp 4/50 (CER=6.6%): 'quote except in the u s the living standard is a lidle highere '\n",
      "\thyp 5/50 (CER=1.6%): 'quote except in the u s the living standard is a little higher '\n",
      "\thyp 6/50 (CER=3.3%): 'quote except in the u s the living standard is a little highere '\n",
      "\thyp 7/50 (CER=4.9%): 'quote except in the u s the living standard is a lidtle highr '\n",
      "\thyp 8/50 (CER=6.6%): 'quote except in the u s the living standard is a lidtle highre '\n",
      "\thyp 9/50 (CER=3.3%): 'quote except in the u s the living standard is a litle higher '\n",
      "\thyp 10/50 (CER=4.9%): 'quote except in the u s the living standard is a litle highere '\n",
      "\thyp 11/50 (CER=4.9%): 'quote except in the u s the living standard is a lidtle highe '\n",
      "\thyp 12/50 (CER=6.6%): 'quote except in the u s the living standard is a lidle highr '\n",
      "\thyp 13/50 (CER=8.2%): 'quote except in the u s the living standard is a lidle highre '\n",
      "\thyp 14/50 (CER=4.9%): 'quote except in the u s the living standard is a lidtl higher '\n",
      "\thyp 15/50 (CER=6.6%): 'quote except in the u s the living standard is a lidtl highere '\n",
      "\thyp 16/50 (CER=6.6%): 'quote except in the u s the living standard is a lidle highe '\n",
      "\thyp 17/50 (CER=3.3%): 'quote except in the u s the living standard is a little highr '\n",
      "\thyp 18/50 (CER=4.9%): 'quote except in the u s the living standard is a little highre '\n",
      "\thyp 19/50 (CER=6.6%): 'quote except in the u s the living standard is a lidl higher '\n",
      "\thyp 20/50 (CER=3.3%): 'quote except in the u s the living standard is a little highe '\n",
      "\thyp 21/50 (CER=8.2%): 'quote except in the u s the living standard is a lidl highere '\n",
      "\thyp 22/50 (CER=4.9%): 'quote except in the u s the living standard is a litle highr '\n",
      "\thyp 23/50 (CER=6.6%): 'quote except in the u s the living standard is a litle highre '\n",
      "\thyp 24/50 (CER=4.9%): 'quote except in the u s the living standard is a lidtle highrer '\n",
      "\thyp 25/50 (CER=3.3%): 'quote except in the u s the living standard is a littl higher '\n",
      "\thyp 26/50 (CER=4.9%): 'quote except in the u s the living standard is a littl highere '\n",
      "\thyp 27/50 (CER=4.9%): 'quote except in the u s the living standard is a litle highe '\n",
      "\thyp 28/50 (CER=6.6%): 'quote except in the u s the living standard is a lidtl highr '\n",
      "\thyp 29/50 (CER=6.6%): 'quote except in the u s the living standard is a lidle highrer '\n",
      "\thyp 30/50 (CER=8.2%): 'quote except in the u s the living standard is a lidtl highre '\n",
      "\thyp 31/50 (CER=4.9%): 'quote except in the u s the living standard is a litl higher '\n",
      "\thyp 32/50 (CER=6.6%): 'quote except in the u s the living standard is a litl highere '\n",
      "\thyp 33/50 (CER=6.6%): 'quote except in the u s the living standard is a lidtl highe '\n",
      "\thyp 34/50 (CER=3.3%): 'quote except in the u s the living standard is a little highrer '\n",
      "\thyp 35/50 (CER=8.2%): 'quote except in the u s the living standard is a lidl highr '\n",
      "\thyp 36/50 (CER=9.8%): 'quote except in the u s the living standard is a lidl highre '\n",
      "\thyp 37/50 (CER=8.2%): 'quote except in the u s the living standard is a lidl highe '\n",
      "\thyp 38/50 (CER=4.9%): 'quote except in the u s the living standard is a littl highr '\n",
      "\thyp 39/50 (CER=4.9%): 'quote except in the u s the living standard is a litle highrer '\n",
      "\thyp 40/50 (CER=6.6%): 'quote except in the u s the living standard is a littl highre '\n",
      "\thyp 41/50 (CER=4.9%): 'quote except in the u s the living standard is a littl highe '\n",
      "\thyp 42/50 (CER=6.6%): 'quote except in the u s the living standard is a litl highr '\n",
      "\thyp 43/50 (CER=8.2%): 'quote except in the u s the living standard is a litl highre '\n",
      "\thyp 44/50 (CER=6.6%): 'quote except in the u s the living standard is a lidtl highrer '\n",
      "\thyp 45/50 (CER=4.9%): 'quote except in the u s the livng standard is a lidtle higher '\n",
      "\thyp 46/50 (CER=6.6%): 'quote except in the u s the livng standard is a lidtle highere '\n",
      "\thyp 47/50 (CER=3.3%): 'quote except in the u s the living standard is a litdtle higher '\n",
      "\thyp 48/50 (CER=4.9%): 'quote except in the u s the living standard is a litdtle highere '\n",
      "\thyp 49/50 (CER=6.6%): 'quote except in the u s the living standard is a litl highe '\n",
      "\thyp 50/50 (CER=8.2%): 'quote except in the u s the living standard is a lidl highrer '\n",
      "\t=== Mean CER: 5.7%, Std CER: 1.7% ===\n",
      "\n",
      "sample 5 - (LJ014-0003: 'the cries of his victim a mister delarue')\n",
      "\thyp 1/31 (CER=0.0%): 'the cries of his victim a mister delarue '\n",
      "\thyp 2/31 (CER=2.5%): 'the cries of his victim a mister dellarue '\n",
      "\thyp 3/31 (CER=2.5%): 'the cries of his victim a mister delaru '\n",
      "\thyp 4/31 (CER=2.5%): 'the crie of his victim a mister delarue '\n",
      "\thyp 5/31 (CER=2.5%): 'the cries of his victim a mister delaruee '\n",
      "\thyp 6/31 (CER=2.5%): 'the cries of his victim a mister delaroe '\n",
      "\thyp 7/31 (CER=2.5%): 'the cries of his victim a mister delalrue '\n",
      "\thyp 8/31 (CER=2.5%): 'the cries of his victim a mister dela rue '\n",
      "\thyp 9/31 (CER=2.5%): 'the cries of his vict im a mister delarue '\n",
      "\thyp 10/31 (CER=2.5%): 'the cries of his victdim a mister delarue '\n",
      "\thyp 11/31 (CER=2.5%): 'the cries of his victim a mister delaroue '\n",
      "\thyp 12/31 (CER=2.5%): 'the cries of his victim a mister delaree '\n",
      "\thyp 13/31 (CER=2.5%): 'the cries of his victhim a mister delarue '\n",
      "\thyp 14/31 (CER=2.5%): 'the cries of his victim a mister delaruoe '\n",
      "\thyp 15/31 (CER=2.5%): 'the cries of his victim a mister delaruue '\n",
      "\thyp 16/31 (CER=2.5%): 'the cries of his victim a mister delare '\n",
      "\thyp 17/31 (CER=5.0%): 'the cries of his victim a mister dellaru '\n",
      "\thyp 18/31 (CER=2.5%): 'the cries of his victim a mister delaruwe '\n",
      "\thyp 19/31 (CER=2.5%): 'the cries of his victim a mister delawrue '\n",
      "\thyp 20/31 (CER=5.0%): 'the crie of his victim a mister dellarue '\n",
      "\thyp 21/31 (CER=5.0%): 'the crie of his victim a mister delaru '\n",
      "\thyp 22/31 (CER=2.5%): 'the cries of his victim a mister delearue '\n",
      "\thyp 23/31 (CER=2.5%): 'the cries of his victim ea mister delarue '\n",
      "\thyp 24/31 (CER=2.5%): 'the cries of his victim a mister delarrue '\n",
      "\thyp 25/31 (CER=5.0%): 'the cries of his victim a mister delelarue '\n",
      "\thyp 26/31 (CER=5.0%): 'the cries of his victim a mister dellaruee '\n",
      "\thyp 27/31 (CER=2.5%): 'the cries of his victim ia mister delarue '\n",
      "\thyp 28/31 (CER=0.0%): 'the cries of his victim a mister delarue '\n",
      "\thyp 29/31 (CER=2.5%): 'the cries of his victim a mister delareue '\n",
      "\thyp 30/31 (CER=5.0%): 'the crie of his victim a mister delaruee '\n",
      "\thyp 31/31 (CER=2.5%): 'the crises of his victim a mister delarue '\n",
      "\t=== Mean CER: 2.8%, Std CER: 1.2% ===\n",
      "\n",
      "sample 6 - (LJ009-0005: 'which persons under sentence of death obtain from all the officers of the prison')\n",
      "\thyp 1/48 (CER=1.2%): 'which persons under sentence of death obtain from allthe officers of the prison '\n",
      "\thyp 2/48 (CER=0.0%): 'which persons under sentence of death obtain from all the officers of the prison '\n",
      "\thyp 3/48 (CER=2.5%): 'which persons under sentence of death obtain from althe officers of the prison '\n",
      "\thyp 4/48 (CER=1.2%): 'which persons under sentence of death obtain from al the officers of the prison '\n",
      "\thyp 5/48 (CER=2.5%): 'which persons under sentence of death obtain from allte officers of the prison '\n",
      "\thyp 6/48 (CER=1.2%): 'which persons under sentence of death obtain from all he officers of the prison '\n",
      "\thyp 7/48 (CER=1.2%): 'which persons under sentence of death obtain from all te officers of the prison '\n",
      "\thyp 8/48 (CER=3.8%): 'which persons under sentence of death obtain from alte officers of the prison '\n",
      "\thyp 9/48 (CER=2.5%): 'which persons under sentence of death obtaine from allthe officers of the prison '\n",
      "\thyp 10/48 (CER=2.5%): 'which persons under sentence of death obtain from allth officers of the prison '\n",
      "\thyp 11/48 (CER=2.5%): 'which persons under sentence of death btain from allthe officers of the prison '\n",
      "\thyp 12/48 (CER=2.5%): 'which persons under sentence of deatht obtain from allthe officers of the prison '\n",
      "\thyp 13/48 (CER=2.5%): 'which persons under sentence of death obtain from al he officers of the prison '\n",
      "\thyp 14/48 (CER=2.5%): 'which persons under sentence of death obtain from al te officers of the prison '\n",
      "\thyp 15/48 (CER=1.2%): 'which persons under sentence of death obtaine from all the officers of the prison '\n",
      "\thyp 16/48 (CER=1.2%): 'which persons under sentence of death obtain from all th officers of the prison '\n",
      "\thyp 17/48 (CER=2.5%): 'which persons under sentence of death obtains from allthe officers of the prison '\n",
      "\thyp 18/48 (CER=1.2%): 'which persons under sentence of death btain from all the officers of the prison '\n",
      "\thyp 19/48 (CER=1.2%): 'which persons under sentence of deatht obtain from all the officers of the prison '\n",
      "\thyp 20/48 (CER=3.8%): 'which persons under sentence of death obtaine from althe officers of the prison '\n",
      "\thyp 21/48 (CER=3.8%): 'which persons under sentence of death obtain from alth officers of the prison '\n",
      "\thyp 22/48 (CER=3.8%): 'which persons under sentence of death btain from althe officers of the prison '\n",
      "\thyp 23/48 (CER=3.8%): 'which persons under sentence of deatht obtain from althe officers of the prison '\n",
      "\thyp 24/48 (CER=1.2%): 'which persons under sentence of death obtains from all the officers of the prison '\n",
      "\thyp 25/48 (CER=3.8%): 'which persons under sentence of death obtainen from allthe officers of the prison '\n",
      "\thyp 26/48 (CER=2.5%): 'which persons under sentence of death obtain from allthe officers of the prisons '\n",
      "\thyp 27/48 (CER=1.2%): 'which persons under sentence of death obtain from all the officers of the prisons '\n",
      "\thyp 28/48 (CER=2.5%): 'which persons under sentence of death obtain from allthe officers of the prisonr '\n",
      "\thyp 29/48 (CER=1.2%): 'which persons under sentence of death obtain from all the officers of the prisonr '\n",
      "\thyp 30/48 (CER=3.8%): 'which persons under sentence of death obtain from althe officers of the prisonr '\n",
      "\thyp 31/48 (CER=2.5%): 'which persons under sentence of death obtain from allthe officers of the prisonn '\n",
      "\thyp 32/48 (CER=2.5%): 'which persons under sentence of death obtain from al the officers of the prisonr '\n",
      "\thyp 33/48 (CER=3.8%): 'which persons under sentence of death obtain from allte officers of the prisonr '\n",
      "\thyp 34/48 (CER=1.2%): 'which persons under sentence of death obtain from all the officers of the prisonn '\n",
      "\thyp 35/48 (CER=3.8%): 'which persons under sentence of death obtain from althe officers of the prisons '\n",
      "\thyp 36/48 (CER=3.8%): 'which persons under sentence of death obtain from althe officers of the prisonn '\n",
      "\thyp 37/48 (CER=2.5%): 'which persons under sentence of death obtain from allthe officers of the prisond '\n",
      "\thyp 38/48 (CER=2.5%): 'which persons under sentence of death obtain from all he officers of the prisonr '\n",
      "\thyp 39/48 (CER=1.2%): 'which persons under sentence of death obtain from allthe officers of the prison '\n",
      "\thyp 40/48 (CER=2.5%): 'which persons under sentence of death obtain from all te officers of the prisonr '\n",
      "\thyp 41/48 (CER=2.5%): 'which persons under sentence of death obtain from al the officers of the prisons '\n",
      "\thyp 42/48 (CER=5.0%): 'which persons under sentence of death obtain from alte officers of the prisonr '\n",
      "\thyp 43/48 (CER=2.5%): 'which persons under sentence of death obtain from al the officers of the prisonn '\n",
      "\thyp 44/48 (CER=3.8%): 'which persons under sentence of death obtain from allte officers of the prisons '\n",
      "\thyp 45/48 (CER=3.8%): 'which persons under sentence of death obtaine from allthe officers of the prisonr '\n",
      "\thyp 46/48 (CER=3.8%): 'which persons under sentence of death obtain from allth officers of the prisonr '\n",
      "\thyp 47/48 (CER=3.8%): 'which persons under sentence of death obtain from allte officers of the prisonn '\n",
      "\thyp 48/48 (CER=1.2%): 'which persons under sentence of death obtain from all the officers of the prisond '\n",
      "\t=== Mean CER: 2.5%, Std CER: 1.1% ===\n",
      "\n",
      "sample 7 - (LJ014-0140: 'after judgment was passed she repeatedly cried out shame')\n",
      "\thyp 1/41 (CER=0.0%): 'after judgment was passed she repeatedly cried out shame '\n",
      "\thyp 2/41 (CER=1.8%): 'after judgment was passed she wrepeatedly cried out shame '\n",
      "\thyp 3/41 (CER=1.8%): 'after judgment was passed she repeatedly cried out shaime '\n",
      "\thyp 4/41 (CER=1.8%): 'after judgmant was passed she repeatedly cried out shame '\n",
      "\thyp 5/41 (CER=1.8%): 'after judgment was passed she repeatedly cried out shamme '\n",
      "\thyp 6/41 (CER=1.8%): 'after judgment was passed she repeatedly cried out shanme '\n",
      "\thyp 7/41 (CER=1.8%): 'after judgment was passed she repeatedly cried out sham '\n",
      "\thyp 8/41 (CER=1.8%): 'after judgment was passed she repeatedly cried out shamne '\n",
      "\thyp 9/41 (CER=3.6%): 'after judgment was passed she wrepeatedly cried out shaime '\n",
      "\thyp 10/41 (CER=1.8%): 'after judgment was pased she repeatedly cried out shame '\n",
      "\thyp 11/41 (CER=1.8%): 'after judgment was passed she repeatedly cried out shaame '\n",
      "\thyp 12/41 (CER=1.8%): 'after judgment was passed she repetedly cried out shame '\n",
      "\thyp 13/41 (CER=1.8%): 'after judgment was passed she repeatedly cried out schame '\n",
      "\thyp 14/41 (CER=1.8%): 'after judgment was passed she repeated ly cried out shame '\n",
      "\thyp 15/41 (CER=1.8%): 'after judgment was passed she repeatedly cried out shavme '\n",
      "\thyp 16/41 (CER=1.8%): 'after judgment was passed she repeatedly crid out shame '\n",
      "\thyp 17/41 (CER=1.8%): 'after judgment was passed she repeatedly cried out shamee '\n",
      "\thyp 18/41 (CER=1.8%): 'after judgment was passsed she repeatedly cried out shame '\n",
      "\thyp 19/41 (CER=1.8%): 'after joudgment was passed she repeatedly cried out shame '\n",
      "\thyp 20/41 (CER=1.8%): 'after judgment was passed she repeatedly crided out shame '\n",
      "\thyp 21/41 (CER=3.6%): 'after judgmant was passed she wrepeatedly cried out shame '\n",
      "\thyp 22/41 (CER=1.8%): 'after judgment was passed she repatedly cried out shame '\n",
      "\thyp 23/41 (CER=1.8%): 'after judgmeant was passed she repeatedly cried out shame '\n",
      "\thyp 24/41 (CER=3.6%): 'after judgment was passed she wrepeatedly cried out shamme '\n",
      "\thyp 25/41 (CER=1.8%): 'after judgment was passd she repeatedly cried out shame '\n",
      "\thyp 26/41 (CER=3.6%): 'after judgmant was passed she repeatedly cried out shaime '\n",
      "\thyp 27/41 (CER=1.8%): 'after judment was passed she repeatedly cried out shame '\n",
      "\thyp 28/41 (CER=1.8%): 'aftr judgment was passed she repeatedly cried out shame '\n",
      "\thyp 29/41 (CER=3.6%): 'after judgment was passed she repeatedly cried out shamnme '\n",
      "\thyp 30/41 (CER=0.0%): 'after judgment was passed she repeatedly cried out shame '\n",
      "\thyp 31/41 (CER=1.8%): 'after judgment was passed she repeatedly cried out shamed '\n",
      "\thyp 32/41 (CER=3.6%): 'after judgment was passed she wrepeatedly cried out shamee '\n",
      "\thyp 33/41 (CER=3.6%): 'after judgment was passed she repeatedly cried out shaimee '\n",
      "\thyp 34/41 (CER=1.8%): 'after judgment was passed she wrepeatedly cried out shame '\n",
      "\thyp 35/41 (CER=3.6%): 'after judgmant was passed she repeatedly cried out shamee '\n",
      "\thyp 36/41 (CER=3.6%): 'after judgment was passed she wrepeatedly cried out shamed '\n",
      "\thyp 37/41 (CER=3.6%): 'after judgment was passed she repeatedly cried out shammee '\n",
      "\thyp 38/41 (CER=1.8%): 'after judgment was passed she repeatedly cried out shaime '\n",
      "\thyp 39/41 (CER=3.6%): 'after judgment was passed she repeatedly cried out shaimed '\n",
      "\thyp 40/41 (CER=1.8%): 'after judgment was passed she repeatedly cried out shamen '\n",
      "\thyp 41/41 (CER=3.6%): 'after judgment was passed she repeatedly cried out shanmee '\n",
      "\t=== Mean CER: 2.2%, Std CER: 0.9% ===\n",
      "\n",
      "sample 8 - (LJ050-0017: 'to be able to provide effective continuing supervision')\n",
      "\thyp 1/47 (CER=0.0%): 'to be able to provide effective continuing supervision '\n",
      "\thyp 2/47 (CER=1.9%): 'to be able to provide affective continuing supervision '\n",
      "\thyp 3/47 (CER=1.9%): 'to be able to provide effective continuing upervision '\n",
      "\thyp 4/47 (CER=3.7%): 'to be able to provide affective continuing upervision '\n",
      "\thyp 5/47 (CER=1.9%): 'to behable to provide effective continuing supervision '\n",
      "\thyp 6/47 (CER=1.9%): 'to be able to provide e fective continuing supervision '\n",
      "\thyp 7/47 (CER=1.9%): 'to be able to provide effective continuing superbvision '\n",
      "\thyp 8/47 (CER=1.9%): 't be able to provide effective continuing supervision '\n",
      "\thyp 9/47 (CER=1.9%): 'to be able to provide offective continuing supervision '\n",
      "\thyp 10/47 (CER=1.9%): 'to be apble to provide effective continuing supervision '\n",
      "\thyp 11/47 (CER=3.7%): 'to behable to provide affective continuing supervision '\n",
      "\thyp 12/47 (CER=1.9%): 'to be avble to provide effective continuing supervision '\n",
      "\thyp 13/47 (CER=1.9%): 'to be able to provide efective continuing supervision '\n",
      "\thyp 14/47 (CER=1.9%): 'to be amble to provide effective continuing supervision '\n",
      "\thyp 15/47 (CER=1.9%): 'to be ablel to provide effective continuing supervision '\n",
      "\thyp 16/47 (CER=3.7%): 'to behable to provide effective continuing upervision '\n",
      "\thyp 17/47 (CER=1.9%): 'to be eble to provide effective continuing supervision '\n",
      "\thyp 18/47 (CER=3.7%): 'to be able to provide a fective continuing supervision '\n",
      "\thyp 19/47 (CER=1.9%): 'to be able to provide effective continuing supervbision '\n",
      "\thyp 20/47 (CER=1.9%): 'to be abe to provide effective continuing supervision '\n",
      "\thyp 21/47 (CER=1.9%): 'to be able to provide effective coninuing supervision '\n",
      "\thyp 22/47 (CER=3.7%): 'to be able to provide affective continuing superbvision '\n",
      "\thyp 23/47 (CER=1.9%): 'to b able to provide effective continuing supervision '\n",
      "\thyp 24/47 (CER=3.7%): 't be able to provide affective continuing supervision '\n",
      "\thyp 25/47 (CER=1.9%): 'to beable to provide effective continuing supervision '\n",
      "\thyp 26/47 (CER=1.9%): 'to be abple to provide effective continuing supervision '\n",
      "\thyp 27/47 (CER=0.0%): 'to be able to provide effective continuing supervision '\n",
      "\thyp 28/47 (CER=1.9%): 'to be able to provide affective continuing supervision '\n",
      "\thyp 29/47 (CER=1.9%): 'to be able to provide effective continuing upervision '\n",
      "\thyp 30/47 (CER=3.7%): 'to be able to provide affective continuing upervision '\n",
      "\thyp 31/47 (CER=1.9%): 'to behable to provide effective continuing supervision '\n",
      "\thyp 32/47 (CER=1.9%): 'to be able to provide effective continuing supervisionn '\n",
      "\thyp 33/47 (CER=1.9%): 'to be able to provide e fective continuing supervision '\n",
      "\thyp 34/47 (CER=1.9%): 'to be able to provide effective continuing superbvision '\n",
      "\thyp 35/47 (CER=1.9%): 't be able to provide effective continuing supervision '\n",
      "\thyp 36/47 (CER=1.9%): 'to be able to provide effective continuing supervisionh '\n",
      "\thyp 37/47 (CER=1.9%): 'to be able to provide offective continuing supervision '\n",
      "\thyp 38/47 (CER=1.9%): 'to be able to provide effective continuing supervisionr '\n",
      "\thyp 39/47 (CER=1.9%): 'to be apble to provide effective continuing supervision '\n",
      "\thyp 40/47 (CER=3.7%): 'to behable to provide affective continuing supervision '\n",
      "\thyp 41/47 (CER=1.9%): 'to be avble to provide effective continuing supervision '\n",
      "\thyp 42/47 (CER=3.7%): 'to be able to provide affective continuing supervisionn '\n",
      "\thyp 43/47 (CER=1.9%): 'to be able to provide efective continuing supervision '\n",
      "\thyp 44/47 (CER=1.9%): 'to be amble to provide effective continuing supervision '\n",
      "\thyp 45/47 (CER=1.9%): 'to be able to provide effective continuing supervisiong '\n",
      "\thyp 46/47 (CER=1.9%): 'to be able to provide effective continuing supervisions '\n",
      "\thyp 47/47 (CER=1.9%): 'to be ablel to provide effective continuing supervision '\n",
      "\t=== Mean CER: 2.1%, Std CER: 0.9% ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# generate transcriptions for all batches in test set\n",
    "def transcribe_dataset(asr_brain, dataset, greedy=False, num_batches_to_transcribe=None):\n",
    "    # Now we iterate over the dataset and we simply compute_forward and decode\n",
    "    with torch.no_grad():\n",
    "        transcripts = []\n",
    "        for batch in tqdm(list(dataset)[:num_batches_to_transcribe], dynamic_ncols=True):\n",
    "            orig_transcriptions = batch.words\n",
    "\n",
    "            # Make sure that your compute_forward returns the predictions !!!\n",
    "            # In the case of the template, when stage = TEST, a beam search is applied \n",
    "            # in compute_forward(). \n",
    "            predictions = asr_brain.compute_forward(batch, stage=sb.Stage.TEST)\n",
    "            \n",
    "            ctc_probs = predictions['ctc_logprobs'] # FOR DEBUG\n",
    "\n",
    "            if greedy:\n",
    "                predicted_ids = sb.decoders.ctc_greedy_decode(\n",
    "                    predictions[\"ctc_logprobs\"], asr_brain.feat_lens, blank_id=asr_brain.hparams.blank_index\n",
    "                )\n",
    "                predicted_words = [\n",
    "                    asr_brain.tokenizer.decode_ids(ids).split(\" \")\n",
    "                    for ids in predicted_ids\n",
    "                ]\n",
    "            else:\n",
    "                # get mel lens from wav len ratios since torch ctc decoder requires lens in frames\n",
    "                batch_max_len = predictions[\"ctc_logprobs\"].size(1)\n",
    "                bsz = predictions[\"ctc_logprobs\"].size(0)\n",
    "                mel_lens = torch.zeros(bsz)\n",
    "                for i, len_ratio in enumerate(asr_brain.feat_lens):\n",
    "                    mel_lens[i] = int(torch.round(len_ratio * batch_max_len))\n",
    "                \n",
    "                predicted_ids = ctc_beamsearch_decoder(\n",
    "                    predictions[\"ctc_logprobs\"], lengths=mel_lens\n",
    "                )\n",
    "\n",
    "                predicted_words = []\n",
    "                for i, (utt_id, orig_text, hyps) in enumerate(zip(batch.utt_id, orig_transcriptions, predicted_ids)):\n",
    "                    print(f\"\\nsample {i+1} - ({utt_id}: '{orig_text}')\")\n",
    "                    sample_cers = []\n",
    "                    for j, hyp in enumerate(hyps):\n",
    "                        words = asr_brain.hparams.tokenizer.decode_ids(hyp.tokens.tolist()) # .split(\"|\")\n",
    "                        # words = tokenizer.decode_ids(hyp.tokens.tolist()) # .split(\"|\")\n",
    "                        hyp_cer = 100 * cer(orig_text, words)\n",
    "                        sample_cers.append(hyp_cer)\n",
    "                        print(f\"\\thyp {j+1}/{len(hyps)} (CER={hyp_cer:.1f}%): '{words}'\")\n",
    "                        predicted_words.append((f\"sample {i+1}, hyp {j+1}/{len(hyps)}\", words))\n",
    "                        \n",
    "                    print(f\"\\t=== Mean CER: {np.mean(sample_cers):.1f}%, Std CER: {np.std(sample_cers):.1f}% ===\")\n",
    "\n",
    "            transcripts.append(predicted_words)\n",
    "\n",
    "    return transcripts, ctc_probs\n",
    "\n",
    "transcripts, ctc_probs = transcribe_dataset(asr_brain, test_set, greedy=False, num_batches_to_transcribe=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3cbbebcc",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "stop here for development of ctc beam search decoder",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mstop here for development of ctc beam search decoder\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: stop here for development of ctc beam search decoder"
     ]
    }
   ],
   "source": [
    "raise ValueError(\"stop here for development of ctc beam search decoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a387be4-9390-459d-92a5-2389ae883ed4",
   "metadata": {},
   "source": [
    "# DATAPOOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c8214f-8ecc-4dd2-9e2e-0437d2376261",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "## load SpellerDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf97be5-efdb-4295-852a-9975ce615bba",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from rlspeller.dataset import SpellerDataset\n",
    "\n",
    "def load_dataset(split):\n",
    "    wordaligned_speechreps_dir = '/home/s1785140/data/ljspeech_speechbrain/wordaligned_mels'\n",
    "    wordlists = {\n",
    "        \"train\": '/home/s1785140/data/ljspeech_fastpitch/respeller_train_words.json',\n",
    "        \"val\": '/home/s1785140/data/ljspeech_fastpitch/respeller_val_words.json',\n",
    "        \"test\": '/home/s1785140/data/ljspeech_fastpitch/respeller_test_words.json',\n",
    "    }\n",
    "    \n",
    "    return SpellerDataset(wordaligned_speechreps_dir, wordlists[split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfd206e-a80e-4490-99dc-6a077f829ecc",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# define Sample and Datapool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13ff58d-9f56-4c45-aed4-85ed9ddddff8",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from abc import abstractclassmethod\n",
    "\n",
    "@dataclass(init=True)\n",
    "class Sample:\n",
    "    id: str # \n",
    "    gt_mel_path: str # full path to mel spectrogram\n",
    "    gt_text: str # original spelling of word\n",
    "    meta_data: Dict[str, Any] = None\n",
    "\n",
    "\n",
    "class ASRPool:\n",
    "    def __init__(self, samples: List[Sample]):\n",
    "        self._samples = samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._samples)\n",
    "\n",
    "    def __getitem__(self, ix: int) -> Sample:\n",
    "        if ix >= len(self):\n",
    "            raise StopIteration\n",
    "        sample = self._samples[ix]\n",
    "        return sample, 1.0\n",
    "\n",
    "    def sample(self) -> Sample:\n",
    "        random_sample = random.choice(self._samples)\n",
    "        return random_sample\n",
    "\n",
    "    @abstractclassmethod\n",
    "    def prepare(cls, **args) -> 'TextGenPool':\n",
    "        \"\"\"\n",
    "        A factory method to instantiate data pool\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def split(self, split_ratios: List[float]) -> List['TextGenPool']:\n",
    "        start_ix = 0\n",
    "        pools = []\n",
    "        for ratio in split_ratios:\n",
    "            count = int(len(self) * ratio)\n",
    "            end_ix = start_ix + count\n",
    "            pools.append(type(self)(self._samples[start_ix: end_ix]))\n",
    "            start_ix = end_ix\n",
    "        return pools\n",
    "    \n",
    "class LJSpeech(ASRPool):\n",
    "    @classmethod\n",
    "    def prepare(cls, split: str, **args) -> 'ASRPool':\n",
    "        ds = load_dataset(split)\n",
    "        samples = []\n",
    "        for idx, item in tqdm(enumerate(ds)):\n",
    "            # sample = Sample(\n",
    "            #     id: f\"{split}_{idx}\" \n",
    "            #     gt_mel_path: str \n",
    "            #     gt_text: str \n",
    "            # )\n",
    "            samples.append(sample)\n",
    "\n",
    "        pool_instance = cls(samples)\n",
    "        return pool_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a7e707-e639-4aed-b069-220006aba329",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "train_datapool = LJSpeech.prepare('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f666a7-6b7e-4bb3-a723-af922d4e5565",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "train_datapool._samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe55d99-e0c0-4208-9020-2b297517972d",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "train_datapool = _get_datapool_by_split(\"train\")\n",
    "val_datapool = _get_datapool_by_split(\"val\")\n",
    "test_datapool = _get_datapool_by_split(\"test\")\n",
    "\n",
    "samples_by_split = {\n",
    "    \"train\": [(sample, weight)\n",
    "              for sample, weight in train_datapool],\n",
    "    \"val\": [sample for sample, _ in val_datapool],\n",
    "    \"test\": [sample for sample, _ in test_datapool]\n",
    "}\n",
    "return samples_by_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674722a6-d6ee-4feb-9f63-d8b8b893e619",
   "metadata": {},
   "source": [
    "# REWARD FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835c2708-462e-46a8-a945-bda08b859ed8",
   "metadata": {},
   "source": [
    "## funcs to load pretrained fastpitch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945147f5-5dc5-4367-affe-08ae0291e6d1",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "from fastpitch import models as fastpitch_model\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Fastpitch Model Config Parser', allow_abbrev=False)\n",
    "parser = fastpitch_model.parse_model_args('FastPitch', parser)\n",
    "args, unk_args = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1df11c-91d2-4aeb-b966-1dbc9e465def",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "print(\"WARNING!!! unknown args:\", unk_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ce3ef1-1359-4f8e-bfae-ee91a6bc3a54",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training command for no punctuation fastpitch:\n",
    "\n",
    "```bash\n",
    "cd \n",
    "source activate_respeller.sh\n",
    "\n",
    "cd ~/respeller/fastpitch\n",
    "\n",
    "EXP_NAME=halved_ljspeech_data_nospaces_noeos_pad_lowercase_nopunc\n",
    "\n",
    "DATA_ROOT=~/data/ljspeech_fastpitch\n",
    "CHECKPOINT_DIR=exps\n",
    "mkdir $CHECKPOINT_DIR\n",
    "HIFIGAN_CHKPT=~/pretrained_models/hifigan/ljspeech/LJ_V1/generator_v1\n",
    "HIFIGAN_CFG=~/pretrained_models/hifigan/ljspeech/LJ_V1/config.json\n",
    "MASTER_ADDR=`hostname -s`\n",
    "FILELIST_STEM=wav_text_filelist\n",
    "\n",
    "./sbatch.sh python train.py \\\n",
    "  --dataset-path $DATA_ROOT \\\n",
    "  --output $CHECKPOINT_DIR/$EXP_NAME \\\n",
    "  --training-files $DATA_ROOT/train_meta_half.txt \\\n",
    "  --validation-files $DATA_ROOT/val_meta_half.txt \\\n",
    "  --pitch-mean-std-file $DATA_ROOT/pitches_stats__${FILELIST_STEM}.json \\\n",
    "  --input-type char \\\n",
    "  --symbol-set english_pad_lowercase_nopunc \\\n",
    "  --text-cleaners lowercase_no_punc \\\n",
    "  --epochs 1000 \\\n",
    "  --epochs-per-checkpoint 10 \\\n",
    "  --batch-size 16 \\\n",
    "  --use-mas \\\n",
    "  --cuda \\\n",
    "  --hifigan $HIFIGAN_CHKPT \\\n",
    "  --hifigan-config $HIFIGAN_CFG \\\n",
    "  --use-sepconv \\\n",
    "  --master-addr $MASTER_ADDR \\\n",
    "  --checkpoint-path /home/s1785140/respeller/fastpitch/exps/halved_ljspeech_data_nospaces_noeos_pad_lowercase_nopunc/FastPitch_checkpoint_290.pt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e59671f-126c-46a1-899c-e3ff81adb541",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# change values of some args to match the config of the pretrained model \n",
    "args.local_rank = 0\n",
    "args.use_mas = True\n",
    "args.use_sepconv = True\n",
    "args.cuda = torch.cuda.is_available()\n",
    "args.input_type = 'char'\n",
    "args.symbol_set = 'english_pad_lowercase_nopunc'\n",
    "args.n_speakers = 1\n",
    "args.fastpitch_chkpt = \"/home/s1785140/respeller/fastpitch/exps/halved_ljspeech_data_nospaces_noeos_pad_lowercase_nopunc/FastPitch_checkpoint_1000.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e53c29d-d9ce-4e99-ae38-cac96fd5b007",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def load_checkpoint(args, model, filepath):\n",
    "    if args.local_rank == 0:\n",
    "        print(f'Loading model and optimizer state from {filepath}')\n",
    "    checkpoint = torch.load(filepath, map_location='cpu')\n",
    "    sd = {k.replace('module.', ''): v\n",
    "          for k, v in checkpoint['state_dict'].items()}\n",
    "    getattr(model, 'module', model).load_state_dict(sd)\n",
    "    return model\n",
    "\n",
    "def load_pretrained_fastpitch(args):\n",
    "    # load chkpt\n",
    "    device = torch.device('cuda' if args.cuda else 'cpu')\n",
    "    model_config = fastpitch_model.get_model_config('FastPitch', args)\n",
    "    fastpitch = fastpitch_model.get_model('FastPitch', model_config, device, forward_is_infer=True)\n",
    "    load_checkpoint(args, fastpitch, args.fastpitch_chkpt)\n",
    "    # get information about grapheme embedding table\n",
    "    n_symbols = fastpitch.encoder.word_emb.weight.size(0)\n",
    "    embedding_dim = fastpitch.encoder.word_emb.weight.size(1)\n",
    "    return fastpitch, model_config, n_symbols, embedding_dim\n",
    "\n",
    "# from fastpitch.fastpitch.transformer import FFTransformer\n",
    "fastpitch, model_config, n_symbols, embedding_dim = load_pretrained_fastpitch(args)\n",
    "print(\"Finished loading TTS model!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46e4101-61dc-4e18-8ab2-40ee587d2a64",
   "metadata": {},
   "source": [
    "## TTSMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8540b6e0-950c-4c46-ade2-07870b934c03",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class TTSMetric:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path,\n",
    "    ):\n",
    "        self.tts_model = load_fastpitch(model_path)\n",
    "        self.softdtw_loss = criterion = SoftDTW(\n",
    "            use_cuda=torch.cuda.is_available(), \n",
    "            gamma=hparams[\"softdtw_temp\"], \n",
    "            bandwidth=hparams[\"softdtw_bandwidth\"],\n",
    "            dist_func=hparams[\"dist_func\"],\n",
    "        )\n",
    "    \n",
    "    def __call__(\n",
    "        self,\n",
    "        predicted_texts: List[str], # [bsz]\n",
    "        reference_mels: torch.Tensor, # [bsz, seqlen, dim]\n",
    "    ) -> float:\n",
    "        \"\"\"return softdtw loss between two batches of mel-spectrograms\n",
    "        averaged across batch dimension\"\"\"\n",
    "        predicted_mels = self.tts_model(predicted_texts)\n",
    "        return self.softdtw_loss(predicted_mels, reference_mels).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e306d2-b402-4974-977f-a5205ab7f3ed",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "ttsmetric = TTSMetric(hparams[\"tts_model_path\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def187a5-028a-42a6-8684-b05975969dbd",
   "metadata": {},
   "source": [
    "## TTSRewardFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e002d148-d914-49c8-8569-de1e7e86134d",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class TTSRewardFunction:\n",
    "    \"\"\"TTS reward function\"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_path: str,\n",
    "        shaping_fn: str = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._metric = TTSMetric(model_path)\n",
    "        \n",
    "    def __call__(\n",
    "        self,\n",
    "        current_observation: Observation,\n",
    "        action: int,\n",
    "        next_observation: Observation,\n",
    "        done: bool,\n",
    "        meta_info: Dict[str, Any] = None,\n",
    "    ):\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcffa0fb-8585-42ba-a462-2fd42ce3af17",
   "metadata": {},
   "source": [
    "# ENVIRONMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1166f6f4-446f-4b6b-9960-28add60e8705",
   "metadata": {},
   "source": [
    "## create custom env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd7ecda-44d0-4d9c-8940-65c707b4916b",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class ASREnv(gym.Env):\n",
    "    \"\"\"Custom Environment that follows gym interface.\"\"\"\n",
    "    \n",
    "    # below taken from Gym code https://github.com/openai/gym/blob/master/gym/core.py\n",
    "    r\"\"\"The main OpenAI Gym class.\n",
    "    It encapsulates an environment with arbitrary behind-the-scenes dynamics.\n",
    "    An environment can be partially or fully observed.\n",
    "    The main API methods that users of this class need to know are:\n",
    "    - :meth:`step` - Takes a step in the environment using an action returning the next observation, reward,\n",
    "      if the environment terminated and observation information.\n",
    "    - :meth:`reset` - Resets the environment to an initial state, returning the initial observation and observation information.\n",
    "    - :meth:`render` - Renders the environment observation with modes depending on the output\n",
    "    - :meth:`close` - Closes the environment, important for rendering where pygame is imported\n",
    "    And set the following attributes:\n",
    "    - :attr:`action_space` - The Space object corresponding to valid actions\n",
    "    - :attr:`observation_space` - The Space object corresponding to valid observations\n",
    "    - :attr:`reward_range` - A tuple corresponding to the minimum and maximum possible rewards\n",
    "    - :attr:`spec` - An environment spec that contains the information used to initialise the environment from `gym.make`\n",
    "    - :attr:`metadata` - The metadata of the environment, i.e. render modes\n",
    "    - :attr:`np_random` - The random number generator for the environment\n",
    "    Note: a default reward range set to :math:`(-\\infty,+\\infty)` already exists. Set it if you want a narrower range.\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\"render.modes\": [\"human\"]}\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        tokenizer, \n",
    "        reward_function,\n",
    "        samples,\n",
    "        \n",
    "    ):\n",
    "        \"\"\"Generic RL environment to generate ASR hypotheses from input audio\"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self._vocab_size = tokenizer.vocab_size\n",
    "        self.reward_function = reward_function\n",
    "        for sample, weight in samples:\n",
    "            self.sampler_for_replaying.add(sample, weight)\n",
    "        \n",
    "        # Define action and observation space\n",
    "        # They must be gym.spaces objects\n",
    "        self.action_space = spaces.Discrete(n=self._vocab_size)\n",
    "        self.observation_space = DictSpace(\n",
    "            {\n",
    "                # we have to provide fixed sized inputs (padded) because sb3 support for DictObsersevation is limited\n",
    "                # while creating rollout buffers, observations are concatenated for each key\n",
    "                \"prompt_or_input_encoded_pt\": spaces.Box(\n",
    "                    low=0, high=self._vocab_size, shape=(self._max_text_length,)\n",
    "                ),\n",
    "                \"prompt_or_input_attention_mask_pt\": spaces.Box(\n",
    "                    low=0, high=1, shape=(self._max_text_length,)\n",
    "                ),\n",
    "                \"context_encoded_pt\": spaces.Box(\n",
    "                    low=0, high=self._vocab_size, shape=(self.max_steps,)\n",
    "                ),\n",
    "                \"context_attention_mask_pt\": spaces.Box(\n",
    "                    low=0, high=1, shape=(self.max_steps,)\n",
    "                ),\n",
    "                \"input_encoded_pt\": spaces.Box(\n",
    "                    low=0,\n",
    "                    high=self._vocab_size,\n",
    "                    shape=(self._max_text_length + self.max_steps,),\n",
    "                ),\n",
    "                \"input_attention_mask_pt\": spaces.Box(\n",
    "                    low=0, high=1, shape=(self._max_text_length + self.max_steps,)\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def step(self, action):\n",
    "        self.__time_step += 1\n",
    "\n",
    "        # previous obs\n",
    "        previous_obs = self.__current_obs\n",
    "\n",
    "        # just update the context tensor and gets the new observation\n",
    "        self.__current_obs = self.__current_obs.update(action, self.tokenizer)\n",
    "\n",
    "        # decide if the episode is finished or not\n",
    "        done = (action == self.tokenizer.eos_token_id and self._terminate_on_eos) or (\n",
    "            self.__time_step == self.max_steps\n",
    "        )\n",
    "\n",
    "        # compute reward\n",
    "        if not isinstance(self.reward_function, BatchedRewardFunction):\n",
    "            reward = (\n",
    "                None\n",
    "                if self.reward_function is None\n",
    "                else self.reward_function(\n",
    "                    previous_obs,\n",
    "                    action,\n",
    "                    self.__current_obs,\n",
    "                    done,\n",
    "                    self.__current_obs.meta_info,\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            reward = -inf  # will be overridden later\n",
    "\n",
    "        # populate additional info\n",
    "        info = {\n",
    "            \"output\": self.__current_obs.context_text,\n",
    "            \"action_history\": self.__current_obs.action_history,\n",
    "            \"reference_text\": self.__current_obs.target_or_reference_texts,\n",
    "            \"prompt_text\": self.__current_obs.prompt_or_input_text,\n",
    "            \"prev_output\": previous_obs.context_text,\n",
    "            \"meta_info\": previous_obs.meta_info,\n",
    "        }\n",
    "\n",
    "        dict_observation = self.__current_obs.to_dict()\n",
    "        return dict_observation, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the environment and starts a new episode\n",
    "        \"\"\"\n",
    "        # gets a new sample if not provided\n",
    "        if sample is None:\n",
    "            sample = self.sampler_for_replaying.sample(size=1)[0]\n",
    "        self.__current_sample = sample\n",
    "\n",
    "        # init the observation\n",
    "        self.__current_obs = Observation.init_from_sample(\n",
    "            sample,\n",
    "            self.tokenizer,\n",
    "            self._max_text_length,\n",
    "            self.max_steps,\n",
    "            self._prompt_truncation_side,\n",
    "            self._context_start_token,\n",
    "            sample.meta_data,\n",
    "        )\n",
    "\n",
    "        # start the time step counter\n",
    "        self.__time_step = 0\n",
    "\n",
    "        dict_observation = self.__current_obs.to_dict()\n",
    "        return dict_observation\n",
    "\n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6497b419-fadb-4e3e-a546-d4effe989983",
   "metadata": {
    "tags": []
   },
   "source": [
    "## check that env follows Gym interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525eacca-aed7-4ef9-b7dc-f9c3aa6719e1",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "env = CustomEnv(arg1, ...)\n",
    "# It will check your custom environment and output additional warnings if needed\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dece3d-9a3b-49dc-a5c3-a6edfaf1d366",
   "metadata": {},
   "source": [
    "# POLICY/ALGORITHM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12d79c3-b288-40b5-9d19-8d5c504ef21e",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Callable, Dict, List, Optional, Tuple, Type, Union\n",
    "\n",
    "from gym import spaces\n",
    "import torch as th\n",
    "from torch import nn\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "\n",
    "\n",
    "class PPO(OnPolicyAlgorithm):\n",
    "    \"\"\"\n",
    "    Created with reference to Seq2SeqLMActorCriticPolicy\n",
    "    \n",
    "    Custom network for policy and value function.\n",
    "    It receives as input the features extracted by the features extractor.\n",
    "\n",
    "    :param feature_dim: dimension of the features extracted with the features_extractor (e.g. features from a CNN)\n",
    "    :param last_layer_dim_pi: (int) number of units for the last layer of the policy network\n",
    "    :param last_layer_dim_vf: (int) number of units for the last layer of the value network\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_dim: int,\n",
    "        last_layer_dim_pi: int = 64,\n",
    "        last_layer_dim_vf: int = 64,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # IMPORTANT:\n",
    "        # Save output dimensions, used to create the distributions\n",
    "        self.latent_dim_pi = last_layer_dim_pi\n",
    "        self.latent_dim_vf = last_layer_dim_vf\n",
    "\n",
    "        # Policy network\n",
    "        self.policy_net = nn.Sequential(\n",
    "            nn.Linear(feature_dim, last_layer_dim_pi), nn.ReLU()\n",
    "        )\n",
    "        # Value network\n",
    "        self.value_net = nn.Sequential(\n",
    "            nn.Linear(feature_dim, last_layer_dim_vf), nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, features: th.Tensor) -> Tuple[th.Tensor, th.Tensor]:\n",
    "        \"\"\"\n",
    "        :return: (th.Tensor, th.Tensor) latent_policy, latent_value of the specified network.\n",
    "            If all layers are shared, then ``latent_policy == latent_value``\n",
    "        \"\"\"\n",
    "        return self.forward_actor(features), self.forward_critic(features)\n",
    "\n",
    "    def forward_actor(self, features: th.Tensor) -> th.Tensor:\n",
    "        return self.policy_net(features)\n",
    "\n",
    "    def forward_critic(self, features: th.Tensor) -> th.Tensor:\n",
    "        return self.value_net(features)\n",
    "\n",
    "\n",
    "class CustomActorCriticPolicy(ActorCriticPolicy):\n",
    "    def __init__(\n",
    "        self,\n",
    "        observation_space: spaces.Space,\n",
    "        action_space: spaces.Space,\n",
    "        lr_schedule: Callable[[float], float],\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        super().__init__(\n",
    "            observation_space,\n",
    "            action_space,\n",
    "            lr_schedule,\n",
    "            # Pass remaining arguments to base class\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )\n",
    "        # Disable orthogonal initialization\n",
    "        self.ortho_init = False\n",
    "\n",
    "    def _build_mlp_extractor(self) -> None:\n",
    "        self.mlp_extractor = CustomNetwork(self.features_dim)\n",
    "\n",
    "\n",
    "model = PPO(CustomActorCriticPolicy, \"CartPole-v1\", verbose=1)\n",
    "model.learn(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddd7e9d-1caf-4cba-b431-92a8e3e9bb51",
   "metadata": {},
   "source": [
    "# collect rollouts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1cb4a9-b229-46e6-9700-4b0bf71e434b",
   "metadata": {},
   "source": [
    "# create rollout buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdfc397-9e16-4f15-b1c5-b2571e45265a",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "6a8422eeb13ba8f92f71047f64b5c33152e234c2bbad3f45433feda7b6f3b4c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
