{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d155d10-1c44-4a1b-b2f9-89b168afeb1d",
   "metadata": {},
   "source": [
    "# Goal of this notebook\n",
    "\n",
    "- use certain metrics (LM perplexity, G2P error) to identify wordtypes that are more likely to be mispronounced by our grapheme-input TTS system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbc98ee-f488-432c-b0d1-c266cee8097c",
   "metadata": {},
   "source": [
    "# automatic reloading magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "44dde61e-d22b-4383-85e9-b6994cb14fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61abcb09-f231-4f75-8a6c-bd701d9a19b9",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3f303211-1bcd-4f69-a3be-9ceec333ee22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import jiwer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552fe736-d3c3-4814-88a8-b9ccd339403c",
   "metadata": {},
   "source": [
    "# check if have correct type of node "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b9c11189-ba5b-45ae-a62f-2b06b020217b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greider.inf.ed.ac.uk\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "\n",
    "# print hostname to make sure we are on correct node\n",
    "disallowed_nodes = ['escience6']\n",
    "hostname = socket.gethostname()\n",
    "print(hostname)\n",
    "node = hostname.split('.')[0]\n",
    "if node in disallowed_nodes:\n",
    "    raise ValueError(f\"Running on disallowed node {node}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eada4296-861a-4b7a-8512-82306ddc64f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.is_available()\n",
    "assert torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1334affc-0cfd-441d-81c0-1379701f84d5",
   "metadata": {},
   "source": [
    "# Load OOV list\n",
    "\n",
    "(wordtypes not seen in the half of LJSpeech used to train TTS and ASR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b69cafa0-fb92-49c8-b4b2-c3ca00585194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original before cleaning/sampling len(oovs)=8343\n"
     ]
    }
   ],
   "source": [
    "# get oov wordtypes list (words that are not seen in tts training)\n",
    "oov_wordlist_path = '/home/s1785140/data/ljspeech_fastpitch/oov_list.json'\n",
    "with open(oov_wordlist_path, 'r') as f:\n",
    "    oovs_and_freqs = json.load(f)\n",
    "    \n",
    "oovs = set(w.strip() for w in oovs_and_freqs.keys())\n",
    "print(f'original before cleaning/sampling {len(oovs)=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3585bc13-9dc6-40b1-bba0-b59aef6f0a44",
   "metadata": {},
   "source": [
    "# Load G2P pronunciation lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8d2a0944-4f6e-462e-b831-aac086b839eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset librig2p-nostress (/home/s1785140/.cache/huggingface/datasets/flexthink___librig2p-nostress/default/0.0.0/95c204c6be42796a753ef410b5dfce2bfa21d61b51f0c3ffe85cf6e3a4dee65f)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e88766779864bafb0bcaf4c98065ce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load lexicon that G2P model was trained on\n",
    "dataset_dict = load_dataset(\"flexthink/librig2p-nostress\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "318206e6-a420-4e2f-b9a0-f53c25e42fac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset_dict['lexicon_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0a51fe2e-7c43-44a0-93fa-381f52ff9874",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# combine data splits in the lexicon \n",
    "# (as we are not training G2P, don't need train valid test splits)\n",
    "from datasets import concatenate_datasets\n",
    "datasets = [dataset_dict['lexicon_train'], dataset_dict['lexicon_valid'], dataset_dict['lexicon_test']]\n",
    "dataset = concatenate_datasets(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c7f4b825-85d3-4200-8d84-d03e6b5fb990",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create dict of wordtype to pronunciation\n",
    "lexicon = {}\n",
    "for char, phn in zip(dataset['char'], dataset['phn']):\n",
    "    lexicon[char.lower().strip()] = phn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d513c8b9-1463-4c67-9339-02ff2e810a07",
   "metadata": {},
   "source": [
    "# only consider the OOV words that are in the pronunciation lexicon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f09fd59d-1fbb-40de-a326-3a17ec4fdcc1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8343"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(oovs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1c15bc92-6529-4a0d-99af-f5b67073b7b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7966"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oovs_in_lexicon = oovs.intersection(lexicon.keys())\n",
    "len(oovs_in_lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d53049-9b81-481d-b613-1245b4fd9b5e",
   "metadata": {},
   "source": [
    "# G2P Error Rates\n",
    "\n",
    "Only will work with words that occur in the lexicon?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "863a6de5-9834-45e4-9895-399d60acd860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load G2P model \n",
    "from speechbrain.pretrained import GraphemeToPhoneme\n",
    "g2p = GraphemeToPhoneme.from_hparams(\"speechbrain/soundchoice-g2p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5a838975-712e-4b81-a822-ede67d10120a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "oovs_in_lexicon = sorted(oovs_in_lexicon, key=lambda x: len(x), reverse=True) # sort by len to make batches more efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4c3861bf-2b05-4457-9d81-812f2b804ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [\"AH\", \"BEE\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38dc871-a627-4a6d-a342-ea7f671a3260",
   "metadata": {},
   "source": [
    "## use g2p to predict phn seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a96b6c53-8294-460c-b024-57283e2eb426",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                              | 0/63 [00:00<?, ?it/s]Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|█████████████████████████████████████████████████████| 63/63 [21:51<00:00, 20.82s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "wordtype2predicted_phnseq = {}\n",
    "batch_size = 128\n",
    "warning_phn_seqs = []\n",
    "for i in tqdm(range(0, len(oovs_in_lexicon), batch_size)):\n",
    "    batch = oovs_in_lexicon[i:i+batch_size]\n",
    "    predictions = g2p(batch)\n",
    "    for wordtype, phn_seq in zip(batch, predictions):\n",
    "        if \"\" in phn_seq or \" \" in phn_seq:\n",
    "            warning_phn_seqs.append((wordtype, phn_seq))\n",
    "        phn_seq = [phn for phn in phn_seq if phn not in [\"\", \" \"]] # strip empty phns\n",
    "        wordtype2predicted_phnseq[wordtype] = phn_seq\n",
    "    # break # debug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28da7ef-6357-4e28-942a-c9196876f626",
   "metadata": {},
   "source": [
    "## calc PER for each phn seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c530d9e8-f7fc-43c3-b0ff-77ca979b9d42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "42c7a1ad-e9fb-47d5-8d54-027e83de2dab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import textdistance\n",
    "lev = textdistance.Levenshtein() # create an instance of Levenshtein distance\n",
    "def per(ref, hyp):\n",
    "    dist = lev.distance(ref, hyp) # calculate the distance between ref and hyp\n",
    "    per = dist / len(ref) # calculate the PER by dividing by the length of ref\n",
    "    return per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "925069fa-ffb7-4352-9136-df86d9279693",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for wordtype, predicted_phnseq in wordtype2predicted_phnseq.items():\n",
    "    data.append({\n",
    "        \"wordtype\": wordtype, \n",
    "        \"lexicon_entry\": lexicon[wordtype], \n",
    "        \"predicted_phnseq\": predicted_phnseq, \n",
    "        \"per\": per(lexicon[wordtype], predicted_phnseq),\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7af308b8-759c-4bff-add2-622ea015e934",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wordtype</th>\n",
       "      <th>lexicon_entry</th>\n",
       "      <th>predicted_phnseq</th>\n",
       "      <th>per</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7884</th>\n",
       "      <td>hon</td>\n",
       "      <td>[AH, N, ER, AH, B, AH, L]</td>\n",
       "      <td>[HH, AA, N]</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7953</th>\n",
       "      <td>un</td>\n",
       "      <td>[Y, UW, EH, N]</td>\n",
       "      <td>[AH, N]</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7961</th>\n",
       "      <td>ne</td>\n",
       "      <td>[N, AO, R, TH, IY, S, T]</td>\n",
       "      <td>[N, IY]</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5160</th>\n",
       "      <td>jacques</td>\n",
       "      <td>[ZH, AA, K]</td>\n",
       "      <td>[JH, AA, K, EY]</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7926</th>\n",
       "      <td>moi</td>\n",
       "      <td>[M, W, AA]</td>\n",
       "      <td>[M, OY]</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>detriment</td>\n",
       "      <td>[D, EH, T, R, AH, M, AH, N, T]</td>\n",
       "      <td>[D, EH, T, R, AH, M, AH, N, T]</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>analogous</td>\n",
       "      <td>[AH, N, AE, L, AH, G, AH, S]</td>\n",
       "      <td>[AH, N, AE, L, AH, G, AH, S]</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2994</th>\n",
       "      <td>equerries</td>\n",
       "      <td>[IH, K, W, EH, R, IY, Z]</td>\n",
       "      <td>[IH, K, W, EH, R, IY, Z]</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2993</th>\n",
       "      <td>divisions</td>\n",
       "      <td>[D, IH, V, IH, ZH, AH, N, Z]</td>\n",
       "      <td>[D, IH, V, IH, ZH, AH, N, Z]</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7965</th>\n",
       "      <td>u</td>\n",
       "      <td>[Y, UW]</td>\n",
       "      <td>[Y, UW]</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7966 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       wordtype                   lexicon_entry  \\\n",
       "7884        hon       [AH, N, ER, AH, B, AH, L]   \n",
       "7953         un                  [Y, UW, EH, N]   \n",
       "7961         ne        [N, AO, R, TH, IY, S, T]   \n",
       "5160    jacques                     [ZH, AA, K]   \n",
       "7926        moi                      [M, W, AA]   \n",
       "...         ...                             ...   \n",
       "2996  detriment  [D, EH, T, R, AH, M, AH, N, T]   \n",
       "2995  analogous    [AH, N, AE, L, AH, G, AH, S]   \n",
       "2994  equerries        [IH, K, W, EH, R, IY, Z]   \n",
       "2993  divisions    [D, IH, V, IH, ZH, AH, N, Z]   \n",
       "7965          u                         [Y, UW]   \n",
       "\n",
       "                    predicted_phnseq       per  \n",
       "7884                     [HH, AA, N]  1.000000  \n",
       "7953                         [AH, N]  0.750000  \n",
       "7961                         [N, IY]  0.714286  \n",
       "5160                 [JH, AA, K, EY]  0.666667  \n",
       "7926                         [M, OY]  0.666667  \n",
       "...                              ...       ...  \n",
       "2996  [D, EH, T, R, AH, M, AH, N, T]  0.000000  \n",
       "2995    [AH, N, AE, L, AH, G, AH, S]  0.000000  \n",
       "2994        [IH, K, W, EH, R, IY, Z]  0.000000  \n",
       "2993    [D, IH, V, IH, ZH, AH, N, Z]  0.000000  \n",
       "7965                         [Y, UW]  0.000000  \n",
       "\n",
       "[7966 rows x 4 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df.sort_values('per', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75c221b-67cc-40b3-853a-7c60eeca5c41",
   "metadata": {
    "tags": []
   },
   "source": [
    "##  save oov list with PERs to disk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "508653df-0c7f-4ba0-8f76-e2016c434dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "outpath = '/home/s1785140/data/ljspeech_fastpitch/oov_list_with_PER.pickle'\n",
    "df.to_pickle(outpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e793ff-cd04-4194-adba-bad842066160",
   "metadata": {},
   "source": [
    "## load oov with PER from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "30e6f094-06ed-4970-b05c-fce8aae0db87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7966"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_df = pd.read_pickle(outpath)\n",
    "orig_num = len(loaded_df)\n",
    "orig_num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8935b906-27bc-48e9-86c8-b072a4bf7a61",
   "metadata": {},
   "source": [
    "### do some filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9a49f005-6ecd-4875-8843-35051b3ce6a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered out 5967\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1999"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_min_len_threshold = 10\n",
    "loaded_df = loaded_df[loaded_df['wordtype'].str.len() >= str_min_len_threshold]\n",
    "new_num = len(loaded_df)\n",
    "print(f'filtered out {orig_num - new_num}')\n",
    "new_num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed35ad9e-91a8-4d43-98ef-8da636a07220",
   "metadata": {},
   "source": [
    "### display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1cf77b32-a02e-4881-bb54-f6fbb683619c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wordtype</th>\n",
       "      <th>lexicon_entry</th>\n",
       "      <th>predicted_phnseq</th>\n",
       "      <th>per</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1505</th>\n",
       "      <td>exhumation</td>\n",
       "      <td>[EH, K, S, HH, Y, UW, M, EY, SH, AH, N]</td>\n",
       "      <td>[EH, G, Z, AH, M, EY, SH, AH, N]</td>\n",
       "      <td>0.454545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1223</th>\n",
       "      <td>subpoenaed</td>\n",
       "      <td>[S, AH, P, IY, N, AH, D]</td>\n",
       "      <td>[S, AH, P, OW, AH, N, EH, D]</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>experimental</td>\n",
       "      <td>[IH, K, S, P, ER, M, EH, N, AH, L]</td>\n",
       "      <td>[IH, K, S, P, EH, R, AH, M, EH, N, T, AH, L]</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>assignation</td>\n",
       "      <td>[AH, S, AY, N, EY, SH, AH, N]</td>\n",
       "      <td>[AE, S, AH, G, N, EY, SH, AH, N]</td>\n",
       "      <td>0.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>accelerator</td>\n",
       "      <td>[AE, K, S, EH, L, ER, EY, T, ER]</td>\n",
       "      <td>[AE, K, S, EH, L, ER, AH, T, AO, R]</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>individually</td>\n",
       "      <td>[IH, N, D, IH, V, IH, JH, AH, L, IY]</td>\n",
       "      <td>[IH, N, D, AH, V, IH, JH, AH, W, AH, L, IY]</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>depredators</td>\n",
       "      <td>[D, IH, P, R, EH, D, AH, T, ER, Z]</td>\n",
       "      <td>[D, EH, P, R, AH, D, EY, T, ER, Z]</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>assurbanipal</td>\n",
       "      <td>[AH, S, ER, B, AH, N, AH, P, AH, L]</td>\n",
       "      <td>[AH, S, ER, B, AE, N, IH, P, AA, L]</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1334</th>\n",
       "      <td>plaintiffs</td>\n",
       "      <td>[P, L, EY, N, IH, F, S]</td>\n",
       "      <td>[P, L, EY, N, T, AH, F, S]</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1277</th>\n",
       "      <td>phosphoric</td>\n",
       "      <td>[F, AA, S, F, ER, IH, K]</td>\n",
       "      <td>[F, AA, S, F, AO, R, IH, K]</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>archimandrite</td>\n",
       "      <td>[AA, R, K, AH, M, AE, N, D, ER, AY, T]</td>\n",
       "      <td>[AA, R, K, AH, M, AE, N, D, R, IH, T, IY]</td>\n",
       "      <td>0.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>unidentified</td>\n",
       "      <td>[AH, N, AY, D, EH, N, T, AH, F, AY, D]</td>\n",
       "      <td>[Y, UW, N, AH, D, EH, N, T, AH, F, AY, D]</td>\n",
       "      <td>0.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>providentially</td>\n",
       "      <td>[P, R, OW, V, IH, D, EH, N, CH, AH, L, IY]</td>\n",
       "      <td>[P, R, AA, V, AH, D, EH, N, SH, AH, L, IY]</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1601</th>\n",
       "      <td>pugnacious</td>\n",
       "      <td>[P, AH, G, N, AE, SH, IH, S]</td>\n",
       "      <td>[P, AH, G, N, EY, SH, AH, S]</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>forgiveness</td>\n",
       "      <td>[F, ER, G, IH, V, N, AH, S]</td>\n",
       "      <td>[F, AO, R, G, IH, V, N, AH, S]</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1642</th>\n",
       "      <td>encourages</td>\n",
       "      <td>[EH, N, K, ER, IH, JH, IH, Z]</td>\n",
       "      <td>[IH, N, K, ER, AH, JH, IH, Z]</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1870</th>\n",
       "      <td>warehouses</td>\n",
       "      <td>[W, EH, R, HH, AW, Z, IH, Z]</td>\n",
       "      <td>[W, EH, R, HH, AW, S, AH, Z]</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1211</th>\n",
       "      <td>sandwiches</td>\n",
       "      <td>[S, AE, N, W, IH, CH, IH, Z]</td>\n",
       "      <td>[S, AE, N, D, W, IH, CH, AH, Z]</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1574</th>\n",
       "      <td>preventing</td>\n",
       "      <td>[P, R, IY, V, EH, N, IH, NG]</td>\n",
       "      <td>[P, R, IH, V, EH, N, T, IH, NG]</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1647</th>\n",
       "      <td>sepulchres</td>\n",
       "      <td>[S, EY, P, UW, L, CH, R, Z]</td>\n",
       "      <td>[S, EY, P, UW, L, K, ER, Z]</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1916</th>\n",
       "      <td>incentives</td>\n",
       "      <td>[IH, N, S, IH, N, IH, V, Z]</td>\n",
       "      <td>[IH, N, S, EH, N, T, IH, V, Z]</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1137</th>\n",
       "      <td>tentatively</td>\n",
       "      <td>[T, EH, N, T, AH, V, L, IY]</td>\n",
       "      <td>[T, EH, N, T, AH, T, IH, V, L, IY]</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>experimenting</td>\n",
       "      <td>[EH, K, S, P, EH, R, AH, M, EH, N, IH, NG]</td>\n",
       "      <td>[IH, K, S, P, EH, R, AH, M, AH, N, T, IH, NG]</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1123</th>\n",
       "      <td>comptroller</td>\n",
       "      <td>[K, AH, M, T, R, OW, L, ER]</td>\n",
       "      <td>[K, AA, M, P, T, R, OW, L, ER]</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>embarrassed</td>\n",
       "      <td>[IH, M, B, EH, R, AH, S, T]</td>\n",
       "      <td>[IH, M, B, EH, R, AH, S, AH, D]</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>impossibility</td>\n",
       "      <td>[IH, M, P, AO, S, IH, B, IH, L, IH, T, IY]</td>\n",
       "      <td>[IH, M, P, AA, S, AH, B, IH, L, AH, T, IY]</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>demonstrative</td>\n",
       "      <td>[D, IH, M, AA, N, S, T, R, AH, T, IH, V]</td>\n",
       "      <td>[D, EH, M, AH, N, S, T, R, EY, T, IH, V]</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929</th>\n",
       "      <td>inexcusably</td>\n",
       "      <td>[IH, N, IH, K, S, K, Y, UW, Z, AH, B, L, IY]</td>\n",
       "      <td>[IH, N, AH, K, Y, UW, Z, AH, B, L, IY]</td>\n",
       "      <td>0.230769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>nationalists</td>\n",
       "      <td>[N, AE, SH, AH, N, AH, L, IH, S]</td>\n",
       "      <td>[N, AE, SH, AH, N, AH, L, IH, S, T, S]</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1917</th>\n",
       "      <td>presenting</td>\n",
       "      <td>[P, R, IH, Z, EH, N, T, IH, NG]</td>\n",
       "      <td>[P, R, EH, Z, AH, N, T, IH, NG]</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1149</th>\n",
       "      <td>admittedly</td>\n",
       "      <td>[AE, D, M, IH, T, IH, D, L, IY]</td>\n",
       "      <td>[AH, D, M, IH, T, AH, D, L, IY]</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1815</th>\n",
       "      <td>partridges</td>\n",
       "      <td>[P, AA, R, T, R, IH, JH, IH, Z]</td>\n",
       "      <td>[P, AA, R, T, R, AH, JH, AH, Z]</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1626</th>\n",
       "      <td>privileged</td>\n",
       "      <td>[P, R, IH, V, IH, L, IH, JH, D]</td>\n",
       "      <td>[P, R, IH, V, L, AH, JH, D]</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1530</th>\n",
       "      <td>durability</td>\n",
       "      <td>[D, ER, AH, B, IH, L, IH, T, IY]</td>\n",
       "      <td>[D, UH, R, AH, B, IH, L, IH, T, IY]</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1292</th>\n",
       "      <td>affidavits</td>\n",
       "      <td>[AE, F, IH, D, EY, V, IH, T, S]</td>\n",
       "      <td>[AE, F, AH, D, EY, V, AH, T, S]</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917</th>\n",
       "      <td>descendants</td>\n",
       "      <td>[D, IH, S, EH, N, IH, N, T, S]</td>\n",
       "      <td>[D, IH, S, EH, N, D, AH, N, T, S]</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179</th>\n",
       "      <td>embonpoint</td>\n",
       "      <td>[IH, M, B, AO, N, P, OY, N, T]</td>\n",
       "      <td>[EH, M, B, AA, N, P, OY, N, T]</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1886</th>\n",
       "      <td>instigator</td>\n",
       "      <td>[IH, N, S, T, AH, G, EY, T, ER]</td>\n",
       "      <td>[IH, N, S, T, AH, G, EY, T, AO, R]</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>helsingfors</td>\n",
       "      <td>[HH, EH, L, S, IH, NG, F, ER, Z]</td>\n",
       "      <td>[HH, EH, L, S, IH, NG, F, AO, R, Z]</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>disinterring</td>\n",
       "      <td>[D, IH, S, IH, N, T, ER, IH, NG]</td>\n",
       "      <td>[D, IH, S, IH, N, T, EH, R, IH, NG]</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>discounters</td>\n",
       "      <td>[D, IH, S, K, AW, N, ER, R, Z]</td>\n",
       "      <td>[D, IH, S, K, AW, N, T, ER, Z]</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1447</th>\n",
       "      <td>orthopedic</td>\n",
       "      <td>[AO, R, TH, AH, P, IY, D, IH, K]</td>\n",
       "      <td>[AO, R, TH, AH, P, EH, D, AH, K]</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1121</th>\n",
       "      <td>chronometer</td>\n",
       "      <td>[K, R, AA, N, AH, M, IY, T, ER]</td>\n",
       "      <td>[K, R, AA, N, AA, M, AH, T, ER]</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395</th>\n",
       "      <td>fragmented</td>\n",
       "      <td>[F, R, AE, G, M, AH, N, IH, D]</td>\n",
       "      <td>[F, R, AE, G, M, AH, N, T, AH, D]</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>inconsistencies</td>\n",
       "      <td>[IH, NG, K, AA, N, S, IH, S, T, EH, N, S, IY, Z]</td>\n",
       "      <td>[IH, N, K, AH, N, S, IH, S, T, AH, N, S, IY, Z]</td>\n",
       "      <td>0.214286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ineffectualness</td>\n",
       "      <td>[IH, N, AH, F, EH, K, CH, AH, W, AH, L, N, EH, S]</td>\n",
       "      <td>[IH, N, IH, F, EH, K, CH, UW, AH, L, N, EH, S]</td>\n",
       "      <td>0.214286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>918</th>\n",
       "      <td>apprentices</td>\n",
       "      <td>[AH, P, R, EH, N, T, AH, S, IH, Z]</td>\n",
       "      <td>[AH, P, R, EH, N, T, IH, S, AH, Z]</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>underclothing</td>\n",
       "      <td>[AH, N, D, ER, K, L, AO, TH, IH, NG]</td>\n",
       "      <td>[AH, N, D, ER, K, L, OW, DH, IH, NG]</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528</th>\n",
       "      <td>republican</td>\n",
       "      <td>[R, IY, P, AH, B, L, AH, K, AH, N]</td>\n",
       "      <td>[R, IH, P, AH, B, L, IH, K, AH, N]</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1592</th>\n",
       "      <td>incubation</td>\n",
       "      <td>[IH, NG, K, Y, UW, B, EY, SH, AH, N]</td>\n",
       "      <td>[IH, NG, K, AH, B, EY, SH, AH, N]</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             wordtype                                      lexicon_entry  \\\n",
       "1505       exhumation            [EH, K, S, HH, Y, UW, M, EY, SH, AH, N]   \n",
       "1223       subpoenaed                           [S, AH, P, IY, N, AH, D]   \n",
       "395      experimental                 [IH, K, S, P, ER, M, EH, N, AH, L]   \n",
       "653       assignation                      [AH, S, AY, N, EY, SH, AH, N]   \n",
       "984       accelerator                   [AE, K, S, EH, L, ER, EY, T, ER]   \n",
       "397      individually               [IH, N, D, IH, V, IH, JH, AH, L, IY]   \n",
       "836       depredators                 [D, IH, P, R, EH, D, AH, T, ER, Z]   \n",
       "527      assurbanipal                [AH, S, ER, B, AH, N, AH, P, AH, L]   \n",
       "1334       plaintiffs                            [P, L, EY, N, IH, F, S]   \n",
       "1277       phosphoric                           [F, AA, S, F, ER, IH, K]   \n",
       "187     archimandrite             [AA, R, K, AH, M, AE, N, D, ER, AY, T]   \n",
       "394      unidentified             [AH, N, AY, D, EH, N, T, AH, F, AY, D]   \n",
       "58     providentially         [P, R, OW, V, IH, D, EH, N, CH, AH, L, IY]   \n",
       "1601       pugnacious                       [P, AH, G, N, AE, SH, IH, S]   \n",
       "890       forgiveness                        [F, ER, G, IH, V, N, AH, S]   \n",
       "1642       encourages                      [EH, N, K, ER, IH, JH, IH, Z]   \n",
       "1870       warehouses                       [W, EH, R, HH, AW, Z, IH, Z]   \n",
       "1211       sandwiches                       [S, AE, N, W, IH, CH, IH, Z]   \n",
       "1574       preventing                       [P, R, IY, V, EH, N, IH, NG]   \n",
       "1647       sepulchres                        [S, EY, P, UW, L, CH, R, Z]   \n",
       "1916       incentives                        [IH, N, S, IH, N, IH, V, Z]   \n",
       "1137      tentatively                        [T, EH, N, T, AH, V, L, IY]   \n",
       "204     experimenting         [EH, K, S, P, EH, R, AH, M, EH, N, IH, NG]   \n",
       "1123      comptroller                        [K, AH, M, T, R, OW, L, ER]   \n",
       "662       embarrassed                        [IH, M, B, EH, R, AH, S, T]   \n",
       "291     impossibility         [IH, M, P, AO, S, IH, B, IH, L, IH, T, IY]   \n",
       "261     demonstrative           [D, IH, M, AA, N, S, T, R, AH, T, IH, V]   \n",
       "929       inexcusably       [IH, N, IH, K, S, K, Y, UW, Z, AH, B, L, IY]   \n",
       "583      nationalists                   [N, AE, SH, AH, N, AH, L, IH, S]   \n",
       "1917       presenting                    [P, R, IH, Z, EH, N, T, IH, NG]   \n",
       "1149       admittedly                    [AE, D, M, IH, T, IH, D, L, IY]   \n",
       "1815       partridges                    [P, AA, R, T, R, IH, JH, IH, Z]   \n",
       "1626       privileged                    [P, R, IH, V, IH, L, IH, JH, D]   \n",
       "1530       durability                   [D, ER, AH, B, IH, L, IH, T, IY]   \n",
       "1292       affidavits                    [AE, F, IH, D, EY, V, IH, T, S]   \n",
       "917       descendants                     [D, IH, S, EH, N, IH, N, T, S]   \n",
       "1179       embonpoint                     [IH, M, B, AO, N, P, OY, N, T]   \n",
       "1886       instigator                    [IH, N, S, T, AH, G, EY, T, ER]   \n",
       "871       helsingfors                   [HH, EH, L, S, IH, NG, F, ER, Z]   \n",
       "329      disinterring                   [D, IH, S, IH, N, T, ER, IH, NG]   \n",
       "885       discounters                     [D, IH, S, K, AW, N, ER, R, Z]   \n",
       "1447       orthopedic                   [AO, R, TH, AH, P, IY, D, IH, K]   \n",
       "1121      chronometer                    [K, R, AA, N, AH, M, IY, T, ER]   \n",
       "1395       fragmented                     [F, R, AE, G, M, AH, N, IH, D]   \n",
       "34    inconsistencies   [IH, NG, K, AA, N, S, IH, S, T, EH, N, S, IY, Z]   \n",
       "20    ineffectualness  [IH, N, AH, F, EH, K, CH, AH, W, AH, L, N, EH, S]   \n",
       "918       apprentices                 [AH, P, R, EH, N, T, AH, S, IH, Z]   \n",
       "264     underclothing               [AH, N, D, ER, K, L, AO, TH, IH, NG]   \n",
       "1528       republican                 [R, IY, P, AH, B, L, AH, K, AH, N]   \n",
       "1592       incubation               [IH, NG, K, Y, UW, B, EY, SH, AH, N]   \n",
       "\n",
       "                                     predicted_phnseq       per  \n",
       "1505                 [EH, G, Z, AH, M, EY, SH, AH, N]  0.454545  \n",
       "1223                     [S, AH, P, OW, AH, N, EH, D]  0.428571  \n",
       "395      [IH, K, S, P, EH, R, AH, M, EH, N, T, AH, L]  0.400000  \n",
       "653                  [AE, S, AH, G, N, EY, SH, AH, N]  0.375000  \n",
       "984               [AE, K, S, EH, L, ER, AH, T, AO, R]  0.333333  \n",
       "397       [IH, N, D, AH, V, IH, JH, AH, W, AH, L, IY]  0.300000  \n",
       "836                [D, EH, P, R, AH, D, EY, T, ER, Z]  0.300000  \n",
       "527               [AH, S, ER, B, AE, N, IH, P, AA, L]  0.300000  \n",
       "1334                       [P, L, EY, N, T, AH, F, S]  0.285714  \n",
       "1277                      [F, AA, S, F, AO, R, IH, K]  0.285714  \n",
       "187         [AA, R, K, AH, M, AE, N, D, R, IH, T, IY]  0.272727  \n",
       "394         [Y, UW, N, AH, D, EH, N, T, AH, F, AY, D]  0.272727  \n",
       "58         [P, R, AA, V, AH, D, EH, N, SH, AH, L, IY]  0.250000  \n",
       "1601                     [P, AH, G, N, EY, SH, AH, S]  0.250000  \n",
       "890                    [F, AO, R, G, IH, V, N, AH, S]  0.250000  \n",
       "1642                    [IH, N, K, ER, AH, JH, IH, Z]  0.250000  \n",
       "1870                     [W, EH, R, HH, AW, S, AH, Z]  0.250000  \n",
       "1211                  [S, AE, N, D, W, IH, CH, AH, Z]  0.250000  \n",
       "1574                  [P, R, IH, V, EH, N, T, IH, NG]  0.250000  \n",
       "1647                      [S, EY, P, UW, L, K, ER, Z]  0.250000  \n",
       "1916                   [IH, N, S, EH, N, T, IH, V, Z]  0.250000  \n",
       "1137               [T, EH, N, T, AH, T, IH, V, L, IY]  0.250000  \n",
       "204     [IH, K, S, P, EH, R, AH, M, AH, N, T, IH, NG]  0.250000  \n",
       "1123                   [K, AA, M, P, T, R, OW, L, ER]  0.250000  \n",
       "662                   [IH, M, B, EH, R, AH, S, AH, D]  0.250000  \n",
       "291        [IH, M, P, AA, S, AH, B, IH, L, AH, T, IY]  0.250000  \n",
       "261          [D, EH, M, AH, N, S, T, R, EY, T, IH, V]  0.250000  \n",
       "929            [IH, N, AH, K, Y, UW, Z, AH, B, L, IY]  0.230769  \n",
       "583            [N, AE, SH, AH, N, AH, L, IH, S, T, S]  0.222222  \n",
       "1917                  [P, R, EH, Z, AH, N, T, IH, NG]  0.222222  \n",
       "1149                  [AH, D, M, IH, T, AH, D, L, IY]  0.222222  \n",
       "1815                  [P, AA, R, T, R, AH, JH, AH, Z]  0.222222  \n",
       "1626                      [P, R, IH, V, L, AH, JH, D]  0.222222  \n",
       "1530              [D, UH, R, AH, B, IH, L, IH, T, IY]  0.222222  \n",
       "1292                  [AE, F, AH, D, EY, V, AH, T, S]  0.222222  \n",
       "917                 [D, IH, S, EH, N, D, AH, N, T, S]  0.222222  \n",
       "1179                   [EH, M, B, AA, N, P, OY, N, T]  0.222222  \n",
       "1886               [IH, N, S, T, AH, G, EY, T, AO, R]  0.222222  \n",
       "871               [HH, EH, L, S, IH, NG, F, AO, R, Z]  0.222222  \n",
       "329               [D, IH, S, IH, N, T, EH, R, IH, NG]  0.222222  \n",
       "885                    [D, IH, S, K, AW, N, T, ER, Z]  0.222222  \n",
       "1447                 [AO, R, TH, AH, P, EH, D, AH, K]  0.222222  \n",
       "1121                  [K, R, AA, N, AA, M, AH, T, ER]  0.222222  \n",
       "1395                [F, R, AE, G, M, AH, N, T, AH, D]  0.222222  \n",
       "34    [IH, N, K, AH, N, S, IH, S, T, AH, N, S, IY, Z]  0.214286  \n",
       "20     [IH, N, IH, F, EH, K, CH, UW, AH, L, N, EH, S]  0.214286  \n",
       "918                [AH, P, R, EH, N, T, IH, S, AH, Z]  0.200000  \n",
       "264              [AH, N, D, ER, K, L, OW, DH, IH, NG]  0.200000  \n",
       "1528               [R, IH, P, AH, B, L, IH, K, AH, N]  0.200000  \n",
       "1592                [IH, NG, K, AH, B, EY, SH, AH, N]  0.200000  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_TO_DISPLAY = 50\n",
    "loaded_df.sort_values('per', ascending=False).head(NUM_TO_DISPLAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4de7a2-dfcf-437b-9771-6f08e44525c3",
   "metadata": {},
   "source": [
    "# LM Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0c72b6-769e-402b-a7a1-68ea5d213dcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
